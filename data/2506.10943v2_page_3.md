Test-Time Training. Test-Time Training (TTT) temporarily adapts model weights based on the input the model receives [ 33 , 34 , 35 , 36 ]. Aky�rek et a l. [36] show that combining TTT with ICL enables gradient-updates to outperform standard ICL in the few-shot setting. SEAL can be viewed as incorporating a round of TTT in its inner-loop optimization, leveraging TTT's e fciency relative to full-scale training to perform multiple updates and reward the generated data that yields the greatest performance gain. Although our method is trained using single-example TTT episodes, we demonstrate in the knowledge incorporation setting that it generalizes to the continued pretraining (CPT) regime�where placing data directly in context is no longer feasible. Reinforcement Learning for LLMs. Reinforcement learning has played a central role in improving LLM behavior, originally through RLHF [ 37 , 38 ]. More recently, RL with veriable rewards has been applied to boost reasoning performance by optimizing the model directly for task success [ 39 , 40 , 41 ]. SEAL applies RL not to optimize nal answers or trace revisions, but to optimize the generation of self-edit data that is then used for weight updates. Meta-Learning and Self-Modifying Systems. SEAL embodies meta-learning principles [ 42 , 43 , 44 ] by learning an adaptation strategy�how to generate effective self-edits�via its outer optimization loop. The goal is to learn how to learn e fciently from task contexts. In reinforcement learning, meta-learning has been used to train agents that learn new tasks quickly [ 45 , 46 , 47 , 48 ]. Sun et a l. [49] similarly apply RL to learn task-specic weight modulations, offering an alternative to LoRA netuning that is orthogonal to our approach. A natural extension of meta-learning is self-referential networks, where models modify their own parameters [ 50 , 51 ]. In the domain of large language models, recent work has applied meta-learning to improve LLM adaptation [ 52 , 53 , 54 , 55 , 49 ]. Notably, Hu et a l. [53] train a smaller model to output token-specic weights during netuning, addressing a knowledge incorporation task similar to ours, while Chen et a l. [54] propose a hypernetwork that generates LoRA adapters conditioned on the input, enabling dynamic and task-specic parameterization. However, SEAL offers greater generality by leveraging the model's existing generative capabilities to parametrize updates. Self-Improvement. Several recent works fall under the umbrella of self-improvement or selftraining. Methods such as RLAIF [ 56 , 57 ] and self-rewarding language models [ 58 , 59 ] use the model itself to provide reward signals, leveraging the observation that judging outputs is often easier than generating them [ 60 ]. Other recent works improve performance on mathematical tasks by using majority-vote or model condence as reinforcement learning rewards, enabling performance improvement without access to ground-truth labels [ 61 , 62 , 63 , 64 , 65 ]. However, all of these methods are fundamentally limited by the model's current evaluation abilities and self-consistency. In contrast, we view self-improvement through interaction with external data as a more powerful and scalable path. SEAL learns how to best utilize this external data for self-improvement.


## 3 Methods

We propose Self-Adapting LLMs (SEAL), a framework that enables language models to improve themselves by generating their own synthetic data and optimization parameters (�self-edits�) in response to new data. The model is trained to produce these self-edits directly through token generation with the data provided in the model's context. Self-edit generation is learned via reinforcement learning (RL) where the model is rewarded for generating self-edits ( SE ) that, when applied, improve the model's performance at the target task. SEAL can therefore be interpreted as an algorithm with two nested loops: an outer RL loop , which optimizes the self-edit generation, and an inner update loop , which uses the generated self-edit to update the model via gradient descent. Our method can be seen as an instance of meta-learning where we meta-learn how to generate effective self-edits.


## 3.1 General Framework

Let  denote the parameters of the language model LM  . SEAL operates on individual task instances ( C;  ) where C is a context containing information relevant to the task, and  d enes the downstream evaluation used to assess the model's adaptation. For example, in knowledge incorporation, C is the passage intended to be integrated into the model's internal knowledge, and  is a set of questions and associated answers about the passage. In few-shot learning, C includes few-shot demonstrations of a 3