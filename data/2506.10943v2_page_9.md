r e-evaluate the model on all previously seen tasks to measure retention. This setup tests the model's ability to integrate new edits without forgetting earlier ones. As shown in Figure 6, performance on earlier tasks gradually declines as the number of edits increases, suggesting that SEAL is still susceptible to catastrophic forgetting. Still, it can perform multiple updates without complete collapse, indicating possibility for improvement. Future work could enhance this ability through reward shaping [ 75 , 76 , 77 ] to penalize regressions on earlier tasks, or by integrating continual learning strategies such as null-space constrained edits [ 78 ] or representational superposition [ 79 ]. In addition, since RL has been shown to forget less than SFT, SEAL's inner loop could also employ RL instead of SFT [80]. Computational overhead. The TTT reward loop is signicantly more computationally expensive than other reinforcement learning loops used with LLMs. For instance, reward signals based on human preferences typically involve a single model forward pass, and those using verie d solutions may rely on simple pattern matching (e.g., regex). In contrast, our approach requires netuning and evaluating an entire model to compute the reward�each self-edit evaluation takes approximately


## 30�45 seconds, introducing substantial overhead (see �B.5).

Context-dependent evaluation. Our current instantiations assume that every context is paired with an explicit downstream task: few-shot demonstrations arrive with a held out query pair, and each passage comes bundled with reference QA. This coupling simplie s reward computation but prevents RL training of SEAL from scaling to unlabeled corpora. A potential solution is to let the model generate not only self-edits but also its own evaluation questions�e.g., draft QA items or synthetic test cases for each passage�while the original content is still in context. These model-written queries could provide the immediate supervision required for reinforcement learning, broadening applicability to general training domains where external question-and-answer sets are unavailable.


## 6 Discussion and Conclusion

Villalobos et a l. [81] project that frontier LLMs will be trained on all publicly available humangenerated text by 2028. We argue that this impending �data wall� will necessitate the adoption of synthetic data augmentation. Once web-scale corpora are exhausted, progress will hinge on a model's capacity to generate its own high-utility training signal . A natural next step is to meta-train a dedicated SEAL synthetic-data generator model that produces fresh pretraining corpora, allowing future models to scale and achieve greater data e fciency without relying on additional human text. We can imagine a future in which LLMs can ingest new data, such as academic papers, and generate large quantities of explanations and implications for themselves using their existing knowledge and reasoning with the i n-context data. This iterative loop of self-expression and self-r enement could allow models to keep improving on rare or underrepresented topics even in the absence of additional external supervision. In addition, while modern reasoning models are often trained with RL to generate chain-o f-thought (CoT) traces, SEAL could offer a complementary mechanism, allowing the model to learn when and how to update its own weights. These two approaches could synergize: the model may choose to perform weight updates mid-reasoning to guide its current trajectory, or after completing reasoning to distill key insights into its parameters�improving future inference through internalized learning. This continual r enement loop is also promising for building agentic systems�models that operate over extended interactions and adapt dynamically to evolving goals. Agentic models must incrementally acquire and retain knowledge as they act. Our approach supports such behavior by enabling structured self-modication: after an interaction, the agent could synthesize a self-edit which triggers a weight update. This could allow the agent to develop over time, aligning its behavior with prior experience and reducing reliance on repeated supervision. SEAL demonstrates that large language models need not remain static after pretraining: by learning to generate their own synthetic self-edit data and to apply it through lightweight weight updates, they can autonomously incorporate new knowledge and adapt to novel tasks. Looking ahead, we envision extending the SEAL framework to pretraining, continual learning, and agentic models, ultimately enabling language models to self-learn and scale in a data-constrained world. 9