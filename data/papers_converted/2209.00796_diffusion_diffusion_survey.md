## Diffusion Models: A Comprehensive Survey of Methods and Applications

## LING YANG, Peking University, China

ZHILONG ZHANG ∗, Peking University, China YANG SONG, OpenAI, USA SHENDA HONG, Peking University, China RUNSHENG XU, University of California, Los Angeles, USA YUE ZHAO, Carnegie Mellon University, USA WENTAO ZHANG, Peking University, China BIN CUI, Peking University, China MING-HSUAN YANG †, University of California at Merced, USADiffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in manyapplications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidlyexpanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihoodestimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generativemodels for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computervision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. Thissurvey aims to provide a contextualized, i n-depth look at the state of diffusion models, identifying the key areas of focus and pointingto potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.

CCS Concepts: • Computing methodologies → Computer vision tasks; Natural language generation; Machine learning approaches.

Additional Key Words and Phrases: Generative Models, Diffusion Models, Score-Based Generative Models, Stochastic DifferentialEquationsACM Reference Format:

Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023.

Diffusion Models: A Comprehensive Survey of Methods and Applications. 1, 1 (September 2023), 59 pages. https://doi.org/10.1145/3626235

∗ Contributed equally.

† Ling Yang, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang are corresponding authors.

Authors’ addresses: Ling Yang, Peking University, China, yangling0818@163.com; Zhilong Zhang, Peking University, China, zhilong.zhang@bjmu.edu.c n; Yang Song, OpenAI, USA, songyang@openai.com; Shenda Hong, Peking University, China, hongshenda@pku.edu.c n; Runsheng Xu, University of California, Los Angeles, USA, rxx3386@ucla.edu; Yue Zhao, Carnegie Mellon University, USA, zhaoy@cmu.edu; Wentao Zhang, Peking University, China, wentao.zhang@pku.edu.c n; Bin Cui, Peking University, China, bin.cui@pku.edu.c n; Ming-Hsuan Yang, University of California at Merced, USA, mhyang@ucmerced.edu.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/o ra fee. Request permissions from permissions@acm.org.

© 2023 Association for Computing Machinery. Accepted by ACM Computing SurveysAccepted by ACM Computing Surveys 1arXiv:2209.00796v15  [c s.LG]  27 Sep 2025

## 2 Yang et a l.

ContentsAbstract 1Contents 2

## 1 Introduction 3

## 2 Foundations of Diffusion Models 5

## 2.1 Denoising Diffusion Probabilistic Models (DDPMs) 5

## 2.2 Score-Based Generative Models (SGMs) 8

## 2.3 Stochastic Differential Equations (Score SDEs) 9

## 3 Diffusion Models with Efficient Sampling 10

## 3.1 Learning-Free Sampling 10

## 3.1.1 SDE Solvers 10

## 3.1.2 ODE solvers 12

## 3.2 Learning-Based Sampling 13

## 3.2.1 Optimized Discretization 13

## 3.2.2 Truncated Diffusion 14

## 3.2.3 Knowledge Distillation 14

## 4 Diffusion Models with Improved Likelihood 14

## 4.1 Noise Schedule Optimization 14

## 4.2 Reverse Variance Learning 15

## 4.3 Exact Likelihood Computation 16

## 5 Diffusion Models for Data with Special Structures 17

## 5.1 Discrete Data 17

## 5.2 Data with Invariant Structures 18

## 5.3 Data with Manifold Structures 18

## 5.3.1 Known Manifolds 18

## 5.3.2 Learned Manifolds 19

## 6 Connections with Other Generative Models 19

## 6.1 Large Language Models and Connections with Diffusion Models 19

## 6.2 Variational Autoencoders and Connections with Diffusion Models 21

## 6.3 Generative Adversarial Networks and Connections with Diffusion Models 22

## 6.4 Normalizing Flows and Connections with Diffusion Models 22

## 6.5 Autoregressive Models and Connections with Diffusion Models 23

## 6.6 Energy-based Models and Connections with Diffusion Models 24

## 7 Applications of Diffusion Models 25

## 7.1 Unconditional and Conditional Diffusion Models 25

## 7.1.1 Conditioning Mechanisms in Diffusion Models 25

## 7.1.2 Diffusion with DPO/RLHF 26

## 7.1.3 Condition Diffusion on Labels and Classifiers 26

## 7.1.4 Condition Diffusion on Texts, Images, and Semantic Maps 26

## 7.1.5 Condition Diffusion on Graphs 28

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 3

## 7.2 Computer Vision 28

## 7.2.1 Image Super Resolution, Inpainting, Restoration, Translation, and Editing 28

## 7.2.2 Semantic Segmentation 29

## 7.2.3 Video Generation 29

## 7.2.4 Generating Data from Diffusion Models 30

## 7.2.5 Point Cloud Completion and Generation 30

## 7.2.6 Anomaly Detection 31

## 7.3 Natural Language Generation 32

## 7.4 Multi-Modal Generation 32

## 7.4.1 Text-t o-Image Generation 32

## 7.4.2 Scene Graph-t o-Image Generation 37

## 7.4.3 Text-t o-3D Generation 37

## 7.4.4 Text-t o-Motion Generation 38

## 7.4.5 Text-t o-Video Generation 38

## 7.4.6 Text-t o-Audio Generation 40

## 7.5 Temporal Data Modeling 40

## 7.5.1 Time Series Imputation 40

## 7.5.2 Time Series Forecasting 40

## 7.5.3 Waveform Signal Processing 41

## 7.6 Robust Learning 41

## 7.7 Interdisciplinary Applications 42

## 7.7.1 Drug Design and Life Science 42

## 7.7.2 Material Design 43

## 7.7.3 Medical Image Reconstruction 43

## 8 Future Directions 44

Revisiting Assumptions 44Theoretical Understanding 44Latent Representations 44AIGC and Diffusion Foundation Models 44

## 9 Conclusion 45

References 45

## 1 INTRODUCTION

Diffusion models [112, 277, 282, 287] have emerged as the new state-o f-the-art family of deep generative models.

They have broken the long-time dominance of generative adversarial networks (GANs) [88] in the challenging taskof image synthesis [60, 112, 282, 287] and have also shown potential in a variety of domains, ranging from computervision [4, 15, 25, 29, 113, 115, 146, 150, 172, 196, 208, 227, 259, 261, 319, 360, 361, 385, 395], natural language processing[9, 118, 176, 266, 367], temporal data modeling [3, 39, 160, 251, 293, 339], multi-modal modeling [10, 245, 257, 260, 392],Accepted by ACM Computing Surveys

## 4 Yang et a l.

Fig. 1. Taxonomy of diffusion models variants (i n Sections 3 to 5), connections with other generative models (i n Section 6), applications of diffusion models (i n Section 7), and future directions (i n Section 8).

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 5robust machine learning [23, 33, 145, 310, 363], to interdisciplinary applications in fields such as computational chemistry[5, 116, 134, 167, 170, 198, 331] and medical image reconstruction [31, 47–49, 53, 204, 232, 286, 332].

Numerous methods have been developed to improve diffusion models, either by enhancing empirical perfor-mance [216, 279, 283] or by extending the model’s capacity from a theoretical perspective [189, 190, 281, 287, 377]. Overthe past two years, the body of research on diffusion models has grown significantly, making it increasingly challengingfor new researchers to stay abreast of the recent developments in the field. Additionally, the sheer volume of work canobscure major trends and hinder further research progress. This survey aims to address these problems by providing acomprehensive overview of the state of diffusion model research, categorizing various approaches, and highlightingkey advances. We hope this survey to serve as a helpful entry point for researchers new to the field while providing abroader perspective for experienced researchers.

In this paper, we first explain the foundations of diffusion models (Section 2), providing a brief but self-containedintroduction to three predominant formulations: denoising diffusion probabilistic models (DDPMs) [112, 277], score-based generative models (SGMs) [282, 283], and stochastic differential equations (Score SDEs) [142, 281, 287]. Key toall these approaches is to progressively perturb data with intensifying random noise (called the “diffusion” process),then successively remove noise to generate new data samples. We clarify how they work under the same principle ofdiffusion and explain how these three models are connected and can be reduced to one another.

Next, we present a taxonomy of recent research that maps out the field of diffusion models, categorizing it into threekey areas: efficient sampling (Section 3), improved likelihood estimation (Section 4), and methods for handling data withspecial structures (Section 5), such as relational data, data with permutation/rotational invariance, and data residing onmanifolds. We further examine the models by breaking each category into more detailed sub-categories, as illustratedin Fig. 1. In addition, we discuss the connections of diffusion models to other deep generative models (Section 6),including variational autoencoders (VAEs) [157, 254], generative adversarial networks (GANs) [88], normalizing flows[62, 64, 228, 256], autoregressive models [304], and energy-based models (EBMs) [166, 285]. By combining these modelswith diffusion models, researchers have the potential to achieve even stronger performance.

Following that, our survey reviews six major categories of application that diffusion models have been applied to in theexisting research (Section 7): computer vision, natural language process, temporal data modeling, multi-modal learning,robust learning, and interdisciplinary applications. For each task, we provide a definition, describe how diffusion modelscan be employed to address it and summarize relevant previous work. We conclude our paper (Sections 8 and 9) byproviding an outlook on possible future directions for this exciting new area of research.

## 2 FOUNDATIONS OF DIFFUSION MODELS

Diffusion models are a family of probabilistic generative models that progressively destruct data by injecting noise,then learn to reverse this process for sample generation. We present the intuition of diffusion models in Fig. 2. Currentresearch on diffusion models is mostly based on three predominant formulations: denoising diffusion probabilisticmodels (DDPMs) [112, 216, 277], score-based generative models (SGMs) [282, 283], and stochastic differential equations(Score SDEs) [281, 287]. We give a self-contained introduction to these three formulations in this section, while discussingtheir connections with each other along the way.

## 2.1 Denoising Diffusion Probabilistic Models (DDPMs)

A denoising diffusion probabilistic model (DDPM) [112, 277] makes use of two Markov chains: a forward chain thatperturbs data to noise, and a reverse chain that converts noise back to data. The former is typically hand-designed withAccepted by ACM Computing Surveys

## 6 Yang et a l.

Fig. 2. Diffusion models smoothly perturb data by adding noise, then reverse this process to generate new data from noise. Each denoising step in the reverse process typically requires estimating the score function (see the illustrative figure on the right), which is a gradient pointing to the directions of data with higher likelihood and less noise.

the goal to transform any data distribution into a simple prior distribution (e.g., standard Gaussian), while the latterMarkov chain reverses the former by learning transition kernels parameterized by deep neural networks. New datapoints are subsequently generated by first sampling a random vector from the prior distribution, followed by ancestralsampling through the reverse Markov chain [159].

Formally, given a data distribution x 0 ∼ 𝑞(x 0), the forward Markov process generates a sequence of random variablesx1, x 2 . . . x𝑇 with transition kernel 𝑞(x𝑡 | x𝑡 −1). Using the chain rule of probability and the Markov property, we canfactorize the joint distribution of x 1, x 2 . . . x𝑇 conditioned on x 0, denoted as 𝑞(x 1, . . . , x𝑇 | x 0), into𝑞(x 1, . . . , x𝑇 | x 0) = 𝑇Ö𝑡 =1 𝑞(x𝑡 | x𝑡 −1). (1)In DDPMs, we handcraft the transition kernel 𝑞(x𝑡 | x𝑡 −1) to incrementally transform the data distribution 𝑞(x 0) into atractable prior distribution. One typical design for the transition kernel is Gaussian perturbation, and the most commonchoice for the transition kernel is 𝑞(x𝑡 | x𝑡 −1) = N (x𝑡 ; √︁1 − 𝛽𝑡 x𝑡 −1, 𝛽𝑡 I), (2)where 𝛽𝑡 ∈ (0, 1) is a hyperparameter chosen ahead of model training. We use this kernel to simply our discussion here,although other types of kernels are also applicable in the same vein. As observed by Sohl-Dickstein et a l. (2015) [277],this Gaussian transition kernel allows us to marginalize the joint distribution in Eq. (1) to obtain the analytical form of𝑞(x𝑡 | x 0) for all 𝑡 ∈ {0, 1, · · · ,𝑇 }. Specifically, with 𝛼𝑡 := 1 − 𝛽𝑡 and ¯𝛼𝑡 := Î𝑡 𝑠=0 𝛼𝑠 , we have𝑞(x𝑡 | x 0) = N (x𝑡 ; √ ¯𝛼𝑡 x 0, (1 − ¯𝛼𝑡 )I). (3)Given x 0, we can easily obtain a sample of x𝑡 by sampling a Gaussian vector 𝝐 ∼ N (0, I) and applying the transformationx𝑡 = √ ¯𝛼𝑡 x 0 + √1 − ¯𝛼𝑡 𝝐. (4)When ¯𝛼𝑇 ≈ 0, x𝑇 is almost Gaussian in distribution, so we have 𝑞(x𝑇 ) := ∫ 𝑞(x𝑇 | x 0)𝑞(x 0)dx0 ≈ N (x𝑇 ; 0, I).

Intuitively speaking, this forward process slowly injects noise to data until all structures are lost. For generatingnew data samples, DDPMs start by first generating an unstructured noise vector from the prior distribution (which istypically trivial to obtain), then gradually remove noise therein by running a learnable Markov chain in the reversetime direction. Specifically, the reverse Markov chain is parameterized by a prior distribution 𝑝 (x𝑇 ) = N (x𝑇 ; 0, I) and aAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 7learnable transition kernel 𝑝𝜃 (x𝑡 −1 | x𝑡 ). We choose the prior distribution 𝑝 (x𝑇 ) = N (x𝑇 ; 0, I) because the forwardprocess is constructed such that 𝑞(x𝑇 ) ≈ N (x𝑇 ; 0, I). The learnable transition kernel 𝑝𝜃 (x𝑡 −1 | x𝑡 ) takes the form of𝑝𝜃 (x𝑡 −1 | x𝑡 ) = N (x𝑡 −1; 𝜇𝜃 (x𝑡 , 𝑡), Σ𝜃 (x𝑡 , 𝑡)) (5)where 𝜃 denotes model parameters, and the mean 𝜇𝜃 (x𝑡, 𝑡) and variance Σ𝜃 (x𝑡 , 𝑡) are parameterized by deep neuralnetworks. With this reverse Markov chain in hand, we can generate a data sample x 0 by first sampling a noise vectorx𝑇 ∼ 𝑝 (x𝑇 ), then iteratively sampling from the learnable transition kernel x𝑡 −1 ∼ 𝑝𝜃 (x𝑡 −1 | x𝑡 ) until 𝑡 = 1.

Key to the success of this sampling process is training the reverse Markov chain to match the actual time reversal of theforward Markov chain. That i s, we have to adjust the parameter 𝜃 so that the joint distribution of the reverse Markov chain𝑝𝜃 (x 0, x 1, · · · , x𝑇 ) := 𝑝 (x𝑇 ) Î𝑇 𝑡 =1 𝑝𝜃 (x𝑡 −1 | x𝑡 ) closely approximates that of the forward process 𝑞(x 0, x 1, · · · , x𝑇 ) :=𝑞(x 0) Î𝑇 𝑡 =1 𝑞(x𝑡 | x𝑡 −1) (Eq. (1)). This is achieved by minimizing the Kullback-Leibler (KL) divergence between thesetwo: KL(𝑞(x 0, x 1, · · · , x𝑇 ) || 𝑝𝜃 (x 0, x 1, · · · , x𝑇 )) (6)(𝑖 ) = − E𝑞 (x 0,x 1,··· ,x𝑇 ) [log 𝑝𝜃 (x 0, x 1, · · · , x𝑇 )] + const (7)(𝑖𝑖 ) = E𝑞 (x 0,x 1,··· ,x𝑇 ) [ − log 𝑝 (x𝑇 ) − 𝑇∑︁𝑡 =1 log 𝑝𝜃 (x𝑡 −1 | x𝑡 ) 𝑞(x𝑡 | x𝑡 −1) ]︸                                                                ︷︷                                                                ︸ :=−𝐿VLB (x 0 ) +const (8)(𝑖𝑖𝑖 ) ≥ E [− log 𝑝𝜃 (x 0)] + const, (9)where (i) is from the definition of KL divergence, (i i) is from the fact that 𝑞(x 0, x 1, · · · , x𝑇 ) and 𝑝𝜃 (x 0, x 1, · · · , x𝑇 ) areboth products of distributions, and (iii) is from Jensen’s inequality. The first term in Eq. (8) is the variational lowerbound (VLB) of the log-likelihood of the data x 0, a common objective for training probabilistic generative models.

We use “const” to symbolize a constant that does not depend on the model parameter 𝜃 and hence does not affectoptimization. The objective of DDPM training is to maximize the VLB (o r equivalently, minimizing the negative VLB),which is particularly easy to optimize because it i sa sum of independent terms, and can thus be estimated efficiently byMonte Carlo sampling [214] and optimized effectively by stochastic optimization [288].

Ho et a l. (2020) [112] propose to reweight various terms in 𝐿VLB for better sample quality and noticed an importantequivalence between the resulting loss function and the training objective for noise-conditional score networks (NCSNs),one type of score-based generative models, in Song and Ermon [282]. The loss in [112] takes the form ofE𝑡 ∼U⟦1,𝑇 ⟧,x 0∼𝑞 (x 0 ),𝝐∼N (0,I) [𝜆(𝑡) ∥𝝐 − 𝝐𝜃 (x𝑡 , 𝑡)∥2] (10)where 𝜆(𝑡) is a positive weighting function, x𝑡 is computed from x 0 and 𝝐 by Eq. (4), U⟦1,𝑇 ⟧ is a uniform distributionover the set {1, 2, · · · ,𝑇 }, and 𝝐𝜃 is a deep neural network with parameter 𝜃 that predicts the noise vector 𝝐 given x𝑡 and 𝑡. This objective reduces to Eq. (8) for a particular choice of the weighting function 𝜆(𝑡), and has the same formas the loss of denoising score matching over multiple noise scales for training score-based generative models [282],another formulation of diffusion models to be discussed in the next section. Accepted by ACM Computing Surveys

## 8 Yang et a l.

## 2.2 Score-Based Generative Models (SGMs)

At the core of score-based generative models [282, 283] is the concept of (Stein) score (a.k.a., score or score function) [127].

Given a probability density function 𝑝 (x), its score function is defined as the gradient of the log probability density∇x log 𝑝 (x). Unlike the commonly used Fisher score ∇𝜃 log 𝑝𝜃 (x) in statistics, the Stein score considered here is afunction of the data x rather than the model parameter 𝜃 . It is a vector field that points to directions along which theprobability density function has the largest growth rate.

The key idea of score-based generative models (SGMs) [282] is to perturb data with a sequence of intensifyingGaussian noise and jointly estimate the score functions for all noisy data distributions by training a deep neuralnetwork model conditioned on noise levels (called a noise-conditional score network, NCSN, in [282]). Samplesare generated by chaining the score functions at decreasing noise levels with score-based sampling approaches,including Langevin Monte Carlo [96, 138, 229, 282, 287], stochastic differential equations [137, 287], ordinary differentialequations [142, 190, 281, 287, 377], and their various combinations [287]. Training and sampling are completely decoupledin the formulation of score-based generative models, so one can use a multitude of sampling techniques after theestimation of score functions.

With similar notations in Section 2.1, we let 𝑞(x 0) be the data distribution, and 0 < 𝜎1 < 𝜎2 < · · · < 𝜎𝑡 < · · · < 𝜎𝑇 bea sequence of noise levels. A typical example of SGMs involves perturbing a data point x 0 to x𝑡 by the Gaussian noisedistribution 𝑞(x𝑡 | x 0) = N (x𝑡 ; x 0, 𝜎 2 𝑡 𝐼 ). This yields a sequence of noisy data densities 𝑞(x 1), 𝑞(x 2), · · · , 𝑞(x𝑇 ), where𝑞(x𝑡 ) := ∫ 𝑞(x𝑡 )𝑞(x 0)dx0. A noise-conditional score network is a deep neural network s𝜃 (x, 𝑡) trained to estimate thescore function ∇x𝑡 log 𝑞(x𝑡 ). Learning score functions from data (a.k.a., score estimate) has established techniquessuch as score matching [127], denoising score matching [247, 248, 306], and sliced score matching [284], so we candirectly employ one of them to train our noise-conditional score networks from perturbed data points. For example,with denoising score matching and similar notations in Eq. (10), the training objective is given byE𝑡 ∼U⟦1,𝑇 ⟧,x 0∼𝑞 (x 0 ),x𝑡 ∼𝑞 (x𝑡 |x 0 ) [𝜆(𝑡)𝜎 2 𝑡 ‖ ‖∇x𝑡 log 𝑞(x𝑡 ) − s𝜃 (x𝑡, 𝑡)‖ ‖2] (11)(𝑖 ) = E𝑡 ∼U⟦1,𝑇 ⟧,x 0∼𝑞 (x 0 ),x𝑡 ∼𝑞 (x𝑡 |x 0 ) [𝜆(𝑡)𝜎 2 𝑡 ‖ ‖∇x𝑡 log 𝑞(x𝑡 | x 0) − s𝜃 (x𝑡, 𝑡)‖ ‖2] + const (12)(𝑖𝑖 ) = E𝑡 ∼U⟦1,𝑇 ⟧,x 0∼𝑞 (x 0 ),x𝑡 ∼𝑞 (x𝑡 |x 0 ) [ 𝜆(𝑡) ‖ ‖ ‖ ‖− x𝑡 − x 0 𝜎𝑡 − 𝜎𝑡 s𝜃 (x𝑡 , 𝑡)‖ ‖ ‖ ‖ 2] + const (13)(𝑖𝑖𝑖 ) = E𝑡 ∼U⟦1,𝑇 ⟧,x 0∼𝑞 (x 0 ),𝝐∼N (0,I) [𝜆(𝑡) ∥𝝐 + 𝜎𝑡 s𝜃 (x𝑡 , 𝑡)∥2] + const, (14)where (i) is derived by [306], (i i) is from the assumption that 𝑞(x𝑡 | x 0) = N( x𝑡 ; x 0, 𝜎 2 𝑡 I), and (iii) is from the fact thatx𝑡 = x 0 + 𝜎𝑡 𝝐. Again, we denote by 𝜆(𝑡) a positive weighting function, and “const” a constant that does not dependon the trainable parameter 𝜃 . Comparing Eq. (14) with Eq. (10), it is clear that the training objectives of DDPMs andSGMs are equivalent, once we set 𝝐𝜃 (x, 𝑡) = −𝜎𝑡 s𝜃 (x, 𝑡). Moreover, one can generalize the score matching with higherorder. High-order derivatives of data density provide additional local information about the data distribution. Meng etal. [211] proposes a generalized denoising score matching method to efficiently estimate the high-order score function.

The proposed model can improve the mixing speed of Langevin dynamics and thus the sampling efficiency of diffusionmodels.

For sample generation, SGMs leverage iterative approaches to produce samples from s𝜃 (x,𝑇 ), s𝜃 (x,𝑇 −1), · · · , s𝜃 (x, 0)i n succession. Many sampling approaches exist due to the decoupling of training and inference in SGMs, some of whichare discussed in the next section. Here we introduce the first sampling method for SGMs, called annealed LangevinAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 9dynamics (ALD) [282]. Let 𝑁 be the number of iterations per time step and 𝑠𝑡 > 0 be the step size. We first initializeALD with x(𝑁 ) 𝑇 ∼ N (0, I), then apply Langevin Monte Carlo for 𝑡 = 𝑇 ,𝑇 − 1, · · · , 1 one after the other. At each timestep 0 ≤ 𝑡 < 𝑇 , we start with x(0) 𝑡 = x(𝑁 ) 𝑡 +1 , before iterating according to the following update rule for 𝑖 = 0, 1, · · · , 𝑁 − 1:

𝝐 (𝑖 ) ← N (0, I)x(𝑖+1) 𝑡 ← x(𝑖 ) 𝑡 + 1

## 2𝑠𝑡 s𝜃 (x(𝑖 )

𝑡 , 𝑡) + √ 𝑠𝑡 𝝐 (𝑖 ) .

The theory of Langevin Monte Carlo [229] guarantees that as 𝑠𝑡 → 0 and 𝑁 → ∞, x(𝑁 )

## 0 becomes a valid sample from

the data distribution 𝑞(x 0).

## 2.3 Stochastic Differential Equations (Score SDEs)

DDPMs and SGMs can be further generalized to the case of infinite time steps or noise levels, where the perturbation anddenoising processes are solutions to stochastic differential equations (SDEs). We call this formulation Score SDE [287],a si t leverages SDEs for noise perturbation and sample generation, and the denoising process requires estimating scorefunctions of noisy data distributions.

Score SDEs perturb data to noise with a diffusion process governed by the following stochastic differential equation(SDE) [287]: dx = f (x, 𝑡)d𝑡 + 𝑔(𝑡)d w (15)where f (x, 𝑡) and 𝑔(𝑡) are diffusion and drift functions of the SDE, and wi sa standard Wiener process (a.k.a., Brownianmotion). The forward processes in DDPMs and SGMs are both discretizations of this SDE. As demonstrated in Song etal. (2020) [287], for DDPMs, the corresponding SDE i s:

d x = − 1

## 2 𝛽 (𝑡)x𝑑𝑡 + √︁𝛽 (𝑡)d w (16)

where 𝛽 ( 𝑡 𝑇 ) = 𝑇 𝛽𝑡 as 𝑇 goes to infinity; and for SGMs, the corresponding SDE is given bydx = √︂ d[𝜎 (𝑡)2] d𝑡 d w, (17)where 𝜎 ( 𝑡 𝑇 ) = 𝜎𝑡 as 𝑇 goes to infinity. Here we use 𝑞𝑡 (x) to denote the distribution of x𝑡 in the forward process.

Crucially, for any diffusion process in the form of Eq. (15), Anderson [6] shows that it can be reversed by solving thefollowing reverse-time SDE: dx = [f (x, 𝑡) − 𝑔(𝑡)2∇x log 𝑞𝑡 (x)] d𝑡 + 𝑔(𝑡)d ¯w (18)where ¯w is a standard Wiener process when time flows backwards, and d𝑡 denotes an infinitesimal negative time step.

The solution trajectories of this reverse SDE share the same marginal densities as those of the forward SDE, except thatthey evolve in the opposite time direction [287]. Intuitively, solutions to the reverse-time SDE are diffusion processesthat gradually convert noise to data. Moreover, Song et a l. (2020) [287] prove the existence of an ordinary differentialequation (ODE), namely the probability flow ODE, whose trajectories have the same marginals as the reverse-time SDE.

The probability flow ODE is given b y: dx = [f (x, 𝑡) − 1

## 2𝑔(𝑡)2∇x log 𝑞𝑡 (x)] d𝑡 . (19)

Accepted by ACM Computing Surveys

## 10 Yang et a l.

Both the reverse-time SDE and the probability flow ODE allow sampling from the same data distribution as theirtrajectories have the same marginals.

Once the score function at each time step t, ∇x log 𝑞𝑡 (x), is known, we unlock both the reverse-time SDE (Eq. (18))and the probability flow ODE (Eq. (19)) and can subsequently generate samples by solving them with various numericaltechniques, such as annealed Langevin dynamics [282] (c f ., Section 2.2), numerical SDE solvers [137, 287], numericalODE solvers [142, 190, 279, 287, 377], and predictor-corrector methods (combination of MCMC and numerical ODE/SDEsolvers) [287]. Like in SGMs, we parameterize a time-dependent score model s𝜃 (x𝑡, 𝑡) to estimate the score function bygeneralizing the score matching objective in Eq. (14) to continuous time, leading to the following objective:

E𝑡 ∼U [0,𝑇 ],x 0∼𝑞 (x 0 ),x𝑡 ∼𝑞 (x𝑡 |x 0 ) [𝜆(𝑡) ‖ ‖s𝜃 (x𝑡 , 𝑡) − ∇x𝑡 log 𝑞0𝑡 (x𝑡 | x 0)‖ ‖2] , (20)where U [0,𝑇 ] denotes the uniform distribution over [0,𝑇 ], and the remaining notations follow Eq. (14).

Subsequent research on diffusion models focuses on improving these classical approaches (DDPMs, SGMs, and ScoreSDEs) from three major directions: faster and more efficient sampling, more accurate likelihood and density estimation,and handling data with special structures (such as permutation invariance, manifold structures, and discrete data).

We survey each direction extensively in the next three sections (Sections 3 to 5). In Table 1, we list the three types ofdiffusion models with more detailed categorization, corresponding articles and years, under continuous and discretetime settings.

## 3 DIFFUSION MODELS WITH EFFICIENT SAMPLING

Generating samples from diffusion models typically demands iterative approaches that involve a large number ofevaluation steps. A great deal of recent work has focused on speeding up the sampling process while also improvingquality of the resulting samples. We classify these efficient sampling methods into two main categories: those that donot involve learning (learning-free sampling) and those that require an additional learning process after the diffusionmodel has been trained (learning-based sampling).

## 3.1 Learning-Free Sampling

Many samplers for diffusion models rely on discretizing either the reverse-time SDE present in Eq. (18) or the probabilityflow ODE from Eq. (19). Since the cost of sampling increases proportionally with the number of discretized time steps,many researchers have focused on developing discretization schemes that reduce the number of time steps while alsominimizing discretization errors.

3.1.1 SDE Solvers. The generation process of DDPM [112, 277] can be viewed as a particular discretization of thereverse-time SDE. As discussed in Section 2.3, the forward process of DDPM discretizes the SDE in Eq. (16), whosecorresponding reverse SDE takes the form ofdx = − 1

## 2 𝛽 (𝑡)(x𝑡 − ∇x𝑡 log 𝑞𝑡 (x𝑡 ))d𝑡 + √︁𝛽 (𝑡)d w (21)

Song et a l. (2020) [287] show that the reverse Markov chain defined by Eq. (5) amounts to a numerical SDE solver forEq. (21).

Noise-Conditional Score Networks (NCSNs) [282] and Critically-Damped Langevin Diffusion (CLD) [66] both solvethe reverse-time SDE with inspirations from Langevin dynamics. In particular, NCSNs leverage annealed Langevindynamics (ALD, cf ., Section 2.2) to iteratively generate data while smoothly reducing noise level until the generatedAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 

Table 1. Three types of diffusion models are listed with corresponding articles and years, under continuous and discrete settings.

Primary Secondary Tertiary Article Year SettingEfficient Sampling Learning-Free Sampling SDE Solvers Song et a l. [287] 2020 Continuous Dockhorn et a l. [66] 2021 Continuous Jolicoeur et a l. [138] 2021 Continuous Jolicoeur et a l. [137] 2021 Continuous Chuang et a l. [48] 2022 Continuous Song et a l. [282] 2019 Continuous Karras et a l. [142] 2022 ContinuousODE Solvers Liu et a l. [183] 2021 Continuous Song et a l. [279] 2020 Continuous Zhang et a l. [378] 2022 Continuous Karras et a l. [142] 2022 Continuous Lu et a l. [190] 2022 Continuous Zhang et a l. [377] 2022 ContinuousLearning-Based Sampling Optimized Discretization Watson et a l. [317] 2021 Discrete Watson et a l. [316] 2021 Discrete Dockhorn et a l. [67] 2021 ContinuousKnowledge Distillation Salimans et a l. [262] 2021 Discrete Luhman et a l. [192] 2021 Discrete Meng et a l. [207] 2022 DiscreteTruncated Diffusion Lyu et a l. [201] 2022 Discrete Zheng et a l. [387] 2022 DiscreteImproved Likelihood Noise Schedule Optimization Noise Schedule Optimization Nichol et a l. [216] 2021 Discrete Kingma et a l. [155] 2021 Discrete Huang et a l. [125] 2024 Discrete Yang et a l. [357] 2024 DiscreteReverse Variance Learning Reverse Variance Learning Bao et a l.[13] 2021 Discrete Nichol et a l. [216] 2021 DiscreteExact Likelihood Computation Exact Likelihood Computation Song et a l. [281] 2021 Continuous Huang et a l. [120] 2021 Continuous Song et a l. [287] 2020 Continuous Lu et a l. [189] 2022 ContinuousData with Special Structures Manifold Structures Learned Manifolds Vahdat et a l. [301] 2021 Continuous Yang et a l. [350] 2024 Discrete Ramesh et a l. [245] 2022 Discrete Rombach et a l. [257] 2022 DiscreteKnown Manifolds Bortoli et a l. [56] 2022 Continuous Huang et a l. [119] 2022 ContinuousData with Invariant Structures Data with Invariant Structures Niu et a l. [221] 2020 Discrete Jo et a l. [135] 2022 Continuous Shi et a l. [269] 2022 Continuous Xu et a l. [337] 2021 DiscreteDiscrete Data Discrete Data Meng et a l. [206] 2022 Discrete liu et a l. [186] 2023 Continuous Sohl et a l. [277] 2015 Discrete Austin et a l. [9] 2021 Discrete Yang et a l. [351] 2025 Discrete Wang et a l. [311] 2025 Discrete Campbell et a l. [30] 2022 Continuousdata distribution converges to the original data distribution. Although the sampling trajectories of ALD are not exactsolutions to the reverse-time SDE, they have the correct marginals and hence produce correct samples under theassumption that Langevin dynamics converges to its equilibrium at every noise level. The method of ALD is furtherimproved by Consistent Annealed Sampling (CAS) [138], a score-based MCMC approach with better scaling of timesteps and added noise. Inspired by statistical mechanics, CLD proposes an augmented SDE with an auxiliary velocityterm resembling underdamped Langevin diffusion. To obtain the time reversal of the extended SDE, CLD only needs toAccepted by ACM Computing Surveys

## 12 Yang et a l.

learn the score function of the conditional distribution of velocity given data, arguably easier than learning scores ofdata directly. The added velocity term is reported to improve sampling speed as well as quality.

The reverse diffusion method proposed in [287] discretizes the reverse-time SDE in the same way as the forward one.

For any one-step discretization of the forward SDE, one may write the general form below:

x𝑖+1 = x𝑖 + f𝑖 (x𝑖 ) + g𝑖 z𝑖, 𝑖 = 0, 1, · · · , 𝑁 − 1 (22)where z𝑖 ∼ N (0, I), f𝑖 and g𝑖 are determined by drift/diffusion coefficients of the SDE and the discretization scheme.

Reverse diffusion proposes to discretize the reverse-time SDE similarly to the forward SDE, i.e.,x𝑖 = x𝑖+1 − f𝑖+1 (x𝑖+1) + g𝑖+1g 𝑡 𝑖+1s𝜃 ∗ (x𝑖+1, 𝑡𝑖+1) + g𝑖+1z𝑖 𝑖 = 0, 1, · · · , 𝑁 − 1 (23)where s𝜃 ∗ (x𝑖, 𝑡𝑖 ) is the trained noise-conditional score model. Song et a l. (2020) [287] prove that the reverse diffusionmethod is a numerical SDE solver for the reverse-time SDE in Eq. (18). This process can be applied to any types offorward SDEs, and empirical results indicate this sampler performs slightly better than DDPM [287] for a particulartype of SDEs called the VP-SDE.

Jolicoeur-Martineau et a l. (2021) [137] develop an SDE solver with adaptive step sizes for faster generation. The stepsize is controlled by comparing the output of a high-order SDE solver versus the output of an low-order SDE solver. Ateach time step, the high- and low-order solvers generate new sample x′ high and x′ low from the previous sample x′ 𝑝𝑟𝑒𝑣 respectively. The step size is then adjusted by comparing the difference between the two samples. If x′ high and x′ low are similar, the algorithm will return x′ high and then increase the step size. The similarity between x′ high and x′ low ismeasured b y: 𝐸𝑞 = ‖ ‖ ‖ ‖ ‖ x′ low − x′ high 𝛿 (x′, x′ prev) ‖ ‖ ‖ ‖ ‖

## 2 (24)

where 𝛿 (x′ low, x′ prev) := max(𝜖𝑎𝑏𝑠, 𝜖𝑟𝑒𝑙 max(| x′ low, | x′ prev|)), and 𝜖𝑎𝑏𝑠 and 𝜖𝑟𝑒𝑙 are absolute and relative tolerances.

The predictor-corrector method proposed in [287] solves the reverse SDE by combining numerical SDE solvers(“predictor”) and iterative Markov chain Monte Carlo (MCMC) approaches (“corrector"). At each time step, the predictor-corrector method first employs a numerical SDE solver to produce a coarse sample, followed by a "corrector" thatcorrects the sample’ marginal distribution with score-based MCMC. The resulting samples have the same time-marginalsas solution trajectories of the reverse-time SDE, i.e., they are equivalent in distribution at all time steps. Empiricalresults demonstrate that adding a corrector based on Langevin Monte Carlo is more efficient than using an additionalpredictor without correctors [287]. Karras et a l. (2022) [142] further improve the Langevin dynamics corrector in [287]b y proposing a Langevin-like “churn” step of adding and removing noise, achieving new state-o f-the-art sample qualityon datasets like CIFAR-10 [162] and ImageNet-64 [58].

3.1.2 ODE solvers. A large body of works on faster diffusion samplers are based on solving the probability flow ODE(Eq. (19)) introduced in Section 2.3. In contrast to SDE solvers, the trajectories of ODE solvers are deterministic andthus not affected by stochastic fluctuations. These deterministic ODE solvers typically converge much faster than theirstochastic counterparts at the cost of slightly inferior sample quality.

Denoising Diffusion Implicit Models (DDIM) [279] is one of the earliest work on accelerating diffusion modelsampling. The original motivation was to extend the original DDPM to non-Markovian case with the following MarkovAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 13chain 𝑞(x 1, . . . , x𝑇 | x 0) = 𝑇Ö𝑡 =1 𝑞(x𝑡 | x𝑡 −1, x 0) (25)𝑞𝜎 (x𝑡 −1 | x𝑡, x 0) = N (x𝑡 −1| ˜𝜇𝑡 (x𝑡, x 0), 𝜎 2 𝑡 I) (26)˜𝜇𝑡 (x𝑡, x 0) := √︁ 𝛼𝑡 −1x0 + √︃

## 1 − 𝛼𝑡 −1 − 𝜎 2

𝑡 · x𝑡 − √ 𝛼𝑡 x 0 √1 − 𝛼𝑡 (27)This formulation encapsulates DDPM and DDIM as special cases, where DDPM corresponds to setting 𝜎 2 𝑡 = ˆ𝛽𝑡 −1 ˆ𝛽𝑡 𝛽𝑡 andDDIM corresponds to setting 𝜎 2 𝑡 = 0. DDIM learns a Markov chain to reverse this non-Markov perturbation process,which is fully deterministic when 𝜎 2 𝑡 = 0. It is observed in [142, 190, 262, 279] that the DDIM sampling process amountsto a special discretization scheme of the probability flow ODE. Inspired by an analysis of DDIM on a singleton dataset,generalized Denoising Diffusion Implicit Models (gDDIM) [378] proposes a modified parameterization of the scorenetwork that enables deterministic sampling for more general diffusion processes, such as the one in Critically-DampedLangevin Diffusion (CLD) [66]. PNDM [183] proposes a pseudo numerical method to generate sample along a specificmanifold in R𝑁 . It uses numerical solver with nonlinear transfer part to solve differential equation on manifolds andthen generates sample, which encapsulates DDIM as a special case.

Through extensive experimental investigations, Karras et a l. (2022) [142] show that Heun’s 2𝑛𝑑 order method [8]provides an excellent trade off between sample quality and sampling speed. The higher-order solver leads to smallerdiscretization error at the cost of one additional evaluation of the learned score function per time step. Heun’s methodgenerates samples of comparable, if not better quality than Euler’s method with fewer sampling steps.

Diffusion Exponential Integrator Sampler [377] and DPM-solver [190] leverage the semi-linear structure of probabilityflow ODE to develop customized ODE solvers that are more efficient than general-purpose Runge-Kutta methods.

Specifically, the linear part of probability flow ODE can be analytically computed, while the non-linear part can besolved with techniques similar to exponential integrators in the field of ODE solvers. These methods contain DDIM asa first-order approximation. However, they also allow for higher order integrators, which can produce high-qualitysamples in just 10 to 20 iterations—far fewer than the hundreds of iterations typically required by diffusion modelswithout accelerated sampling.

## 3.2 Learning-Based Sampling

Learning-based sampling is another efficient approach for diffusion models. By using partial steps or training a samplerfor the reverse process, this method achieves faster sampling speeds at the expense of slight degradation in samplequality. Unlike learning-free approaches that use handcrafted steps, learning-based sampling typically involves selectingsteps by optimizing certain learning objectives.

3.2.1 Optimized Discretization. Given a pre-trained diffusion model, Watson et a l. (2021) [317] put forth a strategyfor finding the optimal discretization scheme by selecting the best 𝐾 time steps to maximize the training objective forDDPMs. Key to this approach is the observation that the DDPM objective can be broken down into a sum of individualterms, making it well suited for dynamic programming. However, it is well known that the variational lower boundused for DDPM training does not correlate directly with sample quality [296]. A subsequent work, called DifferentiableDiffusion Sampler Search [316], addresses this issue by directly optimizing a common metric for sample quality calledthe Kernel Inception Distance (KID) [22]. This optimization is feasible with the help of reparameterization [157, 254]Accepted by ACM Computing Surveys

## 14 Yang et a l.

and gradient rematerialization. Based on truncated Taylor methods, Dockhorn et a l. (2022) [67] derive a second-ordersolver for accelerating synthesis by training a additional head on top of the first-order score network.

3.2.2 Truncated Diffusion. One can improve sampling speed by truncating the forward and reverse diffusion processes[201, 387]. The key idea is to halt the forward diffusion process early o n, after just a few steps, and to begin thereverse denoising process with a non-Gaussian distribution. Samples from this distribution can be obtained efficientlyby diffusing samples from pre-trained generative models, such as variational autoencoders [157, 254] or generativeadversarial networks [88].

3.2.3 Knowledge Distillation. Approaches that use knowledge distillation [192, 207, 262] can significantly improve thesampling speed of diffusion models. Specifically, in Progressive Distillation [262], the authors propose distilling the fullsampling process into a faster sampler that requires only half as many steps. By parameterizing the new sampler as adeep neural network, authors are able to train the sampler to match the input and output of the DDIM sampling process.

Repeating this procedure can further reduce sampling steps, although fewer steps can result in reduced sample quality.

To address this issue, the authors suggest new parameterizations for diffusion models and new weighting schemes forthe objective function.

## 4 DIFFUSION MODELS WITH IMPROVED LIKELIHOOD

As discussed in Section 2.1, the training objective for diffusion models is a (negative) variational lower bound (VLB)o n the log-likelihood. This bound, however, may not be tight in many cases [155], leading to potentially suboptimallog-likelihoods from diffusion models. In this section, we survey recent works on likelihood maximization for diffusionmodels. We focus on three types of methods: noise schedule optimization, reverse variance learning, and exact log-likelihood evaluation.

## 4.1 Noise Schedule Optimization

In the classical formulation of diffusion models, noise schedules in the forward process are handcrafted without trainableparameters. By optimizing the forward noise schedule jointly with other parameters of diffusion models, one can furthermaximize the VLB in order to achieve higher log-likelihood values [155, 216].

The work of iDDPM [216] demonstrates that a certain cosine noise schedule can improve log-likelihoods. Specifically,the cosine noise schedule in their work takes the form o f¯𝛼𝑡 = ℎ(𝑡) ℎ(0) , ℎ(𝑡) = cos ( 𝑡/𝑇 + 𝑚

## 1 + 𝑚 · 𝜋

2 ) 2 (28)where ¯𝛼𝑡 and 𝛽𝑡 are defined in Eqs. (2) and (3), and 𝑚 is a hyperparameter to control the noise scale at 𝑡 = 0. They alsopropose a parameterization of the reverse variance with an interpolation between 𝛽𝑡 and 1 − ¯𝛼𝑡 in the log domain.

In Variational Diffusion Models (VDMs) [155], authors propose to improve the likelihood of continuous-timediffusion models by jointly training the noise schedule and other diffusion model parameters to maximize the VLB.

They parameterize the noise schedule using a monotonic neural network 𝛾𝜂 (𝑡), and build the forward perturbationprocess according to 𝜎 2 𝑡 = sigmoid(𝛾𝜂 (𝑡)), 𝑞(x𝑡 | x 0) = N ( ¯𝛼𝑡 x 0, 𝜎 2 𝑡 I), and ¯𝛼𝑡 = √︁(1 − 𝜎 2 𝑡 ). Moreover, authors provethat the VLB for data point x can be simplified to a form that only depends on the signal-t o-noise ratio R(𝑡) := ¯𝛼 2 𝑡 𝜎 2 𝑡 . Inparticular, the 𝐿𝑉 𝐿𝐵 can be decomposed to𝐿𝑉 𝐿𝐵 = −Ex0 KL(𝑞(x𝑇 |x 0) || 𝑝 (x𝑇 )) + Ex0,x 1 log 𝑝 (x 0|x 1) − 𝐿𝐷, (29)Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 15where the first and second terms can be optimized directly in analogy to training variational autoencoders. The thirdterm can be further simplified to the following:

𝐿𝐷 = 1

## 2 Ex0,𝜖∼N (0,I)

∫ RmaxRmin ∥x 0 − ˜x𝜃 (x𝑣, 𝑣) ∥2

## 2 𝑑𝑣, (30)

where Rmax = 𝑅(1), Rmin = 𝑅(𝑇 ), x𝑣 = ¯𝛼𝑣x0 + 𝜎𝑣𝜖 denotes a noisy data point obtained by diffusing x 0 with the forwardperturbation process until 𝑡 = 𝑅 −1 (𝑣), and ˜x𝜃 denotes the predicted noise-free data point by the diffusion model. As aresult, noise schedules do not affect the VLB as long as they share the same values at Rmin and Rmax, and will only affectthe variance of Monte Carlo estimators for VLB.

Another line of works [124, 357] propose to the modify diffusion trajectory through the integration of cross-modalityinformation. Specifically, the cross-modal information, denoted as 𝑟𝜙 (𝑦, 𝑥0), is extracted from any conditional input 𝑦and original sample 𝑥0 with relational network 𝑟𝜙 (·). And then it can be injected to the forward process as an additionalbias to adapt diffusion trajectory: 𝑞𝑡 (𝑥𝑡 |𝑥0, 𝑦) = N (𝑥𝑡, √ ¯𝛼𝑡𝑥0 + 𝑘𝑡𝑟𝜙 (𝑥0, 𝑦), (1 − ¯𝛼𝑡 )𝐼 ) (31)where 𝑘𝑡 is a non-negative scalar that control the magnitude of the bias term. It is important to note that with thismodification, the forward process ceases to b ea Markovian chain. ContextDiff [357] introduces a general frameworkto jointly learn the cross-modal relational network 𝑟𝜙 and the diffusion model, and derives the VLB and samplingprocedure for this modified diffusion process.

## 4.2 Reverse Variance Learning

The classical formulation of diffusion models assumes that Gaussian transition kernels in the reverse Markov chainhave fixed variance parameters. Recall that we formulated the reverse kernel as 𝑞𝜃 (x𝑡 −1 | x𝑡 ) = N (𝜇𝜃 (x𝑡 , 𝑡), Σ𝜃 (x𝑡, 𝑡))i n Eq. (5) but often fixed the reverse variance Σ𝜃 (x𝑡, 𝑡) to 𝛽𝑡 I. Many methods propose to train the reverse variances aswell to further maximize VLB and log-likelihood values.

In iDDPM [216], Nichol and Dhariwal propose to learn the reverse variances by parameterizing them with a formof linear interpolation and training them using a hybrid objective. This results in higher log-likelihoods and fastersampling without losing sample quality. In particular, they parameterize the reverse variance in Eq. (5) a s:

Σ𝜃 (x𝑡 , 𝑡) = exp(𝜃 · log 𝛽𝑡 + (1 − 𝜃 ) · log ˜𝛽𝑡 ), (32)where ˜𝛽𝑡 := 1− ¯𝛼𝑡 −1 1− ¯𝛼𝑡 · 𝛽𝑡 and 𝜃 is jointly trained to maximize VLB. This simple parameterization avoids the instability ofestimating more complicated forms of Σ𝜃 (x𝑡, 𝑡) and is reported to improve likelihood values.

Analytic-DPM [13] shows a remarkable result that the optimal reverse variance can be obtained from a pre-trainedscore function, with the analytic form below:

Σ𝜃 (x𝑡, 𝑡) = 𝜎 2 𝑡 + ©   « √︄ 𝛽𝑡 𝛼𝑡 − √︃𝛽𝑡 −1 − 𝜎 2 𝑡 ª ® ¬ 2 · (1 − 𝛽𝑡 E𝑞𝑡 (x𝑡 ) ||∇x𝑡 log 𝑞𝑡 (x𝑡 )||2𝑑 ) (33)As a result, given a pre-traied score model, we can estimate its first- and second-order moments to obtain the optimalreverse variances. Plugging them into the VLB can lead to tighter VLBs and higher likelihood values.

Accepted by ACM Computing Surveys

## 16 Yang et a l.

## 4.3 Exact Likelihood Computation

In the Score SDE [287] formulation, samples are generated by solving the following reverse SDE, where ∇x𝑡 log 𝑝𝜃 (x𝑡 , 𝑡)i n Eq. (18) is replaced by the learned noise-conditional score model s𝜃 (x𝑡, 𝑡):

d x = 𝑓 (x𝑡, 𝑡) − 𝑔(𝑡)2s𝜃 (x𝑡 , 𝑡)d𝑡 + 𝑔(𝑡)d w. (34)Here we use 𝑝sde 𝜃 to denote the distribution of samples generated by solving the above SDE. One can also generate databy plugging the score model into the probability flow ODE in Eq. (19), which gives:

dx𝑡 d𝑡 = 𝑓 (x𝑡 , 𝑡) − 1

## 2𝑔2 (𝑡)s𝜃 (x𝑡 , 𝑡)

︸                           ︷︷                           ︸:= ˜𝑓𝜃 (x𝑡 ,𝑡 ) (35)Similarly, we use 𝑝ode 𝜃 to denote the distribution of samples generated via solving this ODE. The theory of neuralODEs [40] and continuous normalizing flows [92] indicates that 𝑝ode 𝜃 can be computed accurately albeit with highcomputational cost. For 𝑝sde 𝜃 , several concurrent works [120, 189, 281] demonstrate that there exists an efficientlycomputable variational lower bound, and we can directly train our diffusion models to maximize 𝑝sde 𝜃 using modifieddiffusion losses.

Specifically, Song et a l. (2021) [281] prove that with a special weighting function (likelihood weighting), the objectiveused for training score SDEs implicitly maximizes the expected value of 𝑝sde 𝜃 on data. It is shown thatD𝐾𝐿 (𝑞0 ∥ 𝑝sde 𝜃 ) ≤ L (𝜃 ; 𝑔(·)2) + D𝐾𝐿 (𝑞𝑇 ∥ 𝜋), (36)where L (𝜃 ; 𝑔(·)2) is the Score SDE objective in Eq. (20) with 𝜆(𝑡) = 𝑔(𝑡)2. Since D𝐾𝐿 (𝑞0 ∥ 𝑝sde 𝜃 ) = −E𝑞0 log(𝑝sde 𝜃 ) +const,and D𝐾𝐿 (𝑞𝑇 ∥ 𝜋) is a constant, training with L (𝜃 ; 𝑔(·)2) amounts to minimizing −E𝑞0 log(𝑝sde 𝜃 ), the expected negativelog-likelihood on data. Moreover, Song et a l. (2021) and Huang et a l. (2021) [120, 281] provide the following bound for𝑝sde 𝜃 (x): − log 𝑝sde 𝜃 (x) ≤ L′ (x), (37)where L′ (x) is defined byL′ (x) := ∫ 𝑇

## 0 E [ 1

## 2 ||𝑔(𝑡)s𝜃 (x𝑡 , 𝑡)||2 + ∇ · (𝑔(𝑡)2s𝜃 (x𝑡 , 𝑡) − 𝑓 (x t), t) |

| | | x 0 = x] 𝑑𝑡 − Ex𝑇 [log 𝑝sde 𝜃 (x𝑇 ) | x 0 = 𝑥] (38)The first part of Eq. (38) is reminiscent of implicit score matching [127] and the whole bound can be efficiently estimatedwith Monte Carlo methods.

Since the probability flow ODE is a special case of neural ODEs or continuous normalizing flows, we can usewell-established approaches in those fields to compute log 𝑝ode 𝜃 accurately. Specifically, we havelog 𝑝ode 𝜃 (x 0) = log 𝑝𝑇 (x𝑇 ) + ∫ 𝑇𝑡 =0 ∇ · ˜𝑓𝜃 (x𝑡 , 𝑡)d𝑡 . (39)One can compute the one-dimensional integral above with numerical ODE solvers and the Skilling-Hutchinson traceestimator [126, 276]. Unfortunately, this formula cannot be directly optimized to maximize 𝑝ode 𝜃 on data, as it requirescalling expensive ODE solvers for each data point x 0. To reduce the cost of directly maximizing 𝑝ode 𝜃 with the aboveformula, Song et a l. (2021) [281] propose to maximize the variational lower bound of 𝑝sde 𝜃 as a proxy for maximizing𝑝ode 𝜃 , giving rise to a family of diffusion models called ScoreFlows.

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 17Lu et a l. (2022) [189] further improve ScoreFlows by proposing to minimize not just the vanilla score matchingloss function, but also its higher order generalizations. They prove that log 𝑝ode 𝜃 can be bounded with the first, second,and third-order score matching errors. Building upon this theoretical result, authors further propose efficient trainingalgorithms for minimizing high order score matching losses and reported improved 𝑝ode 𝜃 on data.

## 5 DIFFUSION MODELS FOR DATA WITH SPECIAL STRUCTURES

While diffusion models have achieved great success for data domains like images and audio, they do not necessarilytranslate seamlessly to other modalities. Many important data domains have special structures that must be taken intoaccount for diffusion models to function effectively. Difficulties may arise, for example, when models rely on scorefunctions that are only defined on continuous data domains, or when data reside on low dimensional manifolds. Tocope with these challenges, diffusion models have to be adapted in various ways.

## 5.1 Discrete Data

Most diffusion models are geared towards continuous data domains, because Gaussian noise perturbation as used inDDPMs is not a natural fit for discrete data, and the score functions required by SGMs and Score SDEs are only definedon continuous data domains. To overcome this difficulty, several works [9, 98, 118, 330] build on Sohl-Dickstein et a l.(2015) [277] to generate discrete data of high dimensions. Specifically, VQ-Diffusion [98] replaces Gaussian noise with arandom walk on the discrete data space, or a random masking operation. The resulting transition kernel for the forwardprocess takes the form of 𝑞(x𝑡 | x𝑡 −1) = v ⊤ (x𝑡 )Q𝑡 v( x𝑡 −1) (40)where v( x) is a one-hot column vector, and Q𝑡 is the transition kernel of a lazy random walk.

Building on discrete diffusion principles, MMaDA [351] introduces a novel approach for multi-modal discretedata generation by incorporating masked autoencoder architectures within the diffusion framework. This methoddemonstrates improved performance on complex discrete sequences by leveraging cross-modal information. Similarly,TraceRL [311] extends discrete diffusion models to more comprehensive reinforcement learning settings, where thediscrete state-action sequences are generated through a specialized diffusion process that incorporates reward signalsand policy constraints.

D3PM [9] accommodates discrete data in diffusion models by constructing the forward noising process with absorbingstate kernels or discretized Gaussian kernels. Campbell et a l. (2022) [30] present the first continuous-time framework fordiscrete diffusion models. Leveraging Continuous Time Markov Chains, they are able to derive efficient samplers thatoutperform discrete counterparts, while providing a theoretical analysis on the error between the sample distributionand the true data distribution.

Concrete Score Matching (CSM) [206] proposes a generalization of the score function for discrete random variables.

Concrete score is defined by the rate of change of the probabilities with respect to directional changes of the input,which can be seen as a finite-difference approximation to the continuous (Stein) score. The concrete score can beefficiently trained and applied to MCMC.

Based on the theory of stochastic calculus, Liu et a l. (2023) [186] proposes a framework for diffusion models togenerate data on constrained and structured domains, including discrete data as a special case. Using a fundamentaltheorem in stochastic calculus, the Doob’s h-transform, one can constrain the data distribution on a specific area byAccepted by ACM Computing Surveys

## 18 Yang et a l.

including a special force term in the reverse diffusion process. They use a parameterization of the force term with anEM-based optimization algorithm. Furthermore, the loss function can be transformed to 𝐿2 loss using Girsanov theorem.

Inspired by the success of reinforcement learning in discrete language modeling [101], recent works like MMaDA[351] and TraceRL [311] learn to model discrete textual and visual data with masked diffusion model revolutionize

## 5.2 Data with Invariant Structures

Data in many important domains have invariant structures. For example, graphs are permutation invariant, and pointclouds are both translation and rotation invariant. In diffusion models, these invariances are often ignored, which canlead to suboptimal performance. To address this issue, several works [56, 221] propose to endow diffusion models withthe ability to account for invariance in data.

Niu et a l. (2020) [221] first tackle the problem of permutation invariant graph generation with diffusion models. Theyachieve this by using a permutation equivariant graph neural network [89, 267, 326], called EDP-GNN, to parameterizethe noise-conditional score model. GDSS [135] further develops this idea by proposing a continuous-time graph diffusionprocess. This process models both the joint distribution of nodes and edges through a system of stochastic differentialequations (SDEs), where message-passing operations are used to guarantee permutation invariance.

Similarly, Shi et a l. (2021) [269] and Xu et a l. (2022) [337] enable diffusion models to generate molecular conformationsthat are invariant to both translation and rotation. For example, Xu et a l. (2022) [337] shows that Markov chains startingwith an invariant prior and evolving with equivariant Markov kernels can induce an invariant marginal distribution,which can be used to enforce appropriate data invariance in molecular conformation generation. Formally, let T be arotation or translation operation. Given that 𝑝 (x𝑇 ) = 𝑝 (T (x𝑇 )), 𝑝𝜃 (x𝑡 −1 | x𝑡 ) = 𝑝𝜃 (T (x𝑡 −1) | T (x𝑡 )), Xu et a l. (2022)[337] prove that the distribution of samples is guaranteed to be invariant to T , that i s, 𝑝0 (x) = 𝑝0 (T (x)). As a result,one can build a diffusion model that generates rotation and translation invariant molecular conformations as long asthe prior and transition kernels enjoy the same invariance.

## 5.3 Data with Manifold Structures

Data with manifold structures are ubiquitous in machine learning. As the manifold hypothesis [76] posits, naturaldata often reside on manifolds with lower intrinsic dimensionality. In addition, many data domains have well-knownmanifold structures. For instance, climate and earth data naturally lie on the sphere because that is the shape of ourplanet. Many works have focused on developing diffusion models for data on manifolds. We categorize them based onwhether the manifolds are known or learned, and introduce some representative works below.

5.3.1 Known Manifolds. Recent studies have extended the Score SDE formulation to various known manifolds. Thisadaptation parallels the generalization of neural ODEs [40] and continuous normalizing flows [92] to Riemannianmanifolds [188, 203]. To train these models, researchers have also adapted score matching and score functions toRiemannian manifolds.

The Riemannian Score-Based Generative Model (RSGM) [56] accommodates a wide range of manifolds, includingspheres and toruses, provided they satisfy mild conditions. The RSGM demonstrates that it is possible to extend diffusionmodels to compact Riemannian manifolds. The model also provides a formula for reversing diffusion on a manifold.

Taking an intrinsic view, the RSGM approximates the sampling process on Riemannian manifolds using a GeodesicRandom Walk. It is trained with a generalized denoising score matching objective.

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 19In contrast, the Riemannian Diffusion Model (RDM) [119] employs a variational framework to generalize thecontinuous-time diffusion model to Riemannian manifolds. The RDM uses a variational lower bound (VLB) of thelog-likelihood as its loss function. The authors of the RDM model have shown that maximizing this VLB is equivalentto minimizing a Riemannian score-matching loss. Unlike the RSGM, the RDM takes an extrinsic view, assuming thatthe relevant Riemannian manifold is embedded in a higher dimensional Euclidean space.

5.3.2 Learned Manifolds. According to the manifold hypothesis [76], most natural data lies on manifolds with sig-nificantly reduced intrinsic dimensionality. Consequently, identifying these manifolds and training diffusion modelsdirectly on them can be advantageous due to the lower data dimensionality. Many recent works have built on thisidea, starting by using an autoencoder to condense the data into a lower dimensional manifold, followed by trainingdiffusion models in this latent space. In these cases, the manifold is implicitly defined by the autoencoder and learnedthrough the reconstruction loss. In order to be successful, it is crucial to design a loss function that allows for the jointtraining of the autoencoder and the diffusion models.

The Latent Score-Based Generative Model (LSGM) [301] seeks to address the problem of joint training by pairing aScore SDE diffusion model with a variational autoencoder (VAE) [157, 254]. In this configuration, the diffusion model isresponsible for learning the prior distribution. The authors of the LSGM propose a joint training objective that mergesthe VAE’s evidence lower bound with the diffusion model’s score matching objective. This results in a new lower boundfor the data log-likelihood. By situating the diffusion model within the latent space, the LSGM achieves faster samplegeneration than conventional diffusion models. Additionally, the LSGM can manage discrete data by converting it intocontinuous latent codes.

Rather than jointly training the autoencoder and diffusion model, the Latent Diffusion Model (LDM) [257] addresseseach component separately. First, an autoencoder is trained to produce a low-dimensional latent space. Then, thediffusion model is trained to generate latent codes. DALLE-2 [245] employs a similar strategy by training a diffusionmodel on the CLIP image embedding space, followed by training a separate decoder to create images based on the CLIPimage embeddings.

Structure-guided Adversarial training of Diffusion Models (SADMs) [350], for the first time, propose to utilize thestructural information within the sample batch. Specifically, SADMs incorporate an adversarially-trained structuraldiscriminator to enforce the preservation of manifold structure among samples within each training batch. This approachleverages the intrinsic data manifold to facilitate the generation of realistic samples, thereby significantly advancing thecapabilities of previous diffusion models in tasks such as image synthesis and cross-domain fine-tuning.

## 6 CONNECTIONS WITH OTHER GENERATIVE MODELS

In this section, we first introduce five other important classes of generative models and analyze their advantages andlimitations. Then we introduce how diffusion models are connected with them, and illustrate how these generativemodels are promoted by incorporating diffusion models. The algorithms that integrate diffusion models with othergenerative models are summarized in Table 2, and we also provide a schematic illustration in Fig. 3.

## 6.1 Large Language Models and Connections with Diffusion Models

Large Language Models (LLMs) [1, 7, 27, 130, 312, 352, 354, 396] have profoundly impacted the AI community, showcasingadvanced language comprehension and reasoning abilities. Recent works have begun extending these impressiveAccepted by ACM Computing Surveys

## 20 Yang et a l.

Table 2. Diffusion models are incorporated into different generative models.

Model Article YearLarge Language Model Zhang et a l. [381] 2024 Yang et a l. [351] 2025 Yang et a l. [353] 2024 Tian et a l. [297] 2024 Wang et a l. [311] 2025 Zeng et a l. [372] 2024Variational Auto-Encoder Luo et a l. [193] 2022 Hunag et a l. [120] 2021 Vadhat et a l. [301] 2021Generative Adversarial Network Wang et a l. [315] 2022 Yang et a l. [350] 2021Normalizing Flow Zhang et a l.[376] 2021 Gong et a l. [87] 2021 kim et a l. [151] 2022 Wang et a l. [309] 2024 Yang et a l. [359] 2024Autoregressive Model Meng et a l.[212] 2020 Meng et a l.[210] 2021 Hoogeboom et a l.[117] 2021 Rasul et a l. [249] 2021Energy-based Model Gao et a l. [83] 2021 Yu et a l. [367] 2022reasoning capabilities to visual generative tasks and overall generation planning, establishing powerful synergiesbetween language understanding and multimodal generation.

The collaboration between LLMs [35, 223, 354] and diffusion models [20, 245, 349, 357] can significantly improvetext-image alignment and the quality of generated content [178, 297, 381]. For instance, RealCompo [381] utilizes LLMsto enhance the compositional generation of diffusion models by generating images grounded on bounding box layoutsfrom the LLM. EditWorld [355] composes a set of LLMs and pretrained diffusion models to generate an image editingdataset containing numerous instructions with world knowledge [102]. VideoTetris [297] leverages LLMs to decomposetext prompts along the temporal axis for guiding video generation with smoother and more reasonable transitions.

Beyond traditional text-t o-image synthesis, recent advances have explored more sophisticated integrations. MMaDA[351] demonstrates how LLMs can guide multi-modal discrete diffusion processes, where the language model’s under-standing of cross-modal relationships enhances the generation of complex discrete sequences across different modalities.

In reinforcement learning contexts, TraceRL [311] combines the reasoning capabilities of LLMs with discrete diffusionmodels to generate optimal action sequences, where the language model provides high-level policy guidance while thediffusion process handles fine-grained action sampling.

Notably, RPG [353] leverages the vision-language prior of multimodal LLMs to reason out complementary spatiallayouts from text prompts, manipulating object compositions for diffusion models in both text-guided image generationand editing processes, achieving state-o f-the-art performance in compositional synthesis scenarios.

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 21These developments highlight a broader trend toward intelligent generation systems where LLMs serve as high-levelplanners and coordinators, while diffusion models handle the detailed generation process, creating more controllableand semantically coherent outputs across various domains and modalities.

## 6.2 Variational Autoencoders and Connections with Diffusion Models

Variational Autoencoders [68, 158, 254] aim to learn both an encoder and a decoder to map input data to values ina continuous latent space. In these models, the embedding can be interpreted as a latent variable in a probabilisticgenerative model, and a probabilistic decoder can be formulated by a parameterized likelihood function. In addition,the data xi s assumed to be generated by some unobserved latent variable z using conditional distribution 𝑝𝜃 (x | z),and 𝑞𝜙 (z | x) is used to approximately inference z. To guarantee an effective inference, a variational Bayes approach isused to maximize the evidence lower bound:

L (𝜙, 𝜃 ; x) = E𝑞 (z|x) [log 𝑝𝜃 (x, z) − log 𝑞𝜙 (z | x)] (41)with L (𝜙, 𝜃 ; x) ≤ log 𝑝𝜃 (x). Provided that the parameterized likelihood function 𝑝𝜃 (x | z) and the parameterizedposterior approximation 𝑞𝜙 (z | x) can be computed in a point-wise way and are differentiable with their parameters,the ELBO can be maximized with gradient descent. This formulation allows flexible choices of encoder and decodermodels. Typically, these models are represented by exponential family distributions whose parameters are generated bymulti-layer neural networks.

The DDPM can be conceptualized as a hierarchical Markovian VAE with a fixed encoder. Specifically, DDPM’sforward process functions as the encoder, and this process is structured as a linear Gaussian model (a s described byEq. (2)). The DDPM’s reverse process, on the other hand, corresponds to the decoder, which is shared across multipledecoding steps. The latent variables within the decoder are all the same size as the sample data.

Fig. 3. Illustrations of works incorporating diffusion models with other generative models, such as : LLM [353] where a diffusion model is guided by the LLM planning, VAE [257] where a diffusion model is applied on a latent space, GAN [315] where noise is injected to the discriminator input, normalizing flow [376] where noise is injected in both forward and backward processes in the flow, autoregressive model [117] where the training objective is similar to diffusion models, and EBM [83] where a sequence of EBMs is learned by diffusion recovery likelihood. Accepted by ACM Computing Surveys

## 22 Yang et a l.

In a continuous-time setting, Song et a l. (2021) [287], Huang et a l. (2021) [120], and Kingma et a l. (2021) [155]demonstrate that the score matching objective may be approximated by the Evidence Lower Bound (ELBO) of a deephierarchical VAE. Consequently, optimizing a diffusion model can be seen as training an infinitely deep hierarchicalVAE—a finding that supports the common belief that Score SDE diffusion models can be interpreted as the continuouslimit of hierarchical VAEs.

The Latent Score-Based Generative Model (LSGM) [301] furthers this line of research by illustrating that the ELBOcan be considered a specialized score matching objective in the context of latent space diffusion. Though the cross-entropy term in the ELBO is intractable, it can be transformed into a tractable score matching objective by viewing thescore-based generative model as an infinitely deep VAE.

## 6.3 Generative Adversarial Networks and Connections with Diffusion Models

Generative Adversarial Networks (GANs) [51, 88, 100] mainly consist of two models: a generator 𝐺 and a discriminator 𝐷.

These two models are typically constructed by neural networks but could be implemented in any form of a differentiablesystem that maps input data from one space to another. The optimization of GANs can be viewed as a mini-maxoptimization problem with value function 𝑉 (𝐺, 𝐷):

min 𝐺 max 𝐷 Ex∼𝑝data (x) [log 𝐷 (x)] + Ez∼𝑝z (z) [log(1 − 𝐷 (𝐺 (z)))]. (42)The generator 𝐺 aims to generate new examples and implicitly model the data distribution. The discriminator 𝐷 isusually a binary classifier that is used to identify generated examples from true examples with maximally possibleaccuracy. The optimization process ends at a saddle point that produces a minimum about the generator and a maximumabout the discriminator. Namely, the goal of GAN optimization is to achieve Nash equilibrium [252]. At that point, thegenerator can be considered that it has captured the accurate distribution of real examples.

One of the issues of GAN is the instability in the training process, which is mainly caused by the non-overlappingbetween the distribution of input data and that of the generated data. One solution is to inject noise into the discriminatorinput for widening the support of both the generator and discriminator distributions. Taking advantage of the flexiblediffusion model, Wang et a l. (2022) [315] inject noise to the discriminator with an adaptive noise schedule determinedby a diffusion model. On the other hand, GAN can facilitate sampling speed of diffusion models. Xiao et a l. (2021)[328] show that slow sampling is caused by the Gaussian assumption in the denoising step, which is justified only forsmall step sizes. As such, each denoising step is modeled by a conditional GAN, allowing larger step size. To ensurethe diffusion model captures authentic manifold structures in the data distribution, SADM [350] advocates adversarialtraining of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing realmanifold structures from the generated ones.

## 6.4 Normalizing Flows and Connections with Diffusion Models

Normalizing flows [63, 253] are generative models that generate tractable distributions to model high-dimensionaldata [65, 156]. Normalizing flows can transform simple probability distribution into an extremely complex probabilitydistribution, which can be used in generative models, reinforcement learning, variational inference, and other fields.

Existing normalizing flows are constructed based on the change of variable formula [63, 253]. The trajectory innormalizing flows is formulated by a differential equation. In the discrete-time setting, the mapping from data x tolatent zi n normalizing flows is a composition of a sequence of bijections, taking the form of 𝐹 = 𝐹𝑁 ◦ 𝐹𝑁 −1 ◦ . . . ◦ 𝐹1.

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 23The trajectory {x 1, x 2, . . . x𝑁 } in normalizing flows satisfies :

x𝑖 = 𝐹𝑖 (x𝑖 −1, 𝜃 ), x𝑖 −1 = 𝐹 −1 𝑖 (x𝑖, 𝜃 ) (43)for all 𝑖 ≤ 𝑁 .

Similar to the continuous setting, normalizing flows allow for the retrieval of the exact log-likelihood through achange of variable formula. However, the bijection requirement limits the modeling of complex data in both practicaland theoretical contexts [50, 321]. Several works attempt to relax this bijection requirement [65, 321]. For example,DiffFlow [376] introduces a generative modeling algorithm that combines the benefits of both flow-based and diffusionmodels. As a result, DiffFlow produces sharper boundaries than normalizing flow and learns more general distributionswith fewer discretization steps compared to diffusion probabilistic models. Implicit Nonlinear Diffusion Model (INDM)[151] optimizes the pre-encoding process of latent diffusion, which first encodes the original data into the latent spaceusing normalizing flow, and then performs diffusion in the latent space. Using a non-linear diffusion process, INDM caneffectively improve the likelihood and the sampling speed.

To scale up the training of CNFs, recent works propose efficient simulation-free approaches [2, 180, 185] by p a-rameterizing a vector field which flows from noise samples to data samples. Lipman et a l. (2022) [180] propose FlowMatching (FM) to train CNFs based on constructing explicit conditional probability paths between the noise distributionand each data sample. Wang et a l. (2024) [309] conduct an i n-depth analysis of the essence of rectification in rectifiedflow [185] and extend it to rectified diffusion. Besides, they identify that it is not straightness but first-order propertyis the essential training target of rectified diffusion with theoretical derivations. Yang et a l. (2024) further proposeConsistency Flow Matching [359], a novel FM method that explicitly enforces self-consistency in the velocity field.

Consistency Flow Matching [359] directly defines straight flows starting from different times to the same endpoint,imposing constraints on their velocity values:

L𝜃 = 𝐸𝑡 ∼U𝐸𝑥𝑡 ,𝑥𝑡 +Δ𝑡 ||𝑓𝜃 (𝑡, 𝑥𝑡 ) − 𝑓𝜃 − (𝑡 + Δ𝑡, 𝑥𝑡 +Δ𝑡 )||2

## 2 + 𝛼 ||𝑣𝜃 (𝑡, 𝑥𝑡 ) − 𝑣𝜃 − (𝑡 + Δ𝑡, 𝑥𝑡 +Δ𝑡 )||2

2,𝑓𝜃 (𝑡, 𝑥𝑡 ) = 𝑥𝑡 + (1 − 𝑡) ∗ 𝑣𝜃 (𝑡, 𝑥𝑡 ), (44)where U is the uniform distribution on [0, 1 − Δ𝑡], 𝛼 is a positive scalar, Δ𝑡 denotes a time interval which is a small andpositive scalar. 𝜃 − denotes the running average of past values of 𝜃 using exponential moving average (EMA), 𝑥𝑡 and𝑥𝑡 +Δ𝑡 follows a pre-defined distribution which can be efficiently sampled, for example, VP-SDE [112] or OT path [180].

In this way, Consistency Flow Matching [359] innovatively bridges consistency models and flow matching modelsthrough the novel concept of straight flows characterized by velocity consistency.

## 6.5 Autoregressive Models and Connections with Diffusion Models

Autoregressive Models (ARMs) work by decomposing the joint distribution of data into a product of conditionaldistributions using the probability chain rule:

log 𝑝 (x 1:𝑇 ) = 𝑇∑︁𝑡 =1 log 𝑝 (𝑥𝑡 | x<𝑡 ) (45)where x<𝑡 is a shorthand for 𝑥1, 𝑥2, . . . , 𝑥𝑡 −1 [17, 164]. Recent advances in deep learning have facilitated significantprogress for various data modalities [34, 209, 266], such as images [45, 304], audio [141, 303], and text [18, 27, 95, 205, 213].

Autoregressive models (ARMs) offer generative capabilities through the use of a single neural network. Sampling fromAccepted by ACM Computing Surveys

## 24 Yang et a l.

these models requires the same number of network calls as the data’s dimensionality. While ARMs are effective densityestimators, sampling is a continuous, time-consuming process—particularly for high-dimensional data.

The Autoregressive Diffusion Model (ARDM) [117], on the other hand, is capable of generating arbitrary-order data,including order-agnostic autoregressive models and discrete diffusion models as special cases [9, 118, 278]. Insteadof using causal masking on representations like ARMs, the ARDM is trained with an effective objective that mirrorsthat of diffusion probabilistic models. At the testing stage, the ARDM is able to generate data in parallel—enabling itsapplication to a range of arbitrary-generation tasks.

Ment et a l.(2021) [210] incorporates randomized smoothing into autoregressive generative modeling, in order toimprove the sample quality. The original data distribution is smoothed by convolving it with a smooth distribution, e.g.,a Gaussian or Laplacian kernel. The smoothed data distribution is learned by autoregressive model, and then the learneddistribution is denoised by either applying gradient-based denoising approach or introducing another conditionalautoregressive model. By choosing the level of smoothness appropriately, the proposed method can improve the samplequality of existing autoregressive models while retaining reasonable likelihoods.

On the other hand, Autoregressive conditional score models (AR-CSM) [212] proposes a score matching methodto model the conditional distribution of autoregressive model. The score function of conditional distribution, i.e.,∇𝑥𝑡 log 𝑝 (𝑥𝑡 | x<𝑡 ), does not need to be normalized and thus one can use more flexible and advanced neural networksin the model. Furthermore, the univariate conditional score function can be efficiently estimated, even though thedimension of original data might be very high. For inference, AR-CSM uses Langevin dynamics that only need thescore function to sample from a density.

## 6.6 Energy-based Models and Connections with Diffusion Models

Energy-based Models (EBMs) [36, 59, 70, 77, 81, 82, 90, 93, 94, 154, 163, 166, 215, 220, 240, 255, 329, 384] can be viewedas one generative version of discriminators [94, 131, 165, 169], while can be learned from unlabeled input data. Letx ∼ 𝑝data (x) denote a training example, and 𝑝𝜃 (x) denote a probability density function that aims to approximates𝑝data (x). An energy-based model is defined a s: 𝑝𝜃 (x) = 1 𝑍𝜃 exp(𝑓𝜃 (x)), (46)where 𝑍𝜃 = ∫ exp(𝑓𝜃 (x))𝑑x is the partition function, which is analytically intractable for high-dimensional x. Forimages, 𝑓𝜃 (x) is parameterized by a convolutional neural network with a scalar output. Salimans et a l. (2021) [263]compare both constrained score models and energy-based models for modeling the score of the data distribution, findingthat constrained score models, i.e., energy based models, can perform just as well as unconstrained models when usinga comparable model structure.

Although EBMs have a number of desirable properties, two challenges remain for modeling high-dimensionaldata. First, learning EBMs by maximizing the likelihood requires MCMC method to generate samples from the model,which can be very computationally expensive. Second, as demonstrated in [219], the energy potentials learned withnon-convergent MCMC are not stable, in the sense that samples from long-run Markov chains can be significantlydifferent from the observed samples, and thus it is difficult to evaluate the learned energy potentials. In a recent study,Gao et a l. (2021) [83] present a diffusion recovery likelihood method to tractably learn samples from a sequence ofEBMs in the reverse process of the diffusion model. Each EBM is trained with recovery likelihood, which aims tomaximize the conditional probability of the data at a certain noise level, given their noisy versions at a higher noiseAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 25level. EBMs maximize the recovery likelihood because it is more tractable than marginal likelihood, as sampling fromthe conditional distributions is much easier than sampling from the marginal distributions.

## 7 APPLICATIONS OF DIFFUSION MODELS

Diffusion models have recently been employed to address a variety of challenging real-world tasks due to their flexibilityand strength. We have grouped these applications into six different categories based on the task: computer vision,natural language processing, temporal data modeling, multi-modal learning, robust learning, and interdisciplinaryapplications. For each category, we provide a brief introduction to the task, followed by a detailed explanation of howdiffusion models have been applied to improve performance. Table 3 summarizes the various applications that havemade use of diffusion models.

Table 3. Summary of all the applications utilizing the diffusion models.

Primary Secondary ArticleComputer Vision [172] [261],[257],[191],[259],[238], [113],[16],[227],[49] Super Resolution, Inpainting, Restoration, Translation, and Editing [286],[47],[208],[144],[349],[353] Semantic Segmentation [15],[25],[91],[333] Video Generation [108],[115],[361],[375],[275],[111],[322],[239],[297] Point Cloud Completion and Generation [390],[195],[200],[187],[373] Generating Data from Diffusion Models [355],[26],[391] Natural Language Generation Natural Language Generation [9],[176],[42],[86],[107],[61]Temporal Data Modeling Time Series Imputation [293],[3],[230],[181] Time Series Forecasting [250],[3],[181] Waveform Signal Processing [39],[160]Multi-Modal Learning Text-t o-Image Generation [10],[245],[260],[217],[98],[258],[147],[374],[349],[357],[353],[382],[351] Scene Graph-t o-Image Generation [346] Text-t o-3D/4D Generation [335],[179],[236],[371],[356],[372] Text-t o-Motion Generation [294, 375],[152] Text-t o-Video Generation [275],[111],[322],[239],[109],[357],[297] Text-t o-Audio Generation [237],[340],[324], [171],[290],[121],[153] Robust Learning Robust Learning [218],[363],[23],[310],[323],[289]Interdisciplinary Applications Molecular Graph Modeling [134],[116],[347],[337],[300], [122],[125],[123] Material Design [331],[198] Medical Image Reconstruction [286],[47],[48],[49],[232],[332]

## 7.1 Unconditional and Conditional Diffusion Models

Before we introduce the applications of diffusion models, we illustrate two basic application paradigms of diffusionmodels, namely unconditional diffusion models and conditional diffusion models. As a generative model, the historyof diffusion models is very similar to VAE, GAN, flow models, and other generative models. They all first developedunconditional generation, and then conditional generation followed closely. Unconditional generation is often usedto explore the upper limit of the performance of the generative model, while conditional generation is more aboutapplication-level content because it can enable us to control the generation results according to our intentions. In additionto promising generation quality and sample diversity, diffusion models are especially superior in their controllability.

The main algorithms of unconditional diffusion models have been well discussed in Sections 2 to 5, in next part, wemainly discuss how conditional diffusion models are applied to different applications with different forms of conditions,and choose some typical scenarios for demonstrations.

7.1.1 Conditioning Mechanisms in Diffusion Models. Utilizing different forms of conditions to guide the generationdirections of diffusion models are widely used, such as labels, classifiers, texts, images, semantic maps, graphs andso o n. However, some of the conditions are structural and complex, thus the methods to condition on them aredeserving discussion. There are mainly four kinds of conditioning mechanisms, including concatenation, gradient-based,Accepted by ACM Computing Surveys

## 26 Yang et a l.

cross-attention and adaptive layer normalization (adaLN). The concatenation means diffusion models concatenateinformative guidance with intermediate denoised targets in diffusion process, such as label embedding and semanticfeature maps. The gradient-based mechanism incorporates task-related gradient into the diffusion sampling process forcontrollable generation. For example, in image generation, one can train an auxiliary classifier on noisy images, andthen use gradients to guide the diffusion sampling process towards an arbitrary class label. The cross-attention performsattentional message passing between the guidance and diffusion targets, which is usually conducted in a layer-wisemanner in denoising networks. The adaLN mechanism follows the widespread usage of adaptive normalization layers[233] in GANs [143], Scalable Diffusion Models [231] explores replacing standard layer norm layers in transformer-baseddiffusion backbones with adaptive layer normalization. Instead of directly learning dimension-wise scale and shiftparameters, it regresses them from the sum of the time embedding and conditions.

7.1.2 Diffusion with DPO/RLHF. Building on the success of reinforcement learning from human feedback (RLHF)i n Large Language Models (LLMs) [11, 226], numerous methods in diffusion models have attempted to use similarapproaches for model alignment [75, 168]. Some methods use a pretrained reward model or train a new one to guide thegeneration process. For instance, ImageReward [334] manually annotated a large dataset of human-preferred imagesand trained a reward model to assess the alignment between images and human preferences. Some methods bypass thetraining of a reward model and directly finetune diffusion models on human preference datasets [343]. Diffusion-DPO[308] reformulates Direct Preference Optimization (DPO) to account for a diffusion model’s notion of likelihood, utilizingthe evidence lower bound to derive a differentiable objective. Recently, Zhang et a l. (2024) propose IterComp [382] toiteratively align the base diffusion model with composition-aware model preferences from the model gallery, consistingof six powerful open-source diffusion models, effectively enhancing the performance of base model on conditionaldiffusion generation. As demonstrated in Fig. 4, IterComp [382] outperforms other three types of conditional diffusionmethods while achieving the best inference efficiency.

7.1.3 Condition Diffusion on Labels and Classifiers. Conditioning diffusion process on the guidance of labels is astraight way to add desired properties into generated samples. However, when labels are limited, it is difficult to enablediffusion models to sufficiently capture the whole distribution of data. SGGM [358] proposes a self-guided diffusionprocess conditioning on the self-produced hierarchical label set, while You et a l. (2023) [365] demonstrate large-scalediffusion models and semi-supervised learners benefit mutually with a few labels via dual pseudo training. Dhariwaland Nichol [60] proposes classifier guidance to boost the sample quality of a diffusion model by using an extra trainedclassifier. Ho and Salimans [114] jointly train a conditional and an unconditional diffusion model, and find that it ispossible to combine the resulting conditional and unconditional scores to obtain a trade-off between sample quality anddiversity similar to that obtained by using classifier guidance.

7.1.4 Condition Diffusion on Texts, Images, and Semantic Maps. Recent researches begin to condition diffusion processon the guidance of more semantics, such as texts, images, and semantic maps, to better express rich semantics insamples. DiffuSeq [86] conditions on texts and proposes a seq-t o-seq diffusion framework that helps with four NLPtasks. SDEdit [208] conditions on a styled images to make image-t o-image translation, while LDM [257] unifies thesesemantic conditions with flexible latent diffusion. Kindly note that if conditions and diffusion targets are of differentmodalities, pre-alignment [245, 346] is a practical way to strengthen the guided diffusion. unCLIP [245] and ConPreDiff[349] leverage CLIP latents in text-t o-image generation, which have align the semantics between images and texts. RPG[353] conditions on complementary rectangle and contour regions to enable compositional text-t o-image generationAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 27IterComp (Ours)FLUX-dev RPGA glass sphere sculpture, concealed inside the sphere is a large Pirate Ship in a Lightning storm, large waves, in the dark, moonlight filters through a nearby window, casting a serene glow.

InstanceDiffusionThe little prince standing on small earth in starry sky, With a bright red scarf and golden hair, he gazes at the stars, capturing the essence of adventure. by saint exupery, crocheted style.

Text-controlled LLM-controlled Layout-controlled Reward-controlled

## 5.63 s/Img23.02 s/Img 15.57 s/Img 9.88 s/Img

A wise and intelligent little girl, her face illuminated by the lights of the night, embodies the universe's mysteries. Created from constellations and galaxy nebulae, she holds the endless power of a quasar. Her expression is insightful, as if she understands the depths of a black hole.

In the heart of an enchanted forest, a majestic tree stands illuminated by glowing mushrooms and tiny lights. Its thick roots form a staircase leading to a cozy door, suggesting a hidden world within. The scene is vibrant and magical, inviting wonder and exploration in this mystical woodland realm.

Fig. 4. Qualitative comparison between IterComp [382] and three types of compositional generation methods: text-controlled, LLM-controlled, and layout-controlled approaches. Colored text denotes the advantages of IterComp [382] in generated images.

and complex text-guided image editing. ContextDiff [357] proposes a universal forward-backward consistent diffusionmodel for better conditioning on various input modalities. Accepted by ACM Computing Surveys

## 28 Yang et a l.

7.1.5 Condition Diffusion on Graphs. Graph-structured data usually exhibits complex relations between nodes, thusconditioning on graphs are extremely hard for diffusion models. SGDiff [346] proposes the first diffusion modelspecifically designed for scene graph to image generation with a novel masked contrastive pre-training. Such maskedpre-training paradigm is general and can be extended to any cross-modal diffusion architectures for both coarse- and fine-grained guidance. Other graph-conditioned diffusion models are mainly studied for graph generation. Graphusion [347]conditions on the latent clusters of graph dataset to generate new 2D graphs that greatly align with data distribution.

BindDM [122], IPDiff [125] and IRDiff [123] propose to condition on 3D protein graph to generate 3D molecules withequivariant diffusion.

## 7.2 Computer Vision

7.2.1 Image Super Resolution, Inpainting, Restoration, Translation, and Editing. Generative models have been used totackle a variety of image restoration tasks including super-resolution, inpainting, and translation [16, 58, 74, 129, 172,227, 246, 385]. Image super-resolution aims to restore high-resolution images from low-resolution inputs, while imageinpainting revolves around reconstructing missing or damaged regions in an image.

Several methods make use of diffusion models for these tasks. For example, Super-Resolution via Repeated Refinement(SR3) [261] uses DDPM to enable conditional image generation. SR3 conducts super-resolution through a stochastic,iterative denoising process. The Cascaded Diffusion Model (CDM) [113] consists of multiple diffusion models insequence, each generating images of increasing resolution. Both the SR3 and CDM directly apply the diffusion processto input images, which leads to larger evaluation steps. In order to allow for the training of diffusion models withFig. 5. Image super resolution results produced by LDM [257].

limited computational resources, some methods [257, 301] have shifted the diffusion process to the latent space usingpre-trained autoencoders. The Latent Diffusion Model (LDM) [257] streamlines the training and sampling processes fordenoising diffusion models without sacrificing quality.

For inpainting tasks, RePaint [191] features an enhanced denoising strategy that uses resampling iterations to bettercondition the image. ConPreDiff [349] proposes a universal diffusion model based on context prediction to consistentlyimprove unconditional/conditional image generation and image inpainting (see Figure Fig. 6). Meanwhile, Palette [259]employs conditional diffusion models to create a unified framework for four image generation tasks: colorization,inpainting, uncropping, and JPEG restoration. Image translation focuses on synthesizing images with specific desiredAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 29Fig. 6. Image inpainting results produced by ConPreDiff [349].

styles [129]. SDEdit [208] uses a Stochastic Differential Equation (SDE) prior to improve fidelity. Specifically, it beginsby adding noise to the input image, then denoises the image through the SDE. Denoising Diffusion Restoration Models(DDRM) [144] takes advantage of a pre-trained denoising diffusion generative model for solving linear inverse problem,and demonstrates DDRM’s versatility on several image datasets for super-resolution, deblurring, inpainting, andcolorization under various amounts of measurement noise. Please refer to Section 7.4.1 for more text-t o-imagediffusion models.

7.2.2 Semantic Segmentation. Semantic segmentation aims to label each image pixel according to established objectcategories. Generative pre-training can enhance the label utilization of semantic segmentation models, and recentwork has shown that representations learned through DDPM contain high-level semantic information that is usefulfor segmentation tasks [15, 91]. The few-shot method that leverages these learned representations has outperformedalternatives such as VDVAE [44] and ALAE [234]. Similarly, Decoder Denoising Pretraining (DDeP) [25] integratesdiffusion models with denoising autoencoders [307] and delivers promising results on label-efficient semantic segmen-tation. ODISE [333] explores diffusion models for open-vocabulary segmentation tasks, and proposes a novel implicitcaptioner to generate captions for images for better utilizing pre-trained large-scale text-t o-image diffusion models.

7.2.3 Video Generation. Generating high-quality videos remains a challenge in the deep learning era due to thecomplexity and spatio-temporal continuity of video frames [345, 368]. Recent research has turned to diffusion models toimprove the quality of generated videos [115]. For example, the Flexible Diffusion Model (FDM) [108] uses a generativemodel to allow for the sampling of any arbitrary subset of video frames, given any other subset. The FDM also includes aspecialized architecture designed for this purpose. Additionally, the Residual Video Diffusion (RVD) model [361] utilizesan autoregressive, end-t o-end optimized video diffusion model. It generates future frames by amending a deterministicAccepted by ACM Computing Surveys

## 30 Yang et a l.

next-frame prediction, using a stochastic residual produced through an inverse diffusion process. Please refer toSection 7.4.5 for more text-t o-video diffusion models.

What happens if a hole appears in the balloon?

Turn the pocket watch on its back.

Training Data Examples What would it be like if they got married and 60 years have passed.

Melting the snowman with a hairdryer.

Generated Results of EditWorldMelting the snowman with a hairdryer. Melting the snowman with a hairdryer.

Generated Results of InstructPix2Pix Generated Results of MagicBrushFig. 7. Comparing EditWorld [355] with InstructPix2Pix and MagicBrush.

7.2.4 Generating Data from Diffusion Models. Synthesizing datasets from generative models can effectively advancevarious tasks like classification [12, 313, 391]. Recent works have begun to utilize diffusion models to achieve this goalfor vision tasks. For example, Trabucco et a l. [299] adopt diffusion models to make effective data augmentation for few-shot image classification. DistDiff [391] proposes a training-free data expansion framework with a distribution-awarediffusion model. It constructs hierarchical prototypes to approximate the real data distribution, and optimizes latentdata points in generation process with hierarchical energy guidance. InstructPix2Pix [26] leverages two large pretrainedmodels (i.e., GPT-3 and Stable Diffusion) to generate a large dataset of input-goal-instruction triplet examples, andtrains an instruction-following image editing model on the dataset. To enable image editing to reflect chalenging worldknowledge and dynamics from both real physical world and virtual media, EditWorld [355], introduces a new tasknamed world-instructed image editing, as the data examples presented in Fig. 7. EditWorld proposes an innovativecompositional framework with a set of pretrained LLMs and Diffusion Models, illustrated in Fig. 8, to synthesize aworld-instructed training dataset for instruction-following image editing.

7.2.5 Point Cloud Completion and Generation. Point clouds are a critical form of 3D representation for capturingreal-world objects. However, scans often generate incomplete point clouds due to partial observation or self-occlusion.

Recent studies have applied diffusion models to address this challenge, using them to infer missing parts in orderto reconstruct complete shapes. This work has implications for many downstream tasks such as 3D reconstruction,augmented reality, and scene understanding [196, 200, 373].

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 31(a) Text-t o-Image Generation for Diverse Scenes( b) Extracting Paired Data from Realistic Video FramesNow you are an "textual prompt creator", Please provide several examples based on real-world physical conditions, each example should sequentially include an initial image description, a final image description, image change instructions, and keywords. Here's one example: The initial image description is "{input_text}", the image change instruction is "{instruct}", the final image description is "{output_text}", and the keywords are "{keywords}". Please present the examples in the format of "1. {input_text}; {instruct}; {output_text}; {keywords}\n 2. ...".

usersusers Video-LLava image pairGPT-3.5Provide the Instructions based on the provided Descriptions, for example, the Description is {Description}, Then provide the {instruct}.

“Describe this video” Select Image Pairs from Videoinput textoutput textkeywordsDescriptionGPT-3.5users videovideosimages Textual  Requirmentinstructions World Instructions World InstructionsCombinational T2I Synthesis&ControlNetSDXL + IP-Adapter image pairFig. 8. EditWorld [355] generates a training dataset of world-instructed image editing from two different branches.

Nx( t) iFig. 9. The directed graphical model of the diffusion process for point clouds [195].

Luo et a l. 2021 [195] has taken the approach of treating point clouds as particles in a thermodynamic system, usinga heat bath to facilitate diffusion from the original distribution to a noise distribution. Meanwhile, the Point-VoxelDiffusion (PVD) model [390] joins denoising diffusion models with the pointvoxel representation of 3D shapes. ThePoint Diffusion-Refinement (PDR) model [200] uses a conditional DDPM to generate a coarse completion from partialobservations; it also establishes a point-wise mapping between the generated point cloud and the ground truth.

7.2.6 Anomaly Detection. Anomaly detection is a critical and challenging problem in machine learning [268, 386] andcomputer vision [341]. Generative models have been shown to own a powerful mechanism for anomaly detection[84, 106, 327], modeling normal or healthy reference data. AnoDDPM [327] utilizes DDPM to corrupt the input imageand reconstruct a healthy approximation of the image. These approaches may perform better than alternatives basedon adversarial training as they can better model smaller datasets with effective sampling and stable training schemes.

DDPM-CD [84] incorporates large numbers of unsupervised remote sensing images into the training process throughAccepted by ACM Computing Surveys

## 32 Yang et a l.

DDPM. Changes of remote sensed images is detected by utilizing a pre-trained DDPM and applying the multi-scalerepresentations from the diffusion model decoder.

## 7.3 Natural Language Generation

Natural language processing aims to understand, model, and manage human languages from different sources such astext or audio. Text generation has become one of the most critical and challenging tasks in natural language processing[128, 173, 174], aiming to compose plausible and readable text in human language given input data (e.g., a sequence andkeywords) or random noise.

Numerous approaches based on diffusion models have been developed for text generation. Discrete DenoisingDiffusion Probabilistic Models (D3PM) [9] introduces diffusion-like generative models for character-level text generation[38], generalizing the multinomial diffusion model [118] by going beyond corruption processes with uniform transitionprobabilities. Analog Bits [42] generates analog bits to represent discrete variables and further improves sample qualitywith self-conditioning and asymmetric time intervals.

While large autoregressive language models (LMs) can generate high-quality text [27, 46, 244, 379], reliable deploy-ment in real-world applications typically requires controllable text generation to satisfy desired requirements (e.g.,topic, syntactic structure). Controlling language model behavior without r e-training remains a major challenge intext generation [54, 148]. Although recent methods have achieved significant success on controlling simple sentenceattributes (e.g., sentiment) [161, 342], there has been limited progress on complex, fine-grained controls (e.g., syntacticstructure).

To tackle more complex controls, Diffusion-LM [176] proposes a new language model based on continuous diffusionthat starts with Gaussian noise vectors and incrementally denoises them into word-corresponding vectors. The gradualdenoising steps help produce hierarchical continuous latent representations, enabling simple gradient-based methodsto accomplish complex control. Similarly, DiffuSeq [86] conducts diffusion processes in latent space and proposes a newconditional diffusion model for challenging text-t o-text generation tasks.

Recent advances have explored more specialized applications of diffusion-based language generation. MMaDA [351]extends diffusion-based text generation to multi-modal settings, where textual content is generated in conjunctionwith other modalities through masked autoencoder-enhanced diffusion processes (i n Fig. 10). In reinforcement learningcontexts, TraceRL [311] applies diffusion models to generate natural language descriptions of optimal action sequences,bridging the gap between policy learning and natural language explanation.

These developments demonstrate the versatility of diffusion models in natural language generation, from basic textsynthesis to complex multi-modal and interactive applications, opening new avenues for controllable and context-awarelanguage generation.

## 7.4 Multi-Modal Generation

7.4.1 Text-t o-Image Generation. Vision-language models have attracted a lot of attention recently due to the number ofpotential applications [242]. Text-t o-Image generation is the task of generating a corresponding image from a descriptivetext [69, 147, 302]. Blended diffusion [10] utilizes both pre-trained DDPM [60] and CLIP [242] models, and it proposes asolution for region-based image editing for general purposes, which uses natural language guidance and is applicableto real and diverse images. On the other hand, unCLIP (DALLE-2) [245] proposes a two-stage approach, a prior modelthat can generate a CLIP-based image embedding conditioned on a text caption, and a diffusion-based decoder that cangenerate an image conditioned on the image embedding. Recently, Imagen [260] proposes a text-t o-image diffusionAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 33Fig. 10. Performance comparison of MMaDA [351] against other models on textual reasoning, multimodal reasoning, and world knowledge-aware text-t o-image generation tasks..

model and a comprehensive benchmark for performance evaluation. It shows that Imagen performs well against thestate-o f-the-art approaches including VQ-GAN+CLIP [52], Latent Diffusion Models [190], and DALL-E 2 [245]. Inspiredby the ability of guided diffusion models [60, 114] to generate photorealistic samples and the ability of text-t o-imagemodels to handle free-form prompts, GLIDE [217] applies guided diffusion to the application of text-conditionedimage synthesis. VQ-Diffusion [98] proposes a vector-quantized diffusion model for text-t o-image generation, and iteliminates the unidirectional bias and avoids accumulative prediction errors. Versatile Diffusion [338] proposes the firstunified multi-flow multimodal diffusion framework, which supports image-t o-text, image-variation, text-t o-image, andtext-variation, and can be further extended to other applications such as semantic-style disentanglement, image-textdual-guided generation, latent image-t o-text-t o-image editing, and more. Following Versatile Diffusion, UniDiffuser[14] proposes a unified diffusion model framework based on Transformer, which can fit multimodal data distributionsand simultaneously handle text-t o-image, image-t o-text, and joint image-text generation tasks. ConPreDiff [349] for thefirst time incorporates context prediction into text-t o-image diffusion models, and significantly improves generationAccepted by ACM Computing Surveys

## 34 Yang et a l.Ours																											Imagen																											LDM

“	A	waterfall	i s	i n	a	lush rainforest	teeming	with vibrant	vegetation.”“	A	scientist	i n	a laboratory,	surrounded b y	equipment	and	notes.”“	A	woman i s	dancing with	a	white background wall	featuring	graffiti	o f	a lion.” “	A	cozy	fireplace	with crackling	flames,	a hearth	rug,	and	a stack	o f	logs	nearby.” “	An	Italian	espresso	with latte	art	i n	the	shape	o f	a heart,	accompanied	b y	a saucer	and	a	spoon.”Fig. 11. Synthesis examples demonstrating text-t o-image capabilities of for various text prompts with LDM, Imagen, and ContextDiff [357].

performance without additional inference costs. ContextDiff [357] proposes general contextualized diffusion model byincorporating the cross-modal context encompassing interactions and alignments into forward and reverse processes.

A qualitative comparison between these models are presented in Fig. 11.

A new interesting line of diffusion model research is to leverage the pre-trained text-t o-image diffusion modelfor more complex or fine-grained control of synthesis results. DreamBooth [258] presents the first technique thattackles the new challenging problem of subject-driven generation, allowing users, from just a few casually capturedimages of a subject, to recontextualize subjects, modify their properties, original art renditions, and more. Differentfrom those image diffusion models conditioned on text prompts, ControlNet [374] attempts to control pre-trainedlarge diffusion models to support additional semantic maps, like edge maps, segmentation maps, keypoints, shapenormals, depths, etc. However, most methods often face challenges when handling complex text prompts involvingmultiple objects with multiple attributes and relationships. To this end, RPG [353] proposes a brand new training-freetext-t o-image generation/editing framework harnessing the powerful chain-o f-thought reasoning ability of multimodalLLMs [383] to enhance the compositionality of text-t o-image diffusion models. This new RPG framework unifies bothtext-guided image generation (i n Fig. 12) and image editing (i n Fig. 13) tasks in a closed-loop fashion. Notably, asdemonstrated in Fig. 14, RPG outperforms all SOTA methods, such as SDXL [235] and DALL-E 3 [20], demonstrate itssuperiority. Furthermore, RPG framework is user-friendly, and can generalize to different MLLM architectures anddiffusion backbones (e.g., ControlNet).

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 35User Text (Base) Prompt: A nobleman is standing under a bright moon while two owls are on a big oak.

Identify key phrases:

## 1、A noble man

## 2、A bright moon

## 3、Two owls

## 4、A big oak

Text-t o-Text Recaptioning

## 1. A nobleman, regal and distinguished, with a

sharp gaze, dressed in a velvet doublet, standing proudly in his ancestral castle

## 2. A bright moon, full and radiant, casting a silver

glow over a tranquil lake, serene and majestic in the night sky.

## 3. Two owls, perched side by side, wise and

mysterious, with piercing eyes, on an ancient, gnarled branch under the starlit sky

## 4. A big oak, towering and robust, its sprawling

branches a testament to centuries, leaves whispering stories in the gentle breeze. Rationale Generation ## Task objectives: Generate complementary subregions based on the recaptioned subprompts and assign them to subregions.

[Instructions + In-context examples][Question]: Subprompts: 1. A nobleman, regal and …..

## 2. A bright moon, full and radiant ……

## 3. Two owls, perched side by side ……

## 4. A big oak, towering and robust ……

Let’s think step by step (trigger CoT reasoning) Rationale:  Here we have 4 subprompts, thus total

## 4 regions. First, we take the nobleman and his

castle as foreground and background, we place them in lower left of the image. Next, bright moon should in the sky so we assign the moon to the higher left corner…… (Coarse grained area division) A moonA nobleman A nobleman (0.8)A moon (0.8) Two owls (0.8)A big oak (0.8)Recaption-Plan-Generate (RPG) for Text-t o-Image GenerationStage 1: Recaption Stage 2: Plan Stage 3: GenerateBase Prompt (0.2)Complementary Regional DiffusionMLLMsMLLMs Weighted sum for each sampling stepRegion-wise Generation Chain-o f-Thought Planning Subregion Planning Therefore, the region split ratio should b e:

## 1,1,1;2,2,3 （Column Mode）

[Assign subprompts to subregions] Region 0： A bright moon, full and  radiant…… Region 1：Two owls, perched side by side …… Region 2：A nobleman, regal and ….. Region 3： A big oak, towering and robust  ……CoT Reasoning Two owlsBig oakStep=10 Step=20 Step=30Step=40Step=5Fig. 12. Overview of RPG [353] framework for text-t o-image generation.

User Text Prompt: Six patterned mugs, arranged in two columns, on a marble surface, and a rose in the vase on the left.

Task Instruction + In-context Examples + User Text Prompt Edited ImageMulti-Round Editing with RPGImage-t o-Text Recaptioning ## Task objectives: Analyze user text prompt to identify key phrases and attributes,  utilizing visual reasoning on the image to detect inconsistency[Task instruction+ In-context examples ][Question] User text prompt: Six patterned mugs, arranged in two columns, on a marble surface, and a rose in the vase on the left. Task description: Analyze the prompt and  recaption the image, find inconsistency between the prompt and the recaption prompt, make an edit plan, Let’s think step by step (trigger CoT reasoning): Recaption the image: Eight patterned mugs, six mugs arranged in two columns on a marble surface, and two roses in the two mugs on the left lower corner.

Stage 1: Recaption Editing Planning: Rationale Generation：

## 1. More mugs staggered （delete）

## 2. More roses exist (delete)

## 3. There is no vase visible in the image (replace or add)

Edit plan: According to the inconsistency, (i) Firstly, we delete the mug on the right down corner along with its reflection image. (i i) Next, we delete the rose in the first column from left to right along with its reflection image. (iii) Finally, here is only one inconsistency, we should replace the mug in the left lower corner with a vase along with its reflection image.

Stage 2:  PlanStage 3: GenerateAnother Loop or Exit Detect Precise ContoursRegion-wise EditingChain-o f-Thought Planning Mask and InpaintingSource/Generated ImageFig. 13. RPG [353] can unify text-guided image generation and editing in a closed-loop approach.

Recent advances have also explored building upon diffusion-based language model (DLLM) foundations for enhancedmulti-modal generation capabilities. MMaDA [351] leverages pre-trained DLLM backbones and extends them withmasked autoencoder architectures to achieve superior multi-modal discrete data generation. By building on establishedDLLM foundations, MMaDA demonstrates how existing diffusion language models can be effectively adapted andenhanced for complex multi-modal synthesis tasks, opening new possibilities for foundation model reuse and extensionin the diffusion domain. Accepted by ACM Computing Surveys

## 36 Yang et a l.

Prompt: A beautiful landscape with a river in the middle, the left of the river is in the evening and in the winter with a big iceberg and a small village while some people are skiing on the river and some people are skating, the right of the river is in the summer with a volcano in the morning and a small village while some people are playing. SDXLDALL-E 3 RPG (Ours)Left Prompt: A Chinese general wearing a crown, with whiskers and golden Chinese style armor, standing with a majestic dragon head on his chest, symbolizing his strength, wearing black and gold boots. His appearance exudes a sense of authority, wisdom, and an unyielding spirit , embodying the ideal ancient Chinese hero. Right Prompt: This painting is a quintessential example of ancient Chinese ink art , At the top of the painting , towering mountains shrouded in mist rise majestically. The mountains‘ craggy peaks are sketched with fine , precise lines , typical of traditional Chinese ink art. A slender swirling mists, meandering waterfall begins its descent here , its water appearing almost ethereal amidst the soft. In the middle section, the waterfall cascades energetically , creating a dynamic contrast with the serene mountains above. Lush pine trees , rendered with graceful , flowing brush strokes , flank the waterfall. These trees appear to dance with the rhythm of the water , adding a vibrant life to the scene. At the bottom , the waterfall concludes its journey in a tranquil pool. The water's surface is calm , reflecting the surrounding nature and the sky above. Here , delicate flowers and small shrubs are depicted along the water's edge , symbolizing peace and harmony with nature.

SDXLDALL-E 3 RPG (Ours)SDXLDALL-E 3 RPG (Ours)Fig. 14. Compared to previous SOTA models, RPG [353] exhibits a superior ability to express intricate and compositional text prompts within generated images (colored text denotes critical part).

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 377.4.2 Scene Graph-t o-Image Generation. Despite text-t o-image generation models has made exciting progress fromnatural language descriptions, they struggle to faithfully reproduce complex sentences with many objects and relation-ships. Generating images from scene graphs (SGs) is an important and challenging task for generative models [136].

Traditional methods [110, 136, 177] mainly predict an image-like layout from SGs, then generate images based on thelayout. However, such intermediate representations would lose some semantics in SGs, and recent diffusion models[257] are also unable to address this limitation. SGDiff [346] proposes the first diffusion model specifically for imagegeneration from scene graphs (Fig. 15), and learns a continuous SG embedding to condition the latent diffusion model,which has been globally and locally semantically-aligned between SGs and images by the designed masked contrastivepre-training. SGDiff can generate images that better express the intensive and complex relations in SGs compared withboth non-diffusion and diffusion methods. However, high-quality paired SG-image datasets are scarce and small-scale,how to leverage large-scale text-image datasets to augment the training or provide a semantic diffusion prior for betterinitialization is still an open problem.

Fig. 15. SGDiff [346] leverages masked contrastive pre-training for scene graph-based image diffusion generation.

7.4.3 Text-t o-3D Generation. 3D content generation [140, 179, 236, 335] has been in high demand for a wide range ofapplications, including gaming, entertainment, and robotics simulation. Augmenting 3D content generation with naturallanguage could considerably help with both novices and experienced artists. DreamFusion [236] adopts a pre-trained 2Dtext-t o-image diffusion model to perform text-t o-3D synthesis. It optimizes a randomly-initialized 3D model (a NeuralRadiance Field, or NeRF) with a probability density distillation loss, which utilizes a 2D diffusion model as a prior foroptimization of a parametric image generator. To obtain fast and high-resolution optimization of NeRF, Magic3D [179]proposes a two-stage diffusion framework built on cascaded low-resolution image diffusion prior and high-resolutionlatent diffusion prior. In order to achieve high-fidelity 3D creation, Make-It-3D [292] optimizes a neural radiance fieldby incorporating constraints from the reference image at the frontal view and diffusion prior at novel views, enhancingAccepted by ACM Computing Surveys

## 38 Yang et a l.

the coarse model into textured point clouds and increasing realism with diffusion prior and high-quality textures fromthe reference image. ProlificDreamer [314] presents Variational Score Distillation (VSD), optimizing a distribution of3D scenes based on textual prompts as random variables to closely align the distribution of rendered images fromall perspectives with a pretrained 2D diffusion model, using KL divergence as the measure. IPDreamer [371] furtherproposes a novel 3D object synthesis framework that enables users to create controllable and high-quality 3D objectseffortlessly. It excels in synthesizing a high-quality 3D object which can greatly align with a provided complex imageprompt.

Modeling compositional 3D data distribution is a fundamental and critical task for generative models. Currentfeed-forward methods [272, 273] are primarily capable of generating single objects and face challenges when creatingmore complex scenes containing multiple objects due to limited training data. Recently, a series of learnable-layoutcompositional methods have been proposed [43, 73, 80, 105, 305] . These methods combine multiple object-a d-hocradiance fields and then optimize the positions of the radiance fields from external feedback. For example, Epsteinet a l. [73] propose learning a distribution of reasonable layouts based solely on the knowledge from a large pre-trainedtext-t o-image model. Vilesov et a l. [305] introduce an optimization method based on Monte-Carlo sampling and physicalconstraints.

7.4.4 Text-t o-Motion Generation. Human motion generation is a fundamental task in computer animation, withapplications covering from gaming to robotics [375]. The generate motion is usually a sequence of human posesrepresented by joint rotations and positions. Motion Diffusion Model (MDM) [294] adapts a classifier-free diffusion-based generative model for the human motion generation, which is transformer-based, combining insights from motiongeneration literature, and regularizes the model with geometric losses on the locations and velocities of the motion.

FLAME [152] involves a transformer-based diffusion to better handle motion data, which manages variable-lengthmotions and well attend to free-form text. Notably, it can edit the parts of the motion, both frame-wise and joint-wise,without any fine-tuning.

7.4.5 Text-t o-Video Generation. Tremendous recent progress in text-t o-image diffusion-based generation motivatesthe development of text-t o-video generation [111, 275, 322]. Make-A-Video [275] proposes to extend a diffusion-basedtext-t o-image model to text-t o-video through a spatiotemporally factorized diffusion model. It leverages joint text-imageprior to bypass the need for paired text-video data, and further presents super-resolution strategies for high-definition,high frame-rate text-t o-video generation. Imagen Video [111] generates high definition videos by designing a cascadedvideo diffusion models, and transfers some findings that work well in the text-t o-image setting to video generation,including frozen T5 text encoder and classifier-free guidance. Tune-A-Video [322] introduces one-shot video tuning fortext-t o-video generation, which eliminates the burden of training with large-scale video datasets. It employs efficientattention tuning and structural inversion to significantly enhance temporal consistency. Text2Video-Zero [149] achieveszero-shot text-t o-video synthesis using a pretrained text-t o-image diffusion model, ensuring temporal consistencythrough motion dynamics in latent codes and cross-frame attention. Its goal is to enable affordable text-guided videogeneration and editing without additional fine-tuning. FateZero [239] is the first framework for temporal-consistentzero-shot text-t o-video editing using pre-trained text-t o-image diffusion model. It fuses the attention maps in the DDIMinversion and generation processes to maximally preserve the consistency of motion and structure during editing.

ContextDiff [357] incorporates the cross-modal context information about the interactions between text condition andvideo sample into forward and reverse processes, forming a forward-backward consistent video diffusion model fortext-t o-video generation.

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 39（a) Video Generation with Compositional PromptsA heroic robot on the l e, and a magical girl on the right  are saving the day.

A handsome young man is drinking coﬀee on a wooden table. ---------> (transi+ons t o) A handsome young man and a beau:ful young lady on his l e, are drinking coﬀee on a wooden table.VideoCrafter2OursAnimateDiffModelScopePikaGen-2 A cute brown dog and a sleepy cat are napping in the sun.FreeNoiseStreamingT2VOurs A cute brown squirrel in Antarctica, on a pile of hazelnuts cinematic. ---------> (transitions t o) A cute brown squirrel and a cute white squirrel in Antarctica, on a pile of hazelnuts cinematicFreeNoiseOursStreamingT2V (b) Long Video Genera9on with Progressive Composi9onal PromptsFig. 16. Comparing VideoTetris [297] with open-sourced or commercial T2V models in short and long video generation.

Accepted by ACM Computing Surveys

## 40 Yang et a l.

Most of text-t o-video diffusion models are trained on fixed-size video datasets, and thus are often limited to generatinga relatively small number of frames, leading to significant degradation in quality when tasked with generating longervideos. Several advancements [109, 297, 394] have sought to overcome this limitation through various strategies.

Vlogger [394] employs a masked diffusion model for conditional frame input facilitating longer video generation, andStreamingT2V [109] utilizes a ControlNet-like conditioning mechanism to enable auto-regressive video generation.

Recent VideoTetris [297] introduces a Spatio-Temporal Compositional Diffusion method for handling scenes withmultiple objects and following progressive complex prompts (i.e., compositional text-t o-video generation). Besides,VideoTetris develops a new video data preprocessing method and a consistency regularization method called ReferenceFrame Attention to improve auto-regressive long video generation through enhanced motion dynamics and promptsemantics. Qualitative comparisons in Fig. 16 show that VideoTetris not only generates superior quality compositionalvideos, but also produces high-quality long videos that align with compositional prompts while maintaining the bestconsistency.

7.4.6 Text-t o-Audio Generation. Text-t o-audio generation is the task to transform normal language texts to voiceoutputs [171, 324]. Grad-TTS [237] presents a novel text-t o-speech model with a score-based decoder and diffusionmodels. It gradually transforms noise predicted by the encoder and is further aligned with text input by the method ofMonotonic Alignment Search [241]. Grad-TTS2 [153] improves Grad-TTS in an adaptive way. Diffsound [340] presentsa non-autoregressive decoder based on the discrete diffusion model [9, 277], which predicts all the mel-spectrogramtokens in every single step, and then refines the predicted tokens in the following steps. EdiTTS [290] leverages thescore-based text-t o-speech model to refine a mel-spectrogram prior that is coarsely modified. Instead of estimating thegradient of data density, ProDiff [121] parameterizes the denoising diffusion model by directly predicting the clean data.

## 7.5 Temporal Data Modeling

7.5.1 Time Series Imputation. Time series data are widely used with many important real-world applications [72, 225,345, 380]. Nevertheless, time series usually contain missing values for multiple reasons, caused by mechanical or artificialerrors [274, 291, 362]. Recent years, imputation methods have been greatly for both deterministic imputation [32, 37, 199]and probabilistic imputation [78], including diffusion-based approaches. Conditional Score-based Diffusion models forImputation (CSDI) [293] presents a novel time series imputation method that leverages score-based diffusion models.

Specifically, for the purpose of exploiting correlations within temporal data, it adopts the form of self-supervised trainingto optimize diffusion models. Its application in some real-world datasets reveals its superiority over previous methods.

Controlled Stochastic Differential Equation (CSDE) [230] proposes a novel probabilistic framework for modelingstochastic dynamics with a neural-controlled stochastic differential equation. Structured State Space Diffusion (SSSD)[3] integrates conditional diffusion models and structured state-space models [97] to particularly capture long-termdependencies in time series. It performs well in both time series imputation and forecasting tasks.

7.5.2 Time Series Forecasting. Time series forecasting is the task of forecasting or predicting the future value over aperiod of time. Neural methods have recently become widely-used for solving the prediction problem with univariatepoint forecasting methods [224] or univariate probabilistic methods [264]. In the multivariate setting, we also havepoint forecasting methods [175] as well as probabilistic methods, which explicitly model the data distribution usingGaussian copulas [265], GANs [364], or normalizing flows [251]. TimeGrad [250] presents an autoregressive model forforecasting multivariate probabilistic time series, which samples from the data distribution at each time step throughestimating its gradient. It utilizes diffusion probabilistic models, which are closely connected with score matching andAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 41 Historical Time Series Prediction… … 𝑥𝑇NoiseDatabase Retrieval …

## References

𝑥𝐻 𝑥𝑡 𝑥𝑡−1 𝑥0… … 𝜇𝜃𝒙𝒕−𝟏 = 𝝁𝜽(𝒙𝒕 , 𝒙𝑹, 𝒙𝑯, 𝑰𝒔, 𝒕)𝑥𝑅 Reverse Forward RetrievalMain FrameworkHistorical Time SeriesDatabase… Vector SimilarityTop k IndicesFeature SetPre-trained EncoderFeature 𝑥𝑅ReferencesRetrieval Mechanism 𝑥𝐻𝑬𝝋𝑬𝝋 Guide𝐷𝑅 …Fig. 17. Overview of retrieval-augmented diffusion models for time series forecasting (RATD [181]). The historical time series 𝑥 𝐻 is inputted into the retrieval module to for the corresponding references 𝑥 𝑅. After that, 𝑥 𝐻 is concatenated with the noise as the main input for the model 𝜇𝜃 . 𝑥 𝑅 will be utilized as the guidance for the denoising process.

energy-based methods. Specifically, it learns gradients by optimizing a variational bound on the data likelihood andtransforms white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling[282] during inference time. To handle complex time series forecasting, as illustrated in Fig. 17, Liu et a l. (2024) for thefirst time introduce Retrieval- Augmented Time series Diffusion (RATD) [181], allowing for greater utilization of thedataset and providing meaningful guidance in the denoising process.

7.5.3 Waveform Signal Processing. In electronics, acoustics, and some related fields, the waveform of a signal is denotedby the shape of its graph as a function of time, independent of its time and magnitude scales. WaveGrad [39] introducesa conditional model for waveform generation that estimates gradients of the data density. It receives a Gaussian whitenoise signal as input and iteratively refines the signal with a gradient-based sampler. WaveGrad naturally tradesinference speed for sample quality by adjusting the number of refinement steps, and make a connection between non-autoregressive and autoregressive models with respect to audio quality. DiffWave [160] presents a versatile and effectivediffusion probabilistic model for conditional or unconditional waveform generation. The model is non-autoregressiveand is efficiently trained by optimizing a variant of variational bound on the data likelihood. Moreover, it produceshigh-fidelity audio in different waveform generation tasks, such as class-conditional generation and unconditionalgeneration.

## 7.6 Robust Learning

Robust learning is a class of defense methods that help learning networks that are robust to adversarial perturbationsor noises [23, 218, 234, 310, 323, 363]. While adversarial training [202] is viewed as a standard defense method againstadversarial attacks for image classifiers, adversarial purification has shown significant performances as an alternativedefense method [363], which purifies attacked images into clean images with a standalone purification model. Givenan adversarial example, DiffPure [218] diffuses it with a small amount of noise following a forward diffusion processand then restores the clean image with a reverse generative process. Adaptive Denoising Purification (ADP) [363]demonstrates that an EBM trained with denoising score matching [306] can effectively purify attacked images withinjust a few steps. It further proposes an effective randomized purification scheme, injecting random noises into imagesbefore purification. Projected Gradient Descent (PGD) [23] presents a novel stochastic diffusion-based pre-processingAccepted by ACM Computing Surveys

## 42 Yang et a l.

robustification, which aims to b ea model-agnostic adversarial defense and yield a high-quality denoised outcome. Inaddition, some works propose to apply a guided diffusion process for advanced adversarial purification [310, 323].

## 7.7 Interdisciplinary Applications

7.7.1 Drug Design and Life Science. Graph Neural Networks [103, 326, 348, 389] and corresponding representationlearning [104] techniques have achieved great success [21, 295, 325, 336, 344, 393] in many areas, including modelingmolecules/proteins in various tasks ranging from property prediction [71, 85] to molecule/protein generation [132,139, 197, 270], where a molecule is naturally represented by a node-edge graph. On one hand, recent works propose topre-train GNN/transformer [194, 388] specifically for molecules/proteins with biomedical or physical insights [184, 370],and achieve remarkable results. On the other hand, more works begin to utilize graph-based diffusion models forenhancing molecule or protein generation. Torsional diffusion [134] presents a new diffusion framework that makesoperations on the space of torsion angles with a diffusion process on the hyperspace and an extrinsic-t o-intrinsic scoringmodel. GeoDiff [337] demonstrates that Markov chains evolving with equivariant Markov kernels can produce aninvariant distribution, and further design blocks for the Markov kernels to preserve the desirable equivariance property.

There are also other works incorporate the equivariance property into 3D molecule generation [116] and proteingeneration [5, 19]. Motivated by the classical force field methods for simulating molecular dynamics, ConfGF [269]directly estimates the gradient fields of the log density of atomic coordinates in molecular conformation generation.

Fig. 18. IPDiff [125] incorporates protein-ligand interactions into both forward and reverve processes of molecular diffusion models.

Recently, given a target protein, the design of 3D small drug molecules that can closely bind to the target beginsto be promoted by diffusion models [99, 123, 125]. IPDiff [125] proposes a novel 3D molecular diffusion model forstructure-based drug design (SBDD). As illustrated in Fig. 18, the pocket-ligand interaction is explicitly considered inboth forward and reverse processes with the proposed prior-conditioning and prior-shifting mechanisms. Notably,IPDiff beats all previous diffusion-based and autoregressive generation models regarding binding-related metrics andmolecular properties. BindDM [122] proposes a hierarchical complex-subcomplex diffusion model for SBDD tasks,Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 43which incorporates essential binding-adaptive subcomplex for 3D molecule diffusion generation. IRDiff [125] proposesan interaction-based retrieval-augmented 3D molecular diffusion model named IRDIFF for SBDD tasks. As deonstratedin Fig. 19, this model guides 3D molecular generation using informative external target-aware references, designingtwo novel augmentation mechanisms, i.e., retrieval augmentation and self augmentation, to incorporate essentialprotein-molecule binding structures for target-aware molecular generation.

Fig. 19. IRDiff [123] designs a interaction-based retrieval-augmented generation frameowrk for SBDD.

There are also studies that use diffusion models for protein generation, such as DiffAb. DiffAb [198] proposes forthe first time a diffusion-based 3D antibody design framework that models both the sequence and structure of thecomplementarity-determining regions (CDRs) that determine antibody complementarity. Experiments show that DiffAbcan be used for various antibody design tasks, such as jointly generating sequence-structure, designing CDRs withfixed frameworks, and optimizing antibodies. SMCDiff [300] proposes to first learn a distribution over diverse andlonger protein backbone structures via an E(3)-equivariant graph neural network, and then efficiently samples scaffoldsfrom this distribution given a motif. The generation results demonstrates the designed backbones is well aligned withAlphaFold2-predicted structures.

7.7.2 Material Design. Solid state materials are the critical foundation of numerous key technologies [28]. CrystalDiffusion Variational Autoencoder (CDVAE) [331] incorporates stability as an inductive bias by proposing a noiseconditional score network, which simultaneously utilizes permutation, translation, rotation, and periodic invarianceproperties. Luo et a l. (2022) [198] model sequences and structures of complementarity-determining regions withequivariant diffusion, and explicitly target specific antigen structures to generate antibodies at atomic resolution.

7.7.3 Medical Image Reconstruction. An inverse problem is to recover an unknown signal from observed measurements,and it i sa n important problem in medical image reconstruction of Computed Tomography (CT) and Magnetic ResonanceImaging (MRI) [47, 48, 232, 286, 332]. Song et a l. (2021) [286] utilize a score-based generative model to reconstructan image consistent with both the prior and the observed measurements. Chung et a l. (2022) [49] train a continuoustime-dependent score function with denoising score matching, and iterate between the numerical SDE solver and dataAccepted by ACM Computing Surveys

## 44 Yang et a l.

consistency step for reconstruction at the evaluation stage. Peng et a l. (2022) [232] perform MR reconstruction bygradually guiding the reverse-diffusion process given observed k-space signal, and propose a coarse-t o-fine samplingalgorithm for efficient sampling.

## 8 FUTURE DIRECTIONS

Research on diffusion models is in its early stages, with much potential for improvement in both theoretical andempirical aspects. As discussed in early sections, key research directions include efficient sampling and improvedlikelihood, as well as exploring how diffusion models can handle special data structures, interface with other types ofgenerative models, and be tailored to a range of applications. In addition, we foresee that future research on diffusionmodels will likely expand to the following avenues.

Revisiting Assumptions. Numerous typical assumptions in diffusion models need to be revisited and analyzed. Forexample, the assumption that the forward process of diffusion models completely erases any information in dataand renders it equivalent to a prior distribution may not always hold. In reality, complete removal of information isunachievable in finite time. It is of great interest to understand when to halt the forward noising process in orderto strike a balance between sampling efficiency and sample quality [79]. Recent advances in Schrödinger bridgesand optimal transport [41, 55, 57, 271, 280] provide promising alternative solutions, suggesting new formulations fordiffusion models that are capable of converging to a specified prior distribution in finite time.

Theoretical Understanding. Diffusion models have emerged as a powerful framework, notably as the only one thatcan rival generative adversarial networks (GANs) in most applications without resorting to adversarial training. Keyto harnessing this potential is an understanding of why and when diffusion models are effective over alternativesfor specific tasks. It is important to identify which fundamental characteristics differentiate diffusion models fromother types of generative models, such as variational autoencoders, energy-based models, or autoregressive models.

Understanding these distinctions will help elucidate why diffusion models are capable of generating samples of excellentquality while achieving top likelihood. Equally important is the need to develop theoretical guidance for selecting anddetermining various hyperparameters of diffusion models systematically.

Latent Representations. Unlike variational autoencoders or generative adversarial networks, diffusion models are lesseffective for providing good representations of data in their latent space. As a result, they cannot be easily used fortasks such as manipulating data based on semantic representations. Furthermore, since the latent space in diffusionmodels often possesses the same dimensionality as the data space, sampling efficiency is negatively affected and themodels may not learn the representation schemes well [133].

AIGC and Diffusion Foundation Models. From Stable Diffusion to ChatGPT, Artificial Intelligence Generated Content(AIGC) has gained much attention in both academic and industrial circles. Generative Pre-Training is the core techniquein GPT-1/2/3/4 [222, 226, 243, 244] and (Visual) ChatGPT [320], which exhibits promising generation performance andsurprising emergent abilities [318] equipped with Large Language Models (LLMs) [298] and Visual Foundation Models[24, 366, 369].

It is interesting to transfer the generative pre-training (decoder-only) from GPT series to diffusion model class,evaluate the diffusion-based generation performance at scale, and analyse the emergent abilities of diffusion foundationmodels. Recent advances have demonstrated the potential of building specialized diffusion foundation models fordifferent domains and applications. MMaDA [351] exemplifies this trend by leveraging diffusion-based language modelAccepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 45foundations to create a multi-modal generation framework that exhibits emergent cross-modal reasoning capabilities.

Similarly, TraceRL [311] explores how diffusion foundation models can be adapted for reinforcement learning scenarios,generating coherent action sequences while maintaining the scalable properties of foundation models.

Furthermore, combining LLMs with diffusion models has been proved to b ea new promising direction [353, 355],opening up possibilities for more sophisticated AIGC systems [182] that leverage the complementary strengths of bothparadigms. These developments suggest that the future of AIGC lies not only in scaling individual model types but alsoin creating hybrid architectures that can harness different generative principles for enhanced performance and broaderapplicability.

## 9 CONCLUSION

We have provided a comprehensive look at diffusion models from various angles. We began with a self-containedintroduction to three fundamental formulations: DDPMs, SGMs, and Score SDEs. We then discussed recent effortsto improve diffusion models, highlighting three major directions: sampling efficiency, likelihood maximization, andnew techniques for data with special structures. We also explored connections between diffusion models and othergenerative models and outlined potential benefits of combining the two. A survey of applications across six domainsillustrated the wide-ranging potential of diffusion models. Finally, we outlined possible avenues for future research.

REFERENCES[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et a l. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Michael Samuel Albergo and Eric Vanden-Eijnden. 2022. Building Normalizing Flows with Stochastic Interpolants. In The Eleventh International Conference on Learning Representations. [3] Juan Miguel Lopez Alcaraz and Nils Strodthoff. 2022. Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models. arXiv preprint arXiv:2208.09399 (2022). [4] Tomer Amit, Eliya Nachmani, Tal Shaharbany, and Lior Wolf. 2021. Segdiff: Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390 (2021). [5] Namrata Anand and Tudor Achim. 2022. Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models. arXiv preprint arXiv:2205.15019 (2022). [6] Brian DO Anderson. 1982. Reverse-time diffusion equation models. Stochastic Processes and their Applications 12, 3 (1982), 313–326. [7] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et a l. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023). [8] Uri M Ascher and Linda R Petzold. 1998. Computer methods for ordinary differential equations and differential-algebraic equations. Vol. 61. Siam. [9] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2021. Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems. [10] Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In IEEE Conference on Computer Vision and Pattern Recognition. 18208–18218. [11] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et a l. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 (2022). [12] Hritik Bansal and Aditya Grover. 2023. Leaving Reality to Imagination: Robust Classification via Generated Datasets. In International Conference on Learning Representations. [13] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. 2021. Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models. In International Conference on Learning Representations. [14] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. 2023. One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. arXiv preprint arXiv:2303.06555 (2023). [15] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2021. Label-Efficient Semantic Segmentation with Diffusion Models. In International Conference on Learning Representations. [16] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, and Christian Etmann. 2021. Conditional image generation with score-based diffusion models. arXiv preprint arXiv:2111.13606 (2021). Accepted by ACM Computing Surveys

## 46 Yang et a l.

[17] Samy Bengio and Yoshua Bengio. 2000. Taking on the curse of dimensionality in joint distributions using neural networks. IEEE Trans. Neural Networks Learn. Syst. (2000). [18] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The journal of machine learning research 3 (2003), 1137–1155. [19] Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. 2000. The protein data bank. Nucleic acids research 28, 1 (2000), 235–242. [20] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et a l. 2023. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf (2023). [21] Piotr Bielak, Tomasz Kajdanowicz, and Nitesh V Chawla. 2021. Graph Barlow Twins: A self-supervised representation learning framework for graphs. arXiv preprint arXiv:2106.02466 (2021). [22] Mikołaj Bińkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. 2018. Demystifying MMD GANs. In International Conference on Learning Representations. [23] Tsachi Blau, Roy Ganz, Bahjat Kawar, Alex Bronstein, and Michael Elad. 2022. Threat Model-Agnostic Adversarial Defense using Diffusion Models. arXiv preprint arXiv:2207.08089 (2022). [24] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et a l. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021). [25] Emmanuel Asiedu Brempong, Simon Kornblith, Ting Chen, Niki Parmar, Matthias Minderer, and Mohammad Norouzi. 2022. Denoising Pretraining for Semantic Segmentation. In IEEE Conference on Computer Vision and Pattern Recognition. 4175–4186. [26] Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In IEEE Conference on Computer Vision and Pattern Recognition. 18392–18402. [27] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et a l. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems. [28] Keith T Butler, Daniel W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. 2018. Machine learning for molecular and materials science. Nature 559, 7715 (2018), 547–555. [29] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath Hariharan. 2020. Learning gradient fields for shape generation. In European Conference on Computer Vision. Springer, 364–381. [30] Andrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and Arnaud Doucet. 2022. A Continuous Time Framework for Discrete Denoising Models. arXiv preprint arXiv:2205.14987 (2022). [31] Chentao Cao, Zhuo-Xu Cui, Shaonan Liu, Dong Liang, and Yanjie Zhu. 2022. High-Frequency Space Diffusion Models for Accelerated MRI. arXiv preprint arXiv:2208.05481 (2022). [32] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. Brits: Bidirectional recurrent imputation for time series. In Advances in Neural Information Processing Systems, Vol. 31. [33] Nicholas Carlini, Florian Tramer, Krishnamurthy Dvijotham1, and Kolter J. Zico. 2022. (Certified!!) Adversarial Robustness for Free! arXiv preprint arXiv:2206.10550 (2022). [34] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. 2022. Maskgit: Masked generative image transformer. In IEEE Conference on Computer Vision and Pattern Recognition. 11315–11325. [35] Introducing ChatGPT. 2022. Introducing ChatGPT. [36] Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua Bengio. 2020. Your GAN is Secretly an Energy-based Model and You Should use Discriminator Driven Latent Sampling. arXiv preprint arXiv:2003.06060 (2020). [37] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. 2018. Recurrent neural networks for multivariate time series with missing values. Scientific reports 8, 1 (2018), 1–12. [38] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005 (2013). [39] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. 2020. WaveGrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713 (2020). [40] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2018. Neural ordinary differential equations. arXiv preprint arXiv:1806.07366 (2018). [41] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. 2021. Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs Theory. In International Conference on Learning Representations. [42] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. 2022. Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning. arXiv preprint arXiv:2208.04202 (2022). [43] Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu. 2024. Comboverse: Compositional 3d assets creation using spatially-aware diffusion guidance. arXiv preprint arXiv:2403.12409 (2024). [44] Rewon Child. 2020. Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images. In International Conference on Learning Representations.

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 47[45] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating Long Sequences with Sparse Transformers. CoRR abs/1904.10509 (2019). arXiv:1904.10509 http://arxiv.org/abs/1904.10509 [46] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et a l. 2022. PaLM: Scaling Language Modeling with Pathways. (2022). [47] Hyungjin Chung, Eun Sun Lee, and Jong Chul Ye. 2022. MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion. arXiv preprint arXiv:2203.12621 (2022). [48] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. 2022. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In IEEE Conference on Computer Vision and Pattern Recognition. 12413–12422. [49] Hyungjin Chung and Jong Chul Ye. 2022. Score-based diffusion models for accelerated MRI. Medical Image Analysis (2022), 102479. [50] Rob Cornish, Anthony Caterini, George Deligiannidis, and Arnaud Doucet. 2020. Relaxing bijectivity constraints with continuously indexed normalising flows. In International Conference on Machine Learning. 2133–2143. [51] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. 2018. Generative adversarial networks: An overview. IEEE signal processing magazine 35, 1 (2018), 53–65. [52] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. 2022. Vqgan-clip: Open domain image generation and editing with natural language guidance. arXiv preprint arXiv:2204.08583 (2022). [53] Salman UH Dar, Şaban Öztürk, Yilmaz Korkmaz, Gokberk Elmas, Muzaffer Özbey, Alper Güngör, and Tolga Çukur. 2022. Adaptive Diffusion Priors for Accelerated MRI Reconstruction. arXiv preprint arXiv:2207.05876 (2022). [54] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and Play Language Models: A Simple Approach to Controlled Text Generation. In International Conference on Learning Representations. [55] Valentin De Bortoli, Arnaud Doucet, Jeremy Heng, and James Thornton. 2021. Simulating diffusion bridges with score matching. arXiv preprint arXiv:2111.07243 (2021). [56] Valentin De Bortoli, Emile Mathieu, Michael Hutchinson, James Thornton, Yee Whye Teh, and Arnaud Doucet. 2022. Riemannian score-based generative modeling. arXiv preprint arXiv:2202.02763 (2022). [57] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. 2021. Diffusion Schrödinger bridge with applications to score-based generative modeling. In Advances in Neural Information Processing Systems, Vol. 34. 17695–17709. [58] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition. 248–255. [59] Guillaume Desjardins, Yoshua Bengio, and Aaron C Courville. 2011. On tracking the partition function. In Advances in Neural Information Processing Systems. 2501–2509. [60] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, Vol. 34. 8780–8794. [61] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et a l. 2022. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089 (2022). [62] Laurent Dinh, David Krueger, and Yoshua Bengio. 2015. Nice: Non-linear independent components estimation. ICLR 2015 Workshop Track (2015). [63] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2016. Density estimation using real nvp. arXiv preprint arXiv:1605.08803 (2016). [64] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2017. Density estimation using Real NVP. In International Conference on Learning Representations. https://openreview.net/forum?i d=HkpbnH9lx [65] Laurent Dinh, Jascha Sohl-Dickstein, Hugo Larochelle, and Razvan Pascanu. 2019. A RAD approach to deep mixture models. arXiv preprint arXiv:1903.07714 (2019). [66] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. 2021. Score-Based Generative Modeling with Critically-Damped Langevin Diffusion. In International Conference on Learning Representations. [67] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. 2022. GENIE: Higher-Order Denoising Diffusion Solvers. Advances in Neural Information Processing Systems (2022). [68] Carl Doersch. 2016. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908 (2016). [69] Yifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao. 2022. A survey of vision-language pre-trained models. arXiv preprint arXiv:2202.10936 (2022). [70] Yilun Du and Igor Mordatch. 2019. Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689 (2019). [71] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems, Vol. 28. [72] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-Series Representation Learning via Temporal and Contextual Contrasting. arXiv preprint arXiv:2106.14112 (2021). [73] Dave Epstein, Ben Poole, Ben Mildenhall, Alexei A Efros, and Aleksander Holynski. 2024. Disentangled 3D Scene Generation with Layout Learning. In International Conference on Machine Learning. [74] Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for high-resolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition. 12873–12883. [75] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. 2024. Reinforcement learning for fine-tuning text-t o-image diffusion models. Advances in Neural Information Processing Systems 36Accepted by ACM Computing Surveys

## 48 Yang et a l.

(2024). [76] Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. 2016. Testing the manifold hypothesis. Journal of the American Mathematical Society

## 29, 4 (2016), 983–1049.

[77] Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. 2016. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852 (2016). [78] Vincent Fortuin, Dmitry Baranchuk, Gunnar Ratsch, and Stephan Mandt. 2020. Gp-vae: Deep probabilistic time series imputation. In International conference on artificial intelligence and statistics. PMLR, 1651–1661. [79] Giulio Franzese, Simone Rossi, Lixuan Yang, Alessandro Finamore, Dario Rossi, Maurizio Filippone, and Pietro Michiardi. 2022. How much is enough? a study on diffusion times in score-based generative models. arXiv preprint arXiv:2206.05173 (2022). [80] Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard Schölkopf. 2024. Graphdreamer: Compositional 3d scene synthesis from scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 21295–21304. [81] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. 2018. Learning generative convnets via multi-grid modeling and sampling. In IEEE Conference on Computer Vision and Pattern Recognition. 9155–9164. [82] Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. 2020. Flow contrastive estimation of energy-based models. In IEEE Conference on Computer Vision and Pattern Recognition. 7518–7528. [83] Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P Kingma. 2020. Learning energy-based models by diffusion recovery likelihood. arXiv preprint arXiv:2012.08125 (2020). [84] Wele Gedara Chaminda Bandara, Nithin Gopalakrishnan Nair, and Vishal M Patel. 2022. Remote Sensing Change Detection (Segmentation) using Denoising Diffusion Probabilistic Models. arXiv e-prints (2022), arXiv–2206. [85] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry. In International Conference on Machine Learning. 1263–1272. [86] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. 2023. Sequence to sequence text generation with diffusion models. In International Conference on Learning Representations. [87] Wenbo Gong and Yingzhen Li. 2021. Interpreting diffusion score matching using normalizing flow. arXiv preprint arXiv:2107.10072 (2021). [88] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems, Vol. 27. 139–144. [89] Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model for learning in graph domains. In Proceedings. 2005 IEEE international joint conference on neural networks, Vol. 2. 729–734. [90] Anirudh Goyal Alias Parth Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. 2017. Variational walkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural Information Processing Systems. 4392–4402. [91] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. 2022. Diffusion models as plug-and-play priors. In Advances in Neural Information Processing Systems. [92] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. 2019. Scalable Reversible Generative Models with Free-form Continuous Dynamics. In International Conference on Learning Representations. https://openreview.net/forum?i d=rJxgknCcK7 [93] Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. 2019. Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One. arXiv preprint arXiv:1912.03263 (2019). [94] Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, and Richard Zemel. 2020. Cutting out the Middle-Man: Training and Evaluating Energy-Based Models without Sampling. arXiv preprint arXiv:2002.05616 (2020). [95] Alex Graves. 2013. Generating Sequences With Recurrent Neural Networks. CoRR abs/1308.0850 (2013). arXiv:1308.0850 http://arxiv.org/abs/1308. 0850 [96] Ulf Grenander and Michael I Miller. 1994. Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B (Methodological) 56, 4 (1994), 549–581. [97] Albert Gu, Karan Goel, and Christopher Re. 2021. Efficiently Modeling Long Sequences with Structured State Spaces. In International Conference on Learning Representations. [98] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. 2022. Vector quantized diffusion model for text-t o-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition. 10696–10706. [99] Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 2023. 3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction. In International Conference on Learning Representations. [100] Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. 2021. A review on generative adversarial networks: Algorithms, theory, and applications. IEEE Transactions on Knowledge and Data Engineering (2021). [101] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et a l. 2025. Deepseek-r 1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [102] David Ha and Jürgen Schmidhuber. 2018. World models. arXiv preprint arXiv:1803.10122 (2018). [103] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems. 1025–1035.

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 49[104] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017). [105] Haonan Han, Rui Yang, Huan Liao, Jiankai Xing, Zunnan Xu, Xiaoming Yu, Junwei Zha, Xiu Li, and Wanhua Li. 2024. REPARO: Compositional 3D Assets Generation with Differentiable 3D Layout Alignment. arXiv preprint arXiv:2405.18525 (2024). [106] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. 2022. ADBench: Anomaly Detection Benchmark. arXiv preprint arXiv:2206.09426 (2022). [107] Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. 2022. Ssd-l m: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432 (2022). [108] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. 2022. Flexible Diffusion Modeling of Long Videos. arXiv preprint arXiv:2205.11495 (2022). [109] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. 2024. StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text. arXiv preprint arXiv:2403.14773 (2024). [110] Roei Herzig, Amir Bar, Huijuan Xu, Gal Chechik, Trevor Darrell, and Amir Globerson. 2020. Learning canonical representations for scene graph to image generation. In European Conference on Computer Vision. 210–227. [111] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et a l. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022). [112] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, Vol. 33. 6840–6851. [113] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. 2022. Cascaded Diffusion Models for High Fidelity Image Generation. J. Mach. Learn. Res. 23 (2022), 47–1. [114] Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022). [115] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. 2022. Video diffusion models. arXiv preprint arXiv:2204.03458 (2022). [116] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. 2022. Equivariant Diffusion for Molecule Generation in 3D. arXiv e-prints (2022), arXiv–2203. [117] Emiel Hoogeboom, Alexey A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. 2021. Autoregressive Diffusion Models. In International Conference on Learning Representations. [118] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. 2021. Argmax flows and multinomial diffusion: Learning categorical distributions. In Advances in Neural Information Processing Systems, Vol. 34. 12454–12465. [119] Chin-Wei Huang, Milad Aghajohari, A. Bose, P. Panangaden, and Aaron C. Courville. 2022. Riemannian Diffusion Models. [120] Chin-Wei Huang, Jae Hyun Lim, and Aaron C Courville. 2021. A variational perspective on diffusion-based generative models and score matching. In Advances in Neural Information Processing Systems, Vol. 34. 22863–22876. [121] Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, and Yi Ren. 2022. ProDiff: Progressive Fast Diffusion Model For High-Quality Text-t o-Speech. arXiv preprint arXiv:2207.06389 (2022). [122] Zhilin Huang, Ling Yang, Zaixi Zhang, Xiangxin Zhou, Yu Bao, Xiawu Zheng, Yuwei Yang, Yu Wang, and Wenming Yang. 2024. Binding-Adaptive Diffusion Models for Structure-Based Drug Design. In The AAAI Conference on Artificial Intelligence. [123] Zhilin Huang, Ling Yang, Xiangxin Zhou, Chujun Qin, Yijie Yu, Xiawu Zheng, Zikun Zhou, Wentao Zhang, Yu Wang, and Wenming Yang. 2024. Interaction-based Retrieval-augmented Diffusion Models for Protein-specific 3D Molecule Generation. In International Conference on Machine Learning. [124] Zhilin Huang, Ling Yang, Xiangxin Zhou, Zhilong Zhang, Wentao Zhang, Xiawu Zheng, Jie Chen, Yu Wang, CUI Bin, and Wenming Yang. 2024. Protein-ligand interaction prior for binding-aware 3d molecule diffusion models. In International Conference on Learning Representations. [125] Zhilin Huang, Ling Yang, Xiangxin Zhou, Zhilong Zhang, Wentao Zhang, Xiawu Zheng, Jie Chen, Yu Wang, Bin CUI, and Wenming Yang. 2024. Protein-Ligand Interaction Prior for Binding-aware 3D Molecule Diffusion Models. In International Conference on Learning Representations. [126] Michael F Hutchinson. 1989. A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. Communications in Statistics-Simulation and Computation 18, 3 (1989), 1059–1076. [127] Aapo Hyvärinen. 2005. Estimation of Non-Normalized Statistical Models by Score Matching. J. Mach. Learn. Res. 6 (2005), 695–709. [128] Touseef Iqbal and Shaima Qureshi. 2020. The survey: Text generation models in deep learning. Journal of King Saud University-Computer and Information Sciences (2020). [129] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-t o-image translation with conditional adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition. 1125–1134. [130] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et a l. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024). [131] Long Jin, Justin Lazarow, and Zhuowen Tu. 2017. Introspective classification with convolutional nets. In Advances in Neural Information Processing Systems, Vol. 30. 823–833. Accepted by ACM Computing Surveys

## 50 Yang et a l.

[132] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. 2018. Junction tree variational autoencoder for molecular graph generation. In International Conference on Machine Learning. 2323–2332. [133] Bowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. 2022. Subspace diffusion generative models. arXiv preprint arXiv:2205.01490 (2022). [134] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. 2022. Torsional Diffusion for Molecular Conformer Generation. arXiv preprint arXiv:2206.01729 (2022). [135] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. 2022. Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning. PMLR, 10362–10383. [136] Justin Johnson, Agrim Gupta, and Li Fei-Fei. 2018. Image generation from scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1219–1228. [137] Alexia Jolicoeur-Martineau, Ke Li, Rémi Piché-Taillefer, Tal Kachman, and Ioannis Mitliagkas. 2021. Gotta Go Fast When Generating Data with Score-Based Models. (2021). [138] Alexia Jolicoeur-Martineau, Remi Piche-Taillefer, Rémi Tachet des Combes, and Ioannis Mitliagkas. 2021. Adversarial score matching and improved sampling for image generation. ArXiv abs/2009.05475 (2021). [139] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et a l. 2021. Highly accurate protein structure prediction with AlphaFold. Nature 596, 7873 (2021), 583–589. [140] Heewoo Jun and Alex Nichol. 2023. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023). [141] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aäron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. 2018. Efficient Neural Audio Synthesis. In International Conference on Machine Learning. 2410–2419. [142] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 2022. Elucidating the Design Space of Diffusion-Based Generative Models. arXiv preprint arXiv:2206.00364 (2022). [143] Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 4401–4410. [144] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. 2022. Denoising diffusion restoration models. arXiv preprint arXiv:2201.11793 (2022). [145] Bahjat Kawar, Roy Ganz, and Michael Elad. 2022. Enhancing diffusion-based image synthesis with robust classifier guidance. arXiv preprint arXiv:2208.08664 (2022). [146] Bahjat Kawar, Gregory Vaksman, and Michael Elad. 2021. Stochastic image denoising by sampling from the posterior distribution. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1866–1875. [147] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2022. Imagic: Text-Based Real Image Editing with Diffusion Models. arXiv preprint arXiv:2210.09276 (2022). [148] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858 (2019). [149] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. 2023. Text2video-zero: Text-t o-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439 (2023). [150] Boah Kim, Inhwa Han, and Jong Chul Ye. 2021. Diffusemorph: Unsupervised deformable image registration along continuous trajectory using diffusion models. arXiv preprint arXiv:2112.05149 (2021). [151] Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-chul Moon. 2022. Maximum Likelihood Training of Implicit Nonlinear Diffusion Model. In Advances in Neural Information Processing Systems. [152] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. 2022. Flame: Free-form language-based motion synthesis & editing. arXiv preprint arXiv:2209.00349 (2022). [153] Sungwon Kim, Heeseung Kim, and Sungroh Yoon. 2022. Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-t o-Speech with Untranscribed Data. arXiv preprint arXiv:2205.15370 (2022). [154] Taesup Kim and Yoshua Bengio. 2016. Deep directed generative models with energy-based probability estimation. arXiv preprint arXiv:1606.03439 (2016). [155] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. 2021. Variational diffusion models. In Advances in Neural Information Processing Systems, Vol. 34. 21696–21707. [156] Diederik P Kingma and Prafulla Dhariwal. 2018. Glow: Generative flow with invertible 1x1 convolutions. arXiv preprint arXiv:1807.03039 (2018). [157] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013). [158] Diederik P Kingma, Max Welling, et a l. 2019. An introduction to variational autoencoders. Foundations and Trends® in Machine Learning 12, 4 (2019), 307–392. [159] Daphne Koller and Nir Friedman. 2009. Probabilistic graphical models: principles and techniques. MIT press. [160] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. 2020. Diffwave: A versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761 (2020). [161] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2020. Gedi: Generative discriminator guided sequence generation. arXiv preprint arXiv:2009.06367 (2020).

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 51[162] Alex Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images. (2009). [163] Rithesh Kumar, Anirudh Goyal, Aaron Courville, and Yoshua Bengio. 2019. Maximum Entropy Generators for Energy-Based Models. arXiv preprint arXiv:1901.08508 (2019). [164] Hugo Larochelle and Iain Murray. 2011. The Neural Autoregressive Distribution Estimator. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS. [165] Justin Lazarow, Long Jin, and Zhuowen Tu. 2017. Introspective neural networks for generative modeling. In Proceedings of the IEEE International Conference on Computer Vision. 2774–2783. [166] Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fujie Huang. 2006. A tutorial on energy-based learning. Predicting structured data 1, 0 (2006). [167] Jin Sub Lee and Philip M Kim. 2022. ProteinSGM: Score-based generative modeling for de novo protein design. bioRxiv (2022). [168] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. 2023. Aligning text-t o-image models using human feedback. arXiv preprint arXiv:2302.12192 (2023). [169] Kwonjoon Lee, Weijian Xu, Fan Fan, and Zhuowen Tu. 2018. Wasserstein introspective neural networks. In IEEE Conference on Computer Vision and Pattern Recognition. 3702–3711. [170] Seul Lee, Jaehyeong Jo, and Sung Ju Hwang. 2022. Exploring Chemical Space with Score-based Out-o f-distribution Generation. arXiv preprint arXiv:2206.07632 (2022). [171] Alon Levkovitch, Eliya Nachmani, and Lior Wolf. 2022. Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models. arXiv preprint arXiv:2206.02246 (2022). [172] Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhi hai Xu, Qi Li, and Yue ting Chen. 2022. SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models. Neurocomputing 479 (2022), 47–59. [173] Junyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaoxuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2021. Textbox: A unified, modularized, and extensible framework for text generation. arXiv preprint arXiv:2101.02046 (2021). [174] Junyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. Pretrained language models for text generation: A survey. arXiv preprint arXiv:2105.10311 (2021). [175] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In Advances in Neural Information Processing Systems, Vol. 32. [176] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. 2022. Diffusion-LM Improves Controllable Text Generation. arXiv preprint arXiv:2205.14217 (2022). [177] Yikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, and Xiaogang Wang. 2019. Pastegan: A semi-parametric method to generate image from scene graph. Advances in Neural Information Processing Systems 32 (2019). [178] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. 2023. LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-t o-Image Diffusion Models with Large Language Models. arXiv preprint arXiv:2305.13655 (2023). [179] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin.

## 2022. Magic3D: High-Resolution Text-t o-3D Content Creation. arXiv preprint arXiv:2211.10440 (2022).

[180] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. 2022. Flow Matching for Generative Modeling. In The Eleventh International Conference on Learning Representations. [181] Jingwei Liu, Ling Yang, Hongyan Li, and Shenda Hong. 2024. Retrieval-Augmented Diffusion Models for Time Series Forecasting. In Advances in Neural Information Processing Systems. [182] Jingwei Liu, Ling Yang, Hao Luo, Fan Wang Hongyan Li, and Mengdi Wang. 2025. Preacher: Paper-t o-Video Agentic System. arXiv preprint arXiv:2508.09632 (2025). [183] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. 2021. Pseudo Numerical Methods for Diffusion Models on Manifolds. In International Conference on Learning Representations. [184] Shengchao Liu, Hongyu Guo, and Jian Tang. 2023. Molecular geometry pretraining with se (3)-invariant denoising distance matching. In International Conference on Learning Representations. [185] Xingchao Liu, Chengyue Gong, et a l. 2022. Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. In The Eleventh International Conference on Learning Representations. [186] Xingchao Liu, Lemeng Wu, Mao Ye, et a l. 2023. Learning Diffusion Bridges on Constrained Domains. In International Conference on Learning Representations. [187] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. 2022. Let us Build Bridges: Understanding and Extending Diffusion Generative Models. arXiv preprint arXiv:2208.14699 (2022). [188] Aaron Lou, Derek Lim, Isay Katsman, Leo Huang, Qingxuan Jiang, Ser Nam Lim, and Christopher M De Sa. 2020. Neural manifold ordinary differential equations. Advances in Neural Information Processing Systems 33 (2020), 17548–17558. [189] Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. 2022. Maximum Likelihood Training for Score-based Diffusion ODEs by High Order Denoising Score Matching. In International Conference on Machine Learning. 14429–14460. [190] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. 2022. DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. arXiv preprint arXiv:2206.00927 (2022). Accepted by ACM Computing Surveys

## 52 Yang et a l.

[191] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. 2022. Repaint: Inpainting using denoising diffusion probabilistic models. In IEEE Conference on Computer Vision and Pattern Recognition. 11461–11471. [192] Eric Luhman and Troy Luhman. 2021. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388 (2021). [193] Calvin Luo. 2022. Understanding Diffusion Models: A Unified Perspective. arXiv preprint arXiv:2208.11970 (2022). [194] Shengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. 2023. One transformer can understand both 2d & 3d molecular data. In International Conference on Learning Representations. [195] Shitong Luo and Wei Hu. 2021. Diffusion probabilistic models for 3d point cloud generation. In IEEE Conference on Computer Vision and Pattern Recognition. 2837–2845. [196] Shitong Luo and Wei Hu. 2021. Score-based point cloud denoising. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 4583–4592. [197] Shitong Luo, Chence Shi, Minkai Xu, and Jian Tang. 2021. Predicting molecular conformation via dynamic graph score matching. In Advances in Neural Information Processing Systems, Vol. 34. 19784–19795. [198] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. 2022. Antigen-specific antibody design and optimization with diffusion-based generative models. bioRxiv (2022). [199] Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, et a l. 2018. Multivariate time series imputation with generative adversarial networks. In Advances in Neural Information Processing Systems, Vol. 31. [200] Zhaoyang Lyu, Zhifeng Kong, XU Xudong, Liang Pan, and Dahua Lin. 2021. A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion. In International Conference on Learning Representations. [201] Zhaoyang Lyu, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo Dai. 2022. Accelerating Diffusion Models via Early Stop of the Diffusion Process. arXiv preprint arXiv:2205.12524 (2022). [202] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial Attacks. In International Conference on Learning Representations. [203] Emile Mathieu and Maximilian Nickel. 2020. Riemannian continuous normalizing flows. Advances in Neural Information Processing Systems 33 (2020), 2503–2515. [204] Siyuan Mei, Fuxin Fan, and Andreas Maier. 2022. Metal Inpainting in CBCT Projections Using Score-based Generative Model. arXiv preprint arXiv:2209.09733 (2022). [205] Gábor Melis, Chris Dyer, and Phil Blunsom. 2018. On the State of the Art of Evaluation in Neural Language Models. In International Conference on Learning Representations. https://openreview.net/forum?i d=ByJHuTgA[206] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. 2022. Concrete Score Matching: Generalized Score Matching for Discrete Data. In Advances in Neural Information Processing Systems. [207] Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. 2022. On Distillation of Guided Diffusion Models. In NeurIPS 2022 Workshop on Score-Based Methods. [208] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations. [209] Chenlin Meng, Jiaming Song, Yang Song, Shengjia Zhao, and Stefano Ermon. 2020. Improved Autoregressive Modeling with Distribution Smoothing. In International Conference on Learning Representations. [210] Chenlin Meng, Jiaming Song, Yang Song, Shengjia Zhao, and Stefano Ermon. 2021. Improved Autoregressive Modeling with Distribution Smoothing. In International Conference on Learning Representations. [211] Chenlin Meng, Yang Song, Wenzhe Li, and Stefano Ermon. 2021. Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems 34 (2021), 25359–25369. [212] Chenlin Meng, Lantao Yu, Yang Song, Jiaming Song, and Stefano Ermon. 2020. Autoregressive score matching. Advances in Neural Information Processing Systems 33 (2020), 6673–6683. [213] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. Regularizing and Optimizing LSTM Language Models. In International Conference on Learning Representations. https://openreview.net/forum?i d=SyyGPP0TZ [214] Nicholas Metropolis and Stanislaw Ulam. 1949. The monte carlo method. Journal of the American statistical association 44, 247 (1949), 335–341. [215] Jiquan Ngiam, Zhenghao Chen, Pang W Koh, and Andrew Y Ng. 2011. Learning deep energy models. In International Conference on Machine Learning. 1105–1112. [216] Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning. 8162–8171. [217] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In International Conference on Machine Learning. 16784–16804. [218] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. 2022. Diffusion Models for Adversarial Purification. arXiv preprint arXiv:2205.07460 (2022).

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 53[219] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. 2019. On the anatomy of mcmc-based maximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370 (2019). [220] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. 2019. On Learning Non-Convergent Short-Run MCMC Toward Energy-Based Model. arXiv preprint arXiv:1904.09770 (2019). [221] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. 2020. Permutation invariant graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and Statistics. PMLR, 4474–4484. [222] OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2023). [223] R OpenAI. 2023. Gpt-4 technical report. arxiv 2303.08774. View in Article 2 (2023), 3. [224] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2019. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations. [225] Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2020. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations. [226] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et a l. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730–27744. [227] Muzaffer Özbey, Salman UH Dar, Hasan A Bedel, Onat Dalmaz, Şaban Özturk, Alper Güngör, and Tolga Çukur. 2022. Unsupervised Medical Image Translation with Adversarial Diffusion Models. arXiv preprint arXiv:2207.08208 (2022). [228] George Papamakarios, Eric T Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. 2021. Normalizing Flows for Probabilistic Modeling and Inference. J. Mach. Learn. Res. 22, 57 (2021), 1–64. [229] Giorgio Parisi. 1981. Correlation functions and computer simulations. Nuclear Physics B 180, 3 (1981), 378–384. [230] Sung Woo Park, Kyungjae Lee, and Junseok Kwon. 2021. Neural Markov Controlled SDE: Stochastic Optimization for Continuous-Time Data. In International Conference on Learning Representations. [231] William Peebles and Saining Xie. 2022. Scalable Diffusion Models with Transformers. arXiv preprint arXiv:2212.09748 (2022). [232] Cheng Peng, Pengfei Guo, S Kevin Zhou, Vishal M Patel, and Rama Chellappa. 2022. Towards performant and reliable undersampled MR reconstruction via diffusion model sampling. In International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 623–633. [233] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. 2018. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32. [234] Stanislav Pidhorskyi, Donald A Adjeroh, and Gianfranco Doretto. 2020. Adversarial latent autoencoders. In IEEE Conference on Computer Vision and Pattern Recognition. 14104–14113. [235] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023). [236] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2022. Dreamfusion: Text-t o-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022). [237] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. 2021. Grad-tts: A diffusion probabilistic model for text-t o-speech. In International Conference on Machine Learning. 8599–8608. [238] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. 2022. Diffusion autoencoders: Toward a meaningful and decodable representation. In IEEE Conference on Computer Vision and Pattern Recognition. 10619–10629. [239] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. 2023. FateZero: Fusing Attentions for Zero-shot Text-based Video Editing. arXiv preprint arXiv:2303.09535 (2023). [240] Yixuan Qiu, Lingsong Zhang, and Xiao Wang. 2019. Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models. In International Conference on Learning Representations. [241] Lawrence R Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. IEEE 77, 2 (1989), 257–286. [242] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et a l. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning. 8748–8763. [243] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et a l. 2018. Improving language understanding by generative pre-training. (2018). [244] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et a l. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9. [245] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 (2022). [246] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-t o-image generation. In International Conference on Machine Learning. 8821–8831. [247] Martin Raphan and Eero P Simoncelli. 2007. Learning to be Bayesian without supervision. In Advances in neural information processing systems. 1145–1152. [248] Martin Raphan and Eero P Simoncelli. 2011. Least squares estimation without priors or supervision. Neural computation 23, 2 (2011), 374–420.

Accepted by ACM Computing Surveys

## 54 Yang et a l.

[249] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting. In International Conference on Machine Learning. 8857–8868. [250] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning. 8857–8868. [251] Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs M Bergmann, and Roland Vollgraf. 2020. Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows. In International Conference on Learning Representations. [252] Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. 2013. Characterization and computation of local Nash equilibria in continuous games. In 2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 917–924. [253] Danilo Rezende and Shakir Mohamed. 2015. Variational inference with normalizing flows. In International Conference on Machine Learning. 1530–1538. [254] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning. 1278–1286. [255] Benjamin Rhodes, Kai Xu, and Michael U Gutmann. 2020. Telescoping Density-Ratio Estimation. In Advances in Neural Information Processing Systems, Vol. 33. 4905–4916. [256] Oren Rippel and Ryan Prescott Adams. 2013. High-dimensional probability estimation with deep density models. arXiv preprint arXiv:1302.5125 (2013). [257] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition. 10684–10695. [258] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2022. DreamBooth: Fine Tuning Text-t o-Image Diffusion Models for Subject-Driven Generation. arXiv preprint arXiv:2208.12242 (2022). [259] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. 2022. Palette: Image-t o-image diffusion models. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings. 1–10. [260] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et a l. 2022. Photorealistic Text-t o-Image Diffusion Models with Deep Language Understanding. arXiv preprint arXiv:2205.11487 (2022). [261] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. 2022. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022). [262] Tim Salimans and Jonathan Ho. 2021. Progressive Distillation for Fast Sampling of Diffusion Models. In International Conference on Learning Representations. [263] Tim Salimans and Jonathan Ho. 2021. Should EBMs model the energy or the score?. In Energy Based Models Workshop-International Conference on Learning Representations. [264] David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus. 2019. High-dimensional multivariate forecasting with low-rank gaussian copula processes. In Advances in Neural Information Processing Systems, Vol. 32. [265] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020. DeepAR: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting 36, 3 (2020), 1181–1191. [266] Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. 2021. Step-unrolled Denoising Autoencoders for Text Generation. In International Conference on Learning Representations. [267] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural network model. IEEE transactions on neural networks 20, 1 (2008), 61–80. [268] Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. 2017. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on information processing in medical imaging. Springer, 146–157. [269] Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. 2021. Learning gradient fields for molecular conformation generation. In International Conference on Machine Learning. 9558–9568. [270] Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. 2020. Graphaf: a flow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382 (2020). [271] Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. 2022. Conditional simulation using diffusion Schrödinger bridges. arXiv preprint arXiv:2202.13460 (2022). [272] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. 2024. MVDream: Multi-view Diffusion for 3D Generation. In The Twelfth International Conference on Learning Representations. [273] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 2023. 3d neural field generation using triplane diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 20875–20886. [274] Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi, and Roger G Mark. 2012. Predicting i n-hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012. In 2012 Computing in Cardiology. IEEE, 245–248. [275] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et a l. 2022. Make-a-video: Text-t o-video generation without text-video data. arXiv preprint arXiv:2209.14792 (2022).

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 55[276] John Skilling. 1989. The eigenvalues of mega-dimensional matrices. In Maximum Entropy and Bayesian Methods. Springer, 455–466. [277] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning. 2256–2265. [278] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In International Conference on Machine Learning, Francis R. Bach and David M. Blei (Eds.). 2256–2265. [279] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising Diffusion Implicit Models. In International Conference on Learning Representations. [280] Ki-Ung Song. 2022. Applying Regularized Schrödinger-Bridge-Based Stochastic Process in Generative Modeling. arXiv preprint arXiv:2208.07131 (2022). [281] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. 2021. Maximum likelihood training of score-based diffusion models. In Advances in Neural Information Processing Systems, Vol. 34. 1415–1428. [282] Yang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, Vol. 32. [283] Yang Song and Stefano Ermon. 2020. Improved techniques for training score-based generative models. In Advances in Neural Information Processing Systems, Vol. 33. 12438–12448. [284] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. 2019. Sliced Score Matching: A Scalable Approach to Density and Score Estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019. 204. http: //auai.org/uai2019/proceedings/papers/204.pdf [285] Yang Song and Diederik P Kingma. 2021. How to train your energy-based models. arXiv preprint arXiv:2101.03288 (2021). [286] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. 2021. Solving Inverse Problems in Medical Imaging with Score-Based Generative Models. In International Conference on Learning Representations. [287] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations. [288] James C Spall. 2012. Stochastic optimization. In Handbook of computational statistics. Springer, 173–201. [289] Jiachen Sun, Weili Nie, Zhiding Yu, Z Morley Mao, and Chaowei Xiao. 2022. PointDP: Diffusion-driven Purification against Adversarial Attacks o n

## 3D Point Cloud Recognition. arXiv preprint arXiv:2208.09801 (2022).

[290] Jaesung Tae, Hyeongju Kim, and Taesu Kim. 2021. EdiTTS: Score-based Editing for Controllable Text-t o-Speech. arXiv preprint arXiv:2110.02584 (2021). [291] Huachun Tan, Guangdong Feng, Jianshuai Feng, Wuhong Wang, Yu-Jin Zhang, and Feng Li. 2013. A tensor-based method for missing traffic data completion. Transportation Research Part C: Emerging Technologies 28 (2013), 15–27. [292] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. 2023. Make-i t-3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint arXiv:2303.14184 (2023). [293] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. CSDI: Conditional score-based diffusion models for probabilistic time series imputation. In Advances in Neural Information Processing Systems, Vol. 34. 24804–24816. [294] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. 2022. Human motion diffusion model. arXiv preprint arXiv:2209.14916 (2022). [295] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Rémi Munos, Petar Veličković, and Michal Valko. 2021. Bootstrapped representation learning on graphs. arXiv preprint arXiv:2102.06514 (2021). [296] Lucas Theis, Aäron van den Oord, and Matthias Bethge. 2015. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844 (2015). [297] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, and Bin Cui. 2024. VideoTetris: Towards Compositional Text-t o-Video Generation. Advances in Neural Information Processing Systems (2024). [298] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et a l. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [299] Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. 2023. Effective Data Augmentation With Diffusion Models. In International Conference on Learning Representations. [300] Brian L Trippe, Jason Yim, Doug Tischer, Tamara Broderick, David Baker, Regina Barzilay, and Tommi Jaakkola. 2023. Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. In International Conference on Learning Representations. [301] Arash Vahdat, Karsten Kreis, and Jan Kautz. 2021. Score-based generative modeling in latent space. In Advances in Neural Information Processing Systems, Vol. 34. 11287–11302. [302] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. 2022. UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image. arXiv preprint arXiv:2210.09477 (2022). [303] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio. In The 9th ISCA Speech Synthesis Workshop. [304] Aäron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. 2016. Pixel Recurrent Neural Networks. In International Conference on Machine Learning, Maria-Florina Balcan and Kilian Q. Weinberger (Eds.). 1747–1756. Accepted by ACM Computing Surveys

## 56 Yang et a l.

[305] Alexander Vilesov, Pradyumna Chari, and Achuta Kadambi. 2023. Cg3d: Compositional generation for text-t o-3d via gaussian splatting. arXiv preprint arXiv:2311.17907 (2023). [306] Pascal Vincent. 2011. A connection between score matching and denoising autoencoders. Neural computation 23, 7 (2011), 1661–1674. [307] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In International Conference on Machine Learning. 1096–1103. [308] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. 2024. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8228–8238. [309] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. 2024. Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow. arXiv preprint arXiv:2410.07303 (2024). [310] Jinyi Wang, Zhaoyang Lyu, Dahua Lin, Bo Dai, and Hongfei Fu. 2022. Guided Diffusion Model for Adversarial Purification. arXiv preprint arXiv:2205.14969 (2022). [311] Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. 2025. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949 (2025). [312] Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. 2025. Co-evolving llm coder and unit tester via reinforcement learning. arXiv preprint arXiv:2506.03136 (2025). [313] Yufei Wang, Jiayi Zheng, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, and Daxin Jiang. 2022. KnowDA: All-i n-one knowledge mixture model for data augmentation in few-shot nlp. arXiv preprint arXiv:2206.10265 (2022). [314] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. 2023. ProlificDreamer: High-Fidelity and Diverse Text-t o-3D Generation with Variational Score Distillation. arXiv preprint arXiv:2305.16213 (2023). [315] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. 2022. Diffusion-GAN: Training GANs with Diffusion. arXiv preprint arXiv:2206.02262 (2022). [316] Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. 2021. Learning fast samplers for diffusion models by differentiating through sample quality. In International Conference on Learning Representations. [317] Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. 2021. Learning to efficiently sample from diffusion probabilistic models. arXiv preprint arXiv:2106.03802 (2021). [318] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et a l. 2022. Emergent Abilities of Large Language Models. Transactions on Machine Learning Research (2022). [319] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G Dimakis, and Peyman Milanfar. 2022. Deblurring via stochastic refinement. In IEEE Conference on Computer Vision and Pattern Recognition. 16293–16303. [320] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models. arXiv preprint arXiv:2303.04671 (2023). [321] Hao Wu, Jonas Köhler, and Frank Noe. 2020. Stochastic Normalizing Flows. In Advances in Neural Information Processing Systems, Vol. 33. 5933–5944. [322] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2022. Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-t o-Video Generation. arXiv preprint arXiv:2212.11565 (2022). [323] Quanlin Wu, Hang Ye, and Yuntian Gu. 2022. Guided Diffusion Model for Adversarial Purification from Random Noise. arXiv preprint arXiv:2206.10875 (2022). [324] Shoule Wu and Ziqiang Shi. 2021. ItôTTS and ItôWave: Linear Stochastic Differential Equation Is All You Need For Audio Generation. arXiv e-prints (2021), arXiv–2105. [325] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2020. Graph neural networks in recommender systems: a survey. ACM Computing Surveys (CSUR) (2020). [326] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems 32, 1 (2020), 4–24. [327] Julian Wyatt, Adam Leach, Sebastian M Schmon, and Chris G Willcocks. 2022. AnoDDPM: Anomaly Detection With Denoising Diffusion Probabilistic Models Using Simplex Noise. In IEEE Conference on Computer Vision and Pattern Recognition. 650–656. [328] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. 2021. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804 (2021). [329] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. 2016. A theory of generative convnet. In International Conference on Machine Learning. 2635–2644. [330] Pan Xie, Qipeng Zhang, Zexian Li, Hao Tang, Yao Du, and Xiaohui Hu. 2022. Vector Quantized Diffusion Model with CodeUnet for Text-t o-Sign Pose Sequences Generation. arXiv preprint arXiv:2208.09141 (2022). [331] Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi S Jaakkola. 2021. Crystal Diffusion Variational Autoencoder for Periodic Material Generation. In International Conference on Learning Representations. [332] Yutong Xie and Quanzheng Li. 2022. Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction. arXiv preprint arXiv:2203.03623 (2022).

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 57[333] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. 2023. Open-Vocabulary Panoptic Segmentation with Text-t o-Image Diffusion Models. In IEEE Conference on Computer Vision and Pattern Recognition. [334] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2024. Imagereward: Learning and evaluating human preferences for text-t o-image generation. Advances in Neural Information Processing Systems 36 (2024). [335] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Shenghua Gao. 2022. Dream3D: Zero-Shot Text-t o-3D Synthesis Using 3D Shape Prior and Text-t o-Image Diffusion Models. arXiv preprint arXiv:2212.14704 (2022). [336] Minghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. 2021. Self-supervised graph-level representation learning with local and global structure. In International Conference on Machine Learning. 11548–11558. [337] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. 2021. GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation. In International Conference on Learning Representations. [338] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. 2022. Versatile Diffusion: Text, Images and Variations All in One Diffusion Model. arXiv preprint arXiv:2211.08332 (2022). [339] Tijin Yan, Hongwei Zhang, Tong Zhou, Yufeng Zhan, and Yuanqing Xia. 2021. ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models. arXiv preprint arXiv:2106.10121 (2021). [340] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. 2022. Diffsound: Discrete Diffusion Model for Text-t o-sound Generation. arXiv preprint arXiv:2207.09983 (2022). [341] Jie Yang, Ruijie Xu, Zhiquan Qi, and Yong Shi. 2021. Visual anomaly detection for images: A survey. arXiv preprint arXiv:2109.13157 (2021). [342] Kevin Yang and Dan Klein. 2021. FUDGE: Controlled Text Generation With Future Discriminators. (2021). [343] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. 2024. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8941–8951. [344] Ling Yang and Shenda Hong. 2022. Omni-Granular Ego-Semantic Propagation for Self-Supervised Graph Representation Learning. arXiv preprint arXiv:2205.15746 (2022). [345] Ling Yang and Shenda Hong. 2022. Unsupervised Time-Series Representation Learning with Iterative Bilinear Temporal-Spectral Fusion. In International Conference on Machine Learning. 25038–25054. [346] Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao Li, Wentao Zhang, Bin Cui, Bernard Ghanem, and Ming-Hsuan Yang. 2022. DiffusionBased Scene Graph to Image Generation with Masked Contrastive Pre-Training. arXiv preprint arXiv:2211.11138 (2022). [347] Ling Yang, Zhilin Huang, Zhilong Zhang, Zhongyi Liu, Shenda Hong, Wentao Zhang, Wenming Yang, Bin Cui, and Luxia Zhang. 2024. Graphusion: Latent Diffusion for Graph Generation. IEEE Transactions on Knowledge and Data Engineering (2024). [348] Ling Yang, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, and Yu Liu. 2020. Dpgn: Distribution propagation graph network for few-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition. 13390–13399. [349] Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, and CUI Bin. 2023. Improving Diffusion-Based Image Synthesis with Context Prediction. In Thirty-seventh Conference on Neural Information Processing Systems. [350] Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, and Bin Cui. 2024. Structure-Guided Adversarial Training of Diffusion Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition. [351] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. 2025. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809 (2025). [352] Ling Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. 2025. Reasonflux: Hierarchical llm reasoning via scaling thought templates. arXiv preprint arXiv:2502.06772 (2025). [353] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. 2024. Mastering Text-t o-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs. In International Conference on Machine Learning. [354] Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, and Bin Cui. 2024. Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models. arXiv preprint arXiv:2406.04271 (2024). [355] Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. 2024. EditWorld: Simulating World Dynamics for Instruction-Following Image Editing. arXiv preprint arXiv:2405.14785 (2024). [356] Ling Yang, Zixiang Zhang, Junlin Han, Bohan Zeng, Runjia Li, Philip Torr, and Wentao Zhang. 2024. Semantic Score Distillation Sampling for Compositional Text-t o-3D Generation. arXiv preprint arXiv:2410.09009 (2024). [357] Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, and Bin CUI. 2024. Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing. In International Conference on Learning Representations. [358] Ling Yang, Zhilong Zhang, Wentao Zhang, and Shenda Hong. 2023. Score-Based Graph Generative Modeling with Self-Guided Latent Diffusion. (2023). https://openreview.net/forum?i d=AykEgQNPJEK [359] Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. 2024. Consistency Flow Matching: Defining Straight Flows with Velocity Consistency. arXiv preprint arXiv:2407.02398 (2024). [360] Ruihan Yang and Stephan Mandt. 2022. Lossy Image Compression with Conditional Diffusion Models. arXiv preprint arXiv:2209.06950 (2022). [361] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. 2022. Diffusion probabilistic modeling for video generation. arXiv preprint arXiv:2203.09481 (2022). Accepted by ACM Computing Surveys

## 58 Yang et a l.

[362] Xiuwen Yi, Yu Zheng, Junbo Zhang, and Tianrui Li. 2016. ST-MVL: filling missing values in geo-sensory time series data. In Proceedings of the 25th International Joint Conference on Artificial Intelligence. [363] Jongmin Yoon, Sung Ju Hwang, and Juho Lee. 2021. Adversarial purification with score-based generative models. In International Conference on Machine Learning. 12062–12072. [364] Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. 2019. Time-series generative adversarial networks. In Advances in Neural Information Processing Systems, Vol. 32. [365] Zebin You, Yong Zhong, Fan Bao, Jiacheng Sun, Chongxuan Li, and Jun Zhu. 2023. Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels. arXiv preprint arXiv:2302.10586 (2023). [366] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917 (2022). [367] Peiyu Yu, Sirui Xie, Xiaojian Ma, Baoxiong Jia, Bo Pang, Ruiqi Gao, Yixin Zhu, Song-Chun Zhu, and Ying Nian Wu. 2022. Latent Diffusion Energy-Based Model for Interpretable Text Modelling. In International Conference on Machine Learning. 25702–25720. [368] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. 2022. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571 (2022). [369] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et a l.

## 2021. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 (2021).

[370] Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter Battaglia, Razvan Pascanu, and Jonathan Godwin. 2023. Pre-training via denoising for molecular property prediction. In International Conference on Learning Representations. [371] Bohan Zeng, Shanglin Li, Yutang Feng, Ling Yang, Hong Li, Sicheng Gao, Jiaming Liu, Conghui He, Wentao Zhang, Jianzhuang Liu, Baochang Zhang, and Shuicheng Yan. 2023. Ipdreamer: Appearance-controllable 3d object generation with image prompts. arXiv preprint arXiv:2310.05375 (2023). [372] Bohan Zeng, Ling Yang, Siyu Li, Jiaming Liu, Zixiang Zhang, Juanxi Tian, Kaixin Zhu, Yongzhen Guo, Fu-Yun Wang, Minkai Xu, Stefano Ermon, and Wentao Zhang. 2024. Trans4D: Realistic Geometry-Aware Transition for Compositional Text-t o-4D Synthesis. arXiv preprint arXiv:2410.07155 (2024). [373] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. 2022. LION: Latent Point Diffusion Models for 3D Shape Generation. In Advances in Neural Information Processing Systems. [374] Lvmin Zhang and Maneesh Agrawala. 2023. Adding conditional control to text-t o-image diffusion models. arXiv preprint arXiv:2302.05543 (2023). [375] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. 2022. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001 (2022). [376] Qinsheng Zhang and Yongxin Chen. 2021. Diffusion Normalizing Flow. In Advances in Neural Information Processing Systems, Vol. 34. 16280–16291. [377] Qinsheng Zhang and Yongxin Chen. 2022. Fast Sampling of Diffusion Models with Exponential Integrator. arXiv preprint arXiv:2204.13902 (2022). [378] Qinsheng Zhang, Molei Tao, and Yongxin Chen. 2022. gDDIM: Generalized denoising diffusion implicit models. arXiv preprint arXiv:2206.05564 (2022). [379] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et a l. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022). [380] Wenrui Zhang, Ling Yang, Shijia Geng, and Shenda Hong. 2022. Cross Reconstruction Transformer for Self-Supervised Time Series Representation Learning. arXiv preprint arXiv:2205.09928 (2022). [381] Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Kaini Wang, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, and Bin Cui. 2024. RealCompo: Balancing Realism and Compositionality Improves Text-t o-Image Diffusion Models. arXiv preprint arXiv:2402.12908 (2024). [382] Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, and Bin Cui. 2024. IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-t o-Image Generation. arXiv preprint arXiv:2410.07171 (2024). [383] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023. Multimodal chain-o f-thought reasoning in language models. arXiv preprint arXiv:2302.00923 (2023). [384] Junbo Zhao, Michael Mathieu, and Yann LeCun. 2016. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126 (2016). [385] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. 2022. Egsde: Unpaired image-t o-image translation via energy-guided stochastic differential equations. arXiv preprint arXiv:2207.06635 (2022). [386] Yue Zhao, Zain Nasrullah, and Zheng Li. 2019. PyOD: A Python Toolbox for Scalable Outlier Detection. Journal of Machine Learning Research 20 (2019), 1–7. [387] Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. 2022. Truncated diffusion probabilistic models. arXiv preprint arXiv:2202.09671 (2022). [388] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. 2023. Uni-mol: A universal 3d molecular representation learning framework. In International Conference on Learning Representations. [389] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks: A review of methods and applications. AI Open 1 (2020), 57–81. [390] Linqi Zhou, Yilun Du, and Jiajun Wu. 2021. 3d shape generation and completion through point-voxel diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5826–5835.

Accepted by ACM Computing SurveysDiffusion Models: A Comprehensive Survey of Methods and Applications 59[391] Haowei Zhu, Ling Yang, Jun-Hai Yong, Wentao Zhang, and Bin Wang. 2024. Distribution-Aware Data Expansion with Diffusion Models. arXiv preprint arXiv:2403.06741 (2024). [392] Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, and Yan Yan. 2022. Discrete contrastive diffusion for cross-modal and conditional generation. arXiv preprint arXiv:2206.07771 (2022). [393] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020. Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131 (2020). [394] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. 2024. Vlogger: Make Your Dream A Vlog. arXiv preprint arXiv:2401.09414 (2024). [395] Roland S Zimmermann, Lukas Schott, Yang Song, Benjamin A Dunn, and David A Klindt. 2021. Score-based generative classifiers. arXiv preprint arXiv:2110.00473 (2021). [396] Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, and Mengdi Wang. 2025. ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-o f-Thought Reasoning in LLMs. arXiv preprint arXiv:2506.18896 (2025). Accepted by ACM Computing Surveys