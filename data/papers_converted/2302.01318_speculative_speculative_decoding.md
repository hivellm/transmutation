2023-2-3


## Accelerating Large Language Model Decoding

with Speculative SamplingCharlie Chen1, Sebastian Borgeaud1, GeoÔ¨Ärey Irving1, Jean-Baptiste Lespiau1, Laurent Sifre1 and John Jumper1


## 1All authors from DeepMind

We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modiÔ¨Åed rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2‚Äì2.5√ó decoding speedup in a distributed setup, without compromising the sample quality or making modiÔ¨Åcations to the model itself.


## Introduction

Scaling transformer models to 500B+ parameters has led to large performance improvements on many natural language, computer vision and reinforcement learning tasks (Arnab et a l., 2021; Brown et a l., 2020; Chowdhery et a l., 2022; Dosovitskiy et a l., 2020; HoÔ¨Ämann et a l., 2022; Rae et a l., 2021). However, transformer decoding remains a highly costly and ineÔ¨Écient process in this regime.

Transformer sampling is typically memory bandwidth bound (Shazeer, 2019), so for a given set of hardware, the time to generate a single token in transformer models is proportional to a Ô¨Årst order approximation to the size of parameters and the size of the transformer memory. The size of language models also necessitates serving with model parallelism ‚Äì adding communication overheads (Pope et a l., 2022) and multiplying resource requirements. Since each new token depends on the past, many such transformer calls are required to sample a new sequence.

We present an algorithm to accelerate transformer sampling for latency critical applications, which we call speculative sampling (SpS). This is achieved b y:

1. Generating a short draft of length ùêæ. This can be attained with either a parallel model (Stern et a l., 2018) or by calling a faster, auto-regressive model ùêæ times. We shall refer to this model as the draft model, and focus on the case where it is auto-regressive.


## 2. Scoring the draft using the larger, more powerful model from we wish to sample from. We shall

refer to this model as the target model. 3. Using a modiÔ¨Åed rejection sampling scheme, accept a subset of the ùêæ draft tokens from left to right, recovering the distribution of the target model in the process.

Intuitively, there are often sequences where the next token might be ‚Äúobvious‚Äù. Therefore, if there is strong agreement between the draft and target model‚Äôs distributions on a given token or sub-sequence of tokens, this setup permits the generation of multiple tokens each time the target model is called.

We show that the expected acceptance rate of draft tokens is suÔ¨Écient to oÔ¨Äset the overhead of theCorresponding author( s): ccharlie@deepmind.com ¬© 2023 DeepMind. All rights reservedarXiv:2302.01318v1  [c s.CL]  2 Feb 2023 Accelerating Large Language Model Decoding with Speculative Samplingdrafting process for large language models, resulting in an eÔ¨Äective and practical method for reducing sampling latency without the need for modifying the target model or biasing the sample distribution. Depending on the evaluation domain, SpS leads to a 2‚Äì2.5√ó speedup when sampling from Chinchilla (HoÔ¨Ämann et a l., 2022). Notably, the mean tokens per second with SpS often exceeds the idealised ceiling on auto-regressive sampling speed imposed by the memory bandwidth.

Related WorkThere has been a substantial body of work focused on improving sampling latency of large transformers and other auto-regressive models.

Since sampling performance is heavily coupled with the model size in memory, quantisation to int8 or even int4 (Dettmers et a l., 2022; Yao et a l., 2022) and distillation (Jiao et a l., 2020; Sanh et a l., 2019) of transformers are eÔ¨Äective techniques for reducing sampling latency with little to no performance penalty. The observation that model size contributes less to the Ô¨Ånal performance than expected (HoÔ¨Ämann et a l., 2022) has also encouraged smaller language models in general.

During sampling, a cache of the keys and values is maintained for every attention layer, and could become a memory bandwidth bottleneck as the batch size increases. Methods such as multi-query attention (Shazeer, 2019) aims to improve sampling performance by shrinking this cache. However these techniques are most eÔ¨Äective at maximising throughout (a t larger batch sizes) instead of latency, especially for larger models where the majority of the memory bandwidth budget is consumed by the parameters.

Using a combination of the above techniques, in addition to a number of low-level optimisations to TPUs, Pope et a l. (2022) have greatly improved the serving latency and eÔ¨Éciency of PaLM 540B.

There is an existing body of similar work exploiting the eÔ¨Éciency of transformers and sequence models operating in parallel. This includes block parallel sampling (Stern et a l., 2018), aggressive decoding (Ge et a l., 2022), in addition to some work in parallelizing autoregressive models in the image domain (Song et a l., 2021; Wiggers and Hoogeboom, 2020). These methods have yet to be adapted to typical language model use-cases since they either only work with greedy sampling, bias the results or are focused on other modalities. Further, to our knowledge none of these techniques have been scaled to distributed setups, which is necessary for the most expensive decoders with the tens or hundreds of billions of parameters.

Coincidentally, the work in this manuscript was undertaken concurrently and independently of the work on speculative decoding from Leviathan et a l. (2022). We focus more heavily the distributed serving setting for large models and oÔ¨Äer some incremental optimisations, but otherwise the core underlying idea is the same.

Auto-regressive SamplingWhilst transformers can be trained eÔ¨Éciently and in parallel on TPUs and GPUs, samples are typically drawn auto-regressively (See algorithm 1). For most applications, auto-regressive sampling (ArS) is highly memory bandwidth bound and thus cannot make eÔ¨Äective use of modern accelerator hardware (Shazeer, 2019). A memory bound model call only generates a single token for every sequence in the batch, hence generating multiple tokens introduces a large amount of latency in any system which makes use of i t. 2Accelerating Large Language Model Decoding with Speculative SamplingThis is especially problematic as the number of parameters in the model increases. Since all the model parameters need to pass through at least one accelerator chip, the model size divided by the total memory bandwidth across all chips gives us a hard ceiling on the maximum auto-regressive sampling speed. Larger models also require serving on multiple accelerators, introducing a further source of latency due to inter-device communication overheads.

Algorithm 1 Auto-regressive (ArS) with Auto-Regressive ModelsGiven auto-regressive target model ùëû(.|.) and initial prompt sequence ùë•1, . . . , ùë•ùë° and target sequence length ùëá. Initialise ùëõ ‚Üê ùë°. while ùëõ < ùëá do Sample ùë•ùëõ+1 ‚àº ùëû(ùë•|ùë•1, . . . , ùë•ùëõ) ùëõ ‚Üê ùëõ + 1 end whileAlgorithm 2 Speculative Sampling (SpS) with Auto-Regressive Target and Draft ModelsGiven lookahead ùêæ and minimum target sequence length ùëá. Given auto-regressive target model ùëû(.|.), and auto-regressive draft model ùëù(.|.), initial prompt sequence ùë•0, . . . , ùë•ùë°. Initialise ùëõ ‚Üê ùë°. while ùëõ < ùëá do for ùë° = 1 : ùêæ do Sample draft auto-regressively Àúùë•ùë° ‚àº ùëù(ùë•|, ùë•1, . . . , ùë•ùëõ, Àúùë•1, . . . , Àúùë•ùë°‚àí1) end for In parallel, compute ùêæ + 1 sets of logits from drafts Àúùë•1, . . . , Àúùë•ùêæ :

ùëû(ùë•|, ùë•1, . . . , ùë•ùëõ), ùëû(ùë•|, ùë•1, . . . , ùë•ùëõ, Àúùë•1), . . . , ùëû(ùë•|, ùë•1, . . . , ùë•ùëõ, Àúùë•1, . . . , Àúùë•ùêæ)for ùë° = 1 : ùêæ do Sample ùëü ‚àº ùëà [0, 1] from a uniform distribution. if ùëü < min (1, ùëû(ùë• |ùë•1,...,ùë•ùëõ+ùë°‚àí1) ùëù(ùë• |ùë•1,...,ùë•ùëõ+ùë°‚àí1) ), then Set ùë•ùëõ+ùë° ‚Üê Àúùë•ùë° and ùëõ ‚Üê ùëõ + 1. else sample ùë•ùëõ+ùë° ‚àº (ùëû(ùë•|ùë•1, . . . , ùë•ùëõ+ùë°‚àí1) ‚àí ùëù(ùë•|ùë•1, . . . , ùë•ùëõ+ùë°‚àí1))+ and exit for loop. end if end for If all tokens ùë•ùëõ+1, . . . , ùë•ùëõ+ùêæ are accepted, sample extra token ùë•ùëõ+ùêæ+1 ‚àº ùëû(ùë•|, ùë•1, . . . , ùë•ùëõ, ùë•ùëõ+ùêæ) and set ùëõ ‚Üê ùëõ + 1. end whileSpeculative SamplingConditional ScoringFor speculative sampling (See algorithm 2), we Ô¨Årst make the observation that computing the logits of a short continuation of ùêæ tokens in parallel has a very similar latency to that of sampling a single3Accelerating Large Language Model Decoding with Speculative Samplingtoken. We focus our attention on large transformers, sharded in the Megatron style (Shoeybi et a l.,


## 2019). For these models the majority of sampling time can be attributed to three components:


## 1. Linear Layers: For small batch sizes, each linear layer only processes a small number o f

embeddings. This causes the dense matrix multiplies in the feed-forward layers, queries, keys, values computations and the Ô¨Ånal attention projection to become memory bound. For small ùêæ, this will continue to be memory bound and therefore take a similar amount of time.


## 2. The Attention Mechanism: The attention mechanism is also memory bound. During sampling,

w e maintain a cache of all the keys and values of the previous tokens in the sequence to avoid r e-computation. These KV-caches are large, and accounts for the majority of the memory bandwidth utilisation for the attention mechanism. However, since the KV-cache size does not change as we increase ùêæ, there is little to no delta in this component.


## 3. All-reduces: As models grow in size, its parameters need to be divided across multiple accelera-

tors, leading to communication overheads. With Megatron, this manifests itself as an all-reduce after every feed-forward and attention layer. Since only the activations for a small number of tokens are transmitted, this operation is typically latency bound instead of throughput bound for both sampling and scoring (for small ùêæ). Again, this results in a similar amount of time spent in the two cases.

Other sources of overhead may exist, depending on the exact transformer implementation. Therefore it is still possible that the choice of positioning encoding, decoding method (e.g. a sort might be required for nucleus sampling), hardware limitations etc. can introduce some deltas between scoring and sampling. However, if the conditions are met such that the above components dominate then scoring should not be signiÔ¨Åcantly slower for small ùêæ.

ModiÔ¨Åed Rejection SamplingWe require a method to recover the distribution of the target model from samples from the draft model, and logits of said tokens from both models.

To achieve this, we introduce the following rejection sampling scheme of the drafted tokens. Given a sequence of tokens ùë•1, . . . , ùë•ùëõ, and ùêæ draft tokens Àúùë•ùëõ+1, . . . , Àúùë•ùëõ+ùêæ generated from ùëù(.|.), we accept Àúùë•ùëõ+1 with probability: min (1, ùëû(Àúùë•ùëõ+1|ùë•1, . . . , ùë•ùëõ) ùëù(Àúùë•ùëõ+1|ùë•1, . . . , ùë•ùëõ) )Where ùëû(Àúùë•ùëõ+1|ùë•1, . . . , ùë•ùëõ) and ùëù(Àúùë•ùëõ+1|ùë•1, . . . , ùë•ùëõ) are the probability of Àúùë•ùëõ+1 according to the target and draft models respectively, conditioned on the context so far.

If the token is accepted, we set ùë•ùëõ+1 ‚Üê Àúùë•ùëõ+1 and repeat the process for Àúùë•ùëõ+2 until either a token is rejected or all tokens have been accepted.

If Àúùë•ùëõ+1 is rejected, we resample ùë•ùëõ+1 from the following distribution:

ùë•ùëõ+1 ‚àº (ùëû(ùë•|ùë•1, . . . , ùë•ùëõ) ‚àí ùëù(ùë•|ùë•1, . . . , ùë•ùëõ))+Where (.)+ denotes: ( ùëì (ùë•))+ = max(0, ùëì (ùë•)) √çùë• max(0, ùëì (ùë•))By applying this sequentially, we recover the distribution of the target model for the accepted tokens (see proof in Theorem 1) within hardware numerics. Note that: 4Accelerating Large Language Model Decoding with Speculative Sampling‚Ä¢ At least one token will always be generated from a draft-accept loop ‚Äì if the Ô¨Årst token is rejected, a valid token is resampled. ‚Ä¢ Since the Ô¨Ånal token of the draft gives us the logits for the next token, if every drafted token is accepted, we can sample from it normally. This gives us a maximum of ùêæ + 1 tokens per loop, over the naive implementation which would only return ùêæ tokens.

With standard sampling methods such as nucleus, top-k sampling and adjusting temperature, we can modify the probabilities accordingly before applying this rejection sampling scheme. We have observed that the overall acceptance rate is robust to the exact parameters used.

Because we do not interact with the body of the transformer itself, this method can be used in conjunction many other techniques for accelerating or optimising the memory use of sampling, such as quantisation and multi-query attention.

Choice of Draft ModelsSince the acceptance criterion guarantees the distribution of the target model in our samples, we are free to choose the method for drafting a continuation as long as it exposes logits, and there is a high enough acceptance rate and/o r low enough latency to break-even. There exist several approaches here:

‚Ä¢ Incorporating draft generation into the target model, and train the model from the start. This is the strategy used by Stern et a l. (2018), which adds multiple heads into the transformer to generate multiple tokens. ‚Ä¢ Using sequence level distillation (Kim and Rush, 2016) to generate a second model which predicts ùêæ tokens in parallel. This strategy was employed by Ge et a l. (2022). ‚Ä¢ Set a portion of the activations of the target model as an input to the draft model, and train the draft model with this input.

Although these methods will likely yield powerful drafts, they require a large number of data generated from the target model or changes to the target model. Sequence level distillation in particular would require a large compute budget. This makes them less practical for large scale applications.

Whilst large language models produce better samples, intuitively there are "easier" tokens to predict for which smaller models may be suÔ¨Écient. Therefore we may simply use a smaller version of the target language model as the draft and obtain high acceptance rates. This would also be convenient from an engineering and workÔ¨Çow perspective, since robust tooling for such models should already exist to train the target model in the Ô¨Årst place.


## Results

We train a 4 billion parameter draft model optimised for sampling latency on 16 TPU v4s ‚Äì the same hardware that is typically used to serve Chinchilla for research purposes. This model was trained with the same tokeniser and dataset as Chinchilla, with a slightly smaller width and with only 8 layers. The relatively few number of layers allows it to achieve a sampling speed of 1.8ms/token compared to 14.1ms/token for Chinchilla. For details, please refer to the hyperparameters in Table 2.

For distributed setups it is insuÔ¨Écient to naively choose a small model as the draft, since diÔ¨Äerent models have diÔ¨Äerent optimal inference setups. For example, it is typical to serve Chinchilla 70B5Accelerating Large Language Model Decoding with Speculative SamplingTable 1 | Chinchilla performance and speed on XSum and HumanEval with naive and speculative sampling at batch size 1 and ùêæ = 4. XSum was executed with nucleus parameter ùëù = 0.8, and HumanEval with ùëù = 0.95 and temperature 0.8.

Sampling Method Benchmark Result Mean Token Time Speed UpArS (Nucleus) XSum (ROUGE-2) 0.112 14.1ms/Token 1√ó SpS (Nucleus) 0.114 7.52ms/Token 1.92√óArS (Greedy) XSum (ROUGE-2) 0.157 14.1ms/Token 1√ó SpS (Greedy) 0.156 7.00ms/Token 2.01√óArS (Nucleus) HumanEval (100 Shot) 45.1% 14.1ms/Token 1√ó SpS (Nucleus) 47.0% 5.73ms/Token 2.46√óo n 16 TPU v4s (where it achieves the aforementioned 14.1ms/token), whereas a chinchilla-optimal


## 7B achieves its lowest sampling latency on 4 TPU v4s (where it achieves 5ms/token). For smaller

models, the additional memory bandwidth and Ô¨Çops are insuÔ¨Écient to oÔ¨Äset the additional communication overhead between more devices ‚Äì serving a 7B on 16 TPUs actually increases the latency. This means the 7B would provide only a modest speedup if used as a draft with its optimal topology, and we will not make full utilisation of the hardware during drafting.

We can sidestep this issue by training a wider model with a relatively few number of layers in order to minimise communication overhead. It has been observed that the performance of language models is relatively robust to changes in model aspect ratio (Levine et a l., 2020), so this allows us to serve a powerful draft model which can be sampled rapidly on the same hardware as the target model.

Evaluation on XSum and HumanEvalWe evaluate speculative sampling with Chinchilla on two tasks and summarize the results in Table 1:

‚Ä¢ The XSum (Narayan et a l., 2018) benchmark. This is a natural language summarisation task using a 1-shot prompt where we sample a total of 11,305 sequences with a maximum sequence length 128. ‚Ä¢ The 100-shot HumanEval task (Chen et a l., 2021). This is a code generation task involves the generation of 16,400 samples with a maximum sequence length of 512.

Even with greedy sampling, a single token deviating due to numerics could result in two sequences diverging wildly. Since pseudo-random seeds are processed diÔ¨Äerently between ArS and SpS, and because the diÔ¨Äerent computation graphs lead to diÔ¨Äerent numerics, we cannot not expect identical outputs. However, we expect the samples to come from the same distribution within numerics and we empirically verify this by evaluating these benchmarks.

We run the tasks at batch size 1 with SpS and ArS. The time taken per SpS/ArS loop has low variance, and we can measure it directly from TPU proÔ¨Åles. To obtain the average speedup, standard deviations and other metrics, we log the amount of tokens generated for each speculative loop. In Table 1 we show the performance on the XSum and HumanEval benchmarks for naive and speculative sampling with Chinchilla. 6Accelerating Large Language Model Decoding with Speculative SamplingWe obtain a substantial speedup in both tasks, with HumanEval reaching speedups of almost 2.5√ó. Yet, we have parity in the benchmark metrics ‚Äì the underlying samples distribution is provably the same up to numerics, and this veriÔ¨Åes that the draft model is not biasing the results empirically. In the case of HumanEval and greedy XSum, this speedup exceeded the theoretical memory bandwidth limit of the hardware for autoregressive sampling (model size divided by the total memory bandwidth).

Acceptance rate changes per domainIt is apparent that the acceptance rate is dependent on the application and the decoding method. HumanEval achieves a signiÔ¨Åcantly larger speedup ‚Äî We hypothesize that this is due to a combination of code containing a lot of common sub-sequences (e.g. for ii n range( len( arr)): would be relatively easy for a draft model to guess), is often decomposed into a smaller set of shorter tokens and the temperature value sharpening both the draft and target logits.


## 0 1 2 3 4 5 6 7

Number of draft tokens (K)40060080010001200140016001800ms Mean Sampling Time (128 tokens)Human Eval XSum


## 0 1 2 3 4 5 6 7

Number of draft tokens (K)0.50.60.70.80.91.0 Acceptance rateHuman Eval XSum


## 0 1 2 3 4 5 6 7

Number of draft tokens (K)1416182022242628ms Total loop timeFigure 1 | Left: The average time to generate 128 tokens, with standard deviation. Note that as ùêæ increases, the overall speedup plateaus or even regresses, with XSum being optimal at ùêæ = 3. The variance consistently increases with ùêæ. Middle: The average number of tokens accepted divided by ùêæ + 1 ‚Äì this serves as a measure of the overall eÔ¨Éciency of the modiÔ¨Åed rejection scheme, which decreases with the lookahead. Right: Average time per loop increases approximately linearly with ùêæ due to the increased number of model calls. Note that the gradient is slightly higher than the sampling speed of the draft model, due to additional overheads in nucleus decoding.

Trade oÔ¨Ä between longer drafts and more frequent scoringWe visualise the trade-oÔ¨Ä of increasing ùêæ, the number of tokens sampled by the draft model in Figure 1. As ùêæ increases, we need fewer scoring calls from the large models to generate the same sequence length, potentially giving us a larger speedup. However, the total loop time increases approximately linearly with the larger number of draft model calls and small increases in the scoring time. The overall eÔ¨Éciency of the proportion of accepted tokens decreases as ùêæ increases, since later tokens depend on the acceptance of previous tokens. This results in the average speedup plateauing or even degrading with a larger ùêæ (for example, XSum with nucleus‚Äôs latency is minimised at ùêæ = 3), depending on the domain. 7Accelerating Large Language Model Decoding with Speculative SamplingFurther, even though larger values of ùêæ may yield marginally greater mean speedups in certain circumstances, it also increases variance of the time to generate a full sequence. This could be problematic for settings where the P90, P99 latencies of concern.


## Conclusion

In this work, we demonstrate a new algorithm and workÔ¨Çow for accelerating the decoding of language models. Speculative sampling does not require making any modiÔ¨Åcations to the target language model‚Äôs parameters or architecture, is provably lossless within numerics, scales well with the appropriate draft model and complements many existing techniques for reducing latency in the small batch size setting.

We optimise and scale the technique to Chinchilla 70B using a draft model which was easy to train with existing infrastructure, demonstrating that it yields a large speedup across benchmark tasks and common decoding methods in the process. We verify that it is indeed lossless empirically in its downstream tasks.


## References

A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lucic, and C. Schmid. Vivit: A video vision transformer. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6816‚Äì6826. IEEE Computer Society, 2021.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et a l. Language models are few-shot learners. Advances in neural information processing systems, 33:1877‚Äì1901, 2020.

M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.

A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et a l. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.

A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et a l. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

T. Ge, H. Xia, X. Sun, S. Chen, and F. Wei. Lossless acceleration for seq2seq generation with aggressive decoding. ArXiv, abs/2205.10350, 2022. 8Accelerating Large Language Model Decoding with Speculative SamplingJ. HoÔ¨Ämann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et a l. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163‚Äì4174, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v 1/2020.Ô¨Åndings-emnlp.372. URL https://aclanthology.org/2020. findings-emnlp.372.

Y. Kim and A. M. Rush. Sequence-level knowledge distillation. CoRR, abs/1606.07947, 2016. URL http://arxiv.org/abs/1606.07947.

Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. ArXiv, abs/2211.17192, 2022.

Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-t o-width interplay in self-attention. arXiv preprint arXiv:2006.12467, 2020.

S. Narayan, S. B. Cohen, and M. Lapata. Don‚Äôt give me the details, just the summary! topicaware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797‚Äì1807, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v 1/D18-1206. URL https://aclanthology.org/D18-1206.

R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. EÔ¨Éciently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.

J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. HoÔ¨Ämann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et a l. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.

V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

N. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. URL http://arxiv.org/abs/1911.02150.

M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-l m: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.

Y. Song, C. Meng, R. Liao, and S. Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9791‚Äì9800. PMLR, 18‚Äì24 Jul 2021. URL https://proceedings.mlr.press/v139/song21a.html.

M. Stern, N. Shazeer, and J. Uszkoreit. Blockwise parallel decoding for deep autoregressive models. CoRR, abs/1811.03115, 2018. URL http://arxiv.org/abs/1811.03115.

A. Wiggers and E. Hoogeboom. Predictive sampling with forecasting autoregressive models. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10260‚Äì10269. PMLR, 13‚Äì18 Jul


## 2020. URL https://proceedings.mlr.press/v119/wiggers20a.html.

9Accelerating Large Language Model Decoding with Speculative SamplingZ. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: EÔ¨Écient and aÔ¨Äordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.

Supplementary MaterialsAuthor Contributions‚Ä¢ Initial proposal: Charlie Chen, John Jumper and GeoÔ¨Ärey Irving ‚Ä¢ Initial Implementation, Optimisation and Scaling: Charlie Chen ‚Ä¢ ModiÔ¨Åed Rejection Sampling Scheme: John Jumper ‚Ä¢ Engineering Improvements: Jean-Baptiste Lespiau and Charlie Chen ‚Ä¢ Experiments: Charlie Chen, Sebastian Borgeaud and Laurent Sifre ‚Ä¢ Draft of Manuscript: Charlie Chen and Sebastian Borgeaud ‚Ä¢ Manuscript Feedback: Laurent Sifre, GeoÔ¨Ärey Irving and John JumperAcknowledgementsWe‚Äôd like to thank Oriol Vinyals and Koray Kavukcuoglu for your kind advice and leadership. We‚Äôd also like to thank Evan Senter for your additional feedback on the manuscript and Amelia Glaese for your support in navigating the publishing process. Finally, w e‚Äôd like to thank Blake Hechtman, Berkin Ilbeyi for your valuable advice on XLA and Nikolai Grigoriev for our discussions on the various tricks that can be applied to the transformer architecture.

HyperparamsTable 2 | Hyperparameters for the draft modelModel ùëëmodel Heads Layers ParamsTarget (Chinchilla) 8192 64 80 70B Draft 6144 48 8 4BProofsTheorem 1 (ModiÔ¨Åed Rejection Sampling recovers the target distribution). Given discrete distributions ùëû, ùëù and a single draft sample Àúùë• ‚àº ùëù, let ùëã be the Ô¨Ånal resulting sample. For ùëã = ùë• to be true, we must either sample Àúùë• = ùë• and then accept i t, or resample it after Àúùë• (o f any value) is rejected. Hence:

‚Ñô(ùëã = ùë•)= ‚Ñô(Àúùë• = ùë•)‚Ñô(Àúùë• accepted|Àúùë• = ùë•) + ‚Ñô(Àúùë• rejected)‚Ñô(ùëã = ùë•|Àúùë• rejected)For the Ô¨Årst term, we apply the acceptance rule:

‚Ñô(Àúùë• = ùë•)‚Ñô(Àúùë• accepted|Àúùë• = ùë•)= ùëù(ùë•) min (1, ùëû(ùë•) ùëù(ùë•) ) 10Accelerating Large Language Model Decoding with Speculative Sampling= min ( ùëù(ùë•), ùëû(ùë•))For the second conditional term, we apply the resampling rule:

‚Ñô(ùëã = ùë•|Àúùë• rejected) = (ùëû(ùë•) ‚àí ùëù(ùë•))+Where (.)+ denotes: ( ùëì (ùë•))+ = max(0, ùëì (ùë•)) √çùë• max(0, ùëì (ùë•))Finally, we calculate the probability of rejection:

‚Ñô(Àúùë• rejected) = 1 ‚àí ‚Ñô(Àúùë• accepted)= 1 ‚àí ‚àëÔ∏Åùë•‚Ä≤ ‚Ñô(ùëã = ùë• ‚Ä≤, Àúùë• accepted)= 1 ‚àí ‚àëÔ∏Åùë•‚Ä≤ min( ùëù(ùë• ‚Ä≤), ùëû(ùë• ‚Ä≤))= ‚àëÔ∏Åùë•‚Ä≤ max(0, ùëû(ùë• ‚Ä≤) ‚àí ùëù(ùë• ‚Ä≤))= ‚àëÔ∏Åùë•‚Ä≤ ùëû(ùë• ‚Ä≤) ‚àí min( ùëù(ùë• ‚Ä≤), ùëû(ùë• ‚Ä≤))= ‚àëÔ∏Åùë•‚Ä≤ max(0, ùëû(ùë• ‚Ä≤) ‚àí ùëù(ùë• ‚Ä≤))This is equal to the denominator of (ùëû(ùë•) ‚àí ùëù(ùë•))+, s o:

‚Ñô(Àúùë• rejected)‚Ñô(ùëã = ùë•|Àúùë• rejected) = max(0, ùëû(ùë•) ‚àí ùëù(ùë•))Hence: ‚Ñô(ùëã = ùë•)= min( ùëù(ùë•), ùëû(ùë•)) + max(0, ùëû(ùë•) ‚àí ùëù(ùë•))= ùëû(ùë•)and we have recovered the desired target. 11