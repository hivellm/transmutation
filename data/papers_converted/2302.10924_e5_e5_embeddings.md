## A REINFORCEMENT LEARNING FRAMEWORK FOR

## ONLINE SPEAKER DIARIZATION

Baihan Lin Columbia University New York, NY 10027, USA baihan.lin@columbia.edu

Xinxin Zhang New York University New York, NY 10012, USA xz3149@nyu.edu

February 23, 2023ABSTRACTSpeaker diarization is a task to label an audio or video recording with the identity of the speaker at each given time stamp. In this work, we propose a novel machine learning framework to conduct real-time multi-speaker diarization and recognition without prior registration and pretraining in a fully online and reinforcement learning setting. Our framework combines embedding extraction, clustering, and resegmentation into the same problem as an online decision-making problem. We discuss practical considerations and advanced techniques such as the ofﬂine reinforcement learning, semi-supervision, and domain adaptation to address the challenges of limited training data and outof-distribution environments. Our approach considers speaker diarization as a fully online learning problem of the speaker recognition task, where the agent receives no pretraining from any training set before deployment, and learns to detect speaker identity on the ﬂy through reward feedbacks. The paradigm of the reinforcement learning approach to speaker diarization presents an adaptive, lightweight, and generalizable system that is useful for multi-user teleconferences, where many people might come and go without extensive pre-registration ahead of time. Lastly, we provide a desktop application that uses our proposed approach as a proof of concept. To the best of our knowledge, this is the ﬁrst approach to apply a reinforcement learning approach to the speaker diarization task.

Keywords Reinforcement Learning · Bandits · Speech processing · Speaker diarization

## 1 Introduction

Speaker diarization is a crucial task in many real-world applications, such as meeting transcription, call center monitoring, and broadcast news processing. The goal of speaker diarization is to partition an audio or video stream into homogeneous segments, each corresponding to a single speaker, without any prior knowledge of the speakers’ identities [1, 2]. This task has traditionally been addressed using unsupervised clustering methods [3, 4, 5], but recent advances in deep learning have led to the development of more powerful embedding-based approaches [6, 7, 5].

Despite the recent progress, speaker diarization remains a challenging problem, particularly in real-time and online scenarios where new speakers may enter or leave the conversation at any time. In such cases, pre-trained models may not be sufﬁcient, and the system must be able to adapt to new speakers on the ﬂy [8, 9, 10]. As in the successful applications to other speech and language tasks [11], the reinforcement learning (RL) has emerged as a promising approach for developing next-generation speaker diarization systems that can learn online and adapt to changing circumstances.

In this paper, we propose a novel RL framework for online speaker diarization that does not require prior registration or pretraining. Our approach combines embedding extraction, clustering, and resegmentation into a single online decision-making problem, where the agent receives feedback in the form of rewards or penalties for each segmentation decision. We demonstrate the effectiveness of our approach using a Q-learning-based diarization agent on a desktop app, and discuss practical considerations for implementing and deploying RL-based speaker diarization systems.arXiv:2302.10924v1  [c s.SD]  21 Feb 2023 A Reinforcement Learning Framework for Online Speaker DiarizationFigure 1: The reinforcement learning framework to speaker diarization

## 2 Problem Setting

We consider the task of real-time multi-speaker diarization and recognition in a fully online and reinforcement learning setting. Speciﬁcally, we aim to develop an intelligent system that can label an audio or video recording with the identity of the speaker at each given time stamp, without prior registration and pretraining. The system must be able to adapt to out-o f-distribution environments and learn continually, receiving feedback in the form of rewards from the user.

## 2.1 System assumptions

In this problem setting, we assume that we have no prior knowledge of the number of speakers in the audio stream and the identity of each speaker. The system must be able to detect and recognize speakers on the ﬂy as new speakers enter and leave the conversation, without requiring pre-registration or prior training on large datasets.

We further assume that the system is equipped with a set of contrastive audio embeddings, clustering and resegmentation modules, but it has not been pre-trained on any speciﬁc task or dataset. Instead, the system will learn to detect speaker identity through reinforcement learning, receiving feedback in the form of rewards from the user.

The goal of the system is to minimize the error in labeling speaker identity at each time stamp and provide a smooth and accurate experience for the user. By framing speaker diarization as a fully online learning problem, we hope to create a system that is adaptable, lightweight, and able to generalize to new users and environments.

## 2.2 Reinforcement learning formulation

In the reinforcement learning setting of the online speaker diarization problem, we consider the speaker diarization task as a Markov decision process (MDP). The agent (speaker diarization system) interacts with an environment (audio data) in a sequence of time steps. At each time step, the agent observes the current state of the environment, takes an action, and receives a reward. The objective of the agent is to learn a policy that maximizes the expected sum of future rewards, as outlined in Figure 1. In this problem, the state is the current audio signal segment, and the action is the speaker label assigned to that segment. The reward is the quality of the diarization result, which can be deﬁned in different ways, such as the accuracy of speaker labeling, the precision or recall of the clustering, or the speaker change detection rate. The reward can also be sparse or delayed, which means that the agent may receive little or no feedback for some time steps before receiving a large reward or punishment.

This framework enables the possiblity of incorporating various and multiple streams to reward signals into our model learning process during deployment phase. These reward signals can be user-provided feedbacks, by directly asking the users to provide explicit feedback on the speaker diarization performance. For example, after each diarization, the system can ask users to rate the quality of the separation or provide a binary feedback indicating whether the system performed well or not. They can also be model-related feedbacks such as time-based rewards to incentivize the model to2A Reinforcement Learning Framework for Online Speaker DiarizationFigure 2: (A) The ﬂowchart of an RL-based speaker diarization system, and (B) RL with extendable action spaceimprove the separation over time. For example, the system can provide a positive reward for each second of high-quality separation and a negative reward for each second of poor separation. This type of reward signal can encourage the model to focus on separating speakers accurately over the entire duration of the audio stream. The uncertainty of the models can also be taken into account by computing the model’s own conﬁdence estimates to conﬁdence-based rewards. Finally, our RL model can combine the above reward signals to create a hybrid reward function. For example, the system can use user-provided feedback to update the model’s weights during training and use time-based rewards to encourage the model to separate speakers accurately over time.

The online speaker diarization problem is challenging because it involves an unknown number of speakers, and the speaker identities can change over time. Furthermore, the quality of the diarization result depends not only on the current segment but also on the previous and future segments. Therefore, the speaker diarization system needs to consider long-term dependencies and uncertainty to make effective decisions.

In a RL scenario without prior knowledge of the number of users, the system must dynamically expand its action space as new labels arrive. This problem can be modeled by bandits with inﬁnitely many arms, where new arms are generated when feedback conﬁrms a new addition [12]. For the speaker registration process, we apply the arm expansion process shown in Figure 2B, starting from a single arm for the "new" action. At each time step, a segment of speech information is fed into a feature extractor, which can be a neural network or other feature engineering technique, and the agent uses this feature to make a decision on the speaker identity. If a user provides feedback, the agent updates its policy based on this reward, which may be a sparse correction term. Figure 2A illustrates the data ﬂow of the system.

## 3 Background

Reinforcement Learning (RL) is a powerful machine learning paradigm in which an agent learns to make decisions by interacting with the environment. The goal of the agent is to maximize a cumulative reward signal by learning an optimal policy, which maps states to actions. Figure 3 outlines the problem settings of the ﬁve main classes of reinforcement learning approaches, which we believe can all facilitate effective online speaker diarization.

One of the simplest forms of RL is the Multi-Armed Bandits (MAB) problem, which consists of a set of K arms, each associated with a stochastic reward distribution. At each time step, the agent chooses an arm to pull, and receives a reward sampled from the corresponding distribution. The goal is to maximize the cumulative reward over a ﬁnite number of time steps. The MAB problem is a fundamental trade-off between exploration (trying out new arms to discover the best one) and exploitation (selecting the best arm discovered so far) [13, 14, 15].

Contextual Bandits (CB) extend the MAB problem by incorporating contextual information, which represents additional features that are speciﬁc to each arm. The agent observes a context vector before choosing an action, and the goal is3A Reinforcement Learning Framework for Online Speaker DiarizationFigure 3: Five classes of reinforcement learning problems (from this review on RL in speech applications [11])t o learn a policy that maps contexts to actions. CB algorithms are widely used in personalized recommendations and online advertising [16, 17, 18].

Reinforcement Learning generalizes MAB and CB to problems with more complex state and action spaces. In RL, the agent observes a state of the environment, takes an action, and receives a reward and the next state. The agent’s goal is to learn a policy that maximizes the expected cumulative reward. Some popular RL algorithms include Q-learning [19], SARSA [20], and Actor-Critic [21].

Inverse Reinforcement Learning (IRL) is the problem of inferring the reward function given the optimal policy [22, 23]. IRL has applications in imitation learning, where an agent learns to mimic the behavior of an expert by inferring its underlying reward function [24, 25].

Behavioral Cloning (BC) is a supervised learning method that trains an agent to mimic the expert’s actions given the expert’s state [26]. BC is simple and computationally efﬁcient, but it suffers from the covariate shift problem, where the distribution of states observed during training differs from the distribution encountered during testing [27]. One exception is the Behavior Cloning with Demonstration Rewards (BCDR) [28, 29], which is a training procedure for solving this problem by reformulating it a sa RL setting.

Imitation Learning (IL) extends BC to address the covariate shift problem by learning a policy that maps states to actions, while leveraging the expert’s behavior as a guide [27, 30]. IL has applications in robotics, where an agent learns to perform complex manipulation tasks by observing an expert [31].

More formally, the reinforcement learning deﬁnes a class of algorithms for solving problems modeled as Markov decision processes (MDP) [32]. An MDP is deﬁned by the tuple (S, A, T , R, γ), where S is a set of possible states, A is a set of actions, T is a transition function deﬁned as T (s, a, s′) = Pr( s ′|s, a), where s, s ′ ∈ S and a ∈ A, and R : S × A × S ↦→ R is a reward function, γ is a discount factor that decreases the impact of the past reward on current action choice. Typically, the objective is to maximize the discounted long-term reward, assuming an inﬁnite-horizon decision process. , i.e. to ﬁnd a policy function π : S ↦→ A which speciﬁes the action to take in a given state, so that the cumulative reward is maximized: maxπ ∑∞ t=0 γtR( s t, a t, s t+1).

## 4 Practical Considerations

In our early work, we have shown a successful proof of concept of a bandit-based speaker diarization system in [10, 9, 8, 33]. The algorithm which drives the system is a contextual bandit algorithm called the BerlinUCB [34]. It takes into account the episodicity of the rewards by tackling with a semi-supervised mechanism. Evaluated in the MiniVox benchmark [10], this one preliminary type of reinforcement learning approach outperforms existing methods4A Reinforcement Learning Framework for Online Speaker DiarizationFigure 4: Four algorithmic routes of training RL-based diarization systemsby a signiﬁcant amount. Here we discuss the practical considerations when further extending it to the full spectrum of reinforcement learning problems, beyond the contextual bandit setting.

## 4.1 Technical challenges

There are several innate technical challenges related to applying RL to speaker diarization problems. One of the most signiﬁcant challenges is dealing with the sparse feedback problem. In many real-world applications, the rewards or feedback from the user are sparse, and this can make it difﬁcult to train a model. This can be addressed by using semi-supervised learning methods, such as generative adversarial networks (GANs) or variational autoencoders (VAEs), to generate additional training data from the limited feedback.

The large state space of speaker diarization problems is another challenge. To address this, the state space can be reduced by using feature engineering, such as MFCC, and/o r dimensionality reduction techniques, such as principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE).

Another practical consideration is dealing with the problem of domain adaptation, which refers to the challenge of adapting the RL model to different domains or environments. One way to address this is by using transfer learning techniques, which involve training the RL model on a source domain and then ﬁne-tuning it on the target domain.

Another consideration is the need for continual learning, which is necessary in real-world applications where the environment is constantly changing. To achieve this, online learning algorithms, such as online Q-learning and deep Q-learning, can be used to update the model in real-time as new data becomes available.

Finally, it is important to consider the scalability of RL algorithms when dealing with large datasets. To address this, distributed RL algorithms, such as parameter server and actor-critic, can be used to parallelize the training process across multiple machines or devices. 5A Reinforcement Learning Framework for Online Speaker DiarizationFigure 5: Screenshots of the Desktop App as a proof of concept to the user scenarios.

## 4.2 Algorithmic considerations

When choosing the right method or algorithms to deploy on speaker diarization domain, there are several advanced strategies worth noting, enabled by our reinforcement learning approach. Figure 4 summarizes the four possible approaches that it can be useful to improve the training of reinforcement learning for speaker diarization.

Deep reinforcement learning (DRL) has recently been applied to various speaker applications [35], along with other deep learning based RL or bandits approach [36, 37]. Deep Q network is a popular method [38]. Compared to traditional reinforcement learning methods, DRL can handle complex high-dimensional input spaces, which is essential for the speaker diarization task since it requires processing audio signals with a high degree of ﬁdelity. However, training deep models requires signiﬁcant computational resources and can lead to overﬁtting, which can be problematic when working with limited datasets. Therefore, it is crucial to carefully balance model complexity and training data availability when applying DRL to speaker diarization.

Inverse reinforcement learning (IRL) has been proposed as an alternative to traditional reinforcement learning. IRL can learn the underlying reward function that a speaker would receive, based on the observed behavior of the speaker [30]. This is a useful technique for the speaker diarization problem since it can allow for the efﬁcient use of historical data, which can help in cases where training data is limited. However, IRL requires more processing time than standard RL techniques and can be more challenging to implement. However, once a reward function is learned, IRL can sometimes provide much more dense reward feedbacks to the diarization system and thus alleviating the aforementioned sparse feedback issue.

Ofﬂine reinforcement learning (ORL) is a variation of RL where the agent learns from a batch of previously collected data. Some popular methods include Conservative Q-Learning, among others [39, 40, 41]. In speaker diarization, ORL can help train the agent on historical audio data, which can then be used to update its speaker recognition policy. ORL can be an attractive alternative to online RL methods since it can be computationally efﬁcient and can handle large-scale datasets. However, ORL assumes that all of the relevant data is available before the agent starts learning, which may not always be feasible in practice.

Transfer learning is a method for reusing a previously trained model to improve performance on a new but related task [42]. Transfer learning is useful in the speaker diarization problem since it allows the agent to use a previously trained model to recognize speaker identity for a new speaker without the need to retrain the model from scratch. For instance, the speaker diarization system trained on adult speech corpus can be helpful to kick start a system for kids. Transfer learning can be applied to both ofﬂine and online RL methods and can signiﬁcantly reduce the amount of data required to train a model.

When applying these algorithms to speaker diarization problems, it is essential to pay attention to the challenges unique to the speaker diarization task. Speciﬁcally, since speaker diarization involves processing audio signals in real-time, there is a need for efﬁcient and lightweight algorithms that can process audio signals quickly. Furthermore, speaker diarization requires the recognition of multiple speakers, which can make it challenging to provide reward signals for RL agents. Careful selection of the RL algorithm and tuning of its hyperparameters is necessary to address these challenges and achieve high performance in speaker diarization tasks.

## 5 Demonstration System

We have developed a desktop application to demonstrate our proposed online speaker diarization system using reinforcement learning. The application is built using Python and the PyQt5 framework for the graphical user interface. The reinforcement learning algorithm is implemented using the Q-learning method with an epsilon-greedy exploration6A Reinforcement Learning Framework for Online Speaker Diarizationstrategy. The audio signal processing is handled by the librosa library, which provides functions for feature extraction such as Mel-frequency cepstral coefﬁcients (MFCCs).

The user interface of the application allows the user to input the number of participants in the audio stream and their names (Figure 5). The system then generates buttons for each participant, which the user can click to indicate which participant is currently speaking. The Q-learning algorithm updates the policy for speaker diarization based on the user feedback. In our earlier web-based demonstration using bandits [8], the system also has the option to provide a live visualization of the audio stream, showing the audio waveform and the currently active speaker. In [10], we have evaluated the system using a dataset of multi-speaker audio recordings and compared its performance to that of a traditional clustering-based speaker diarization method. Our results show that the Q-learning-based approach outperforms the traditional method in terms of accuracy and adaptability to changing speaker conﬁgurations.

Overall, our demonstration system showcases the potential of using online reinforcement learning for real-time speaker diarization without requiring pretraining or registration of users. The system is adaptable to different speaker conﬁgurations and can learn from user feedback to improve its performance over time.

## 6 Conclusion

In this paper, we proposed a novel reinforcement learning framework for online speaker diarization. Our framework provides a solution for real-time multi-speaker recognition and diarization without requiring prior registration and pre-training. The proposed framework combines the embedding extraction, clustering, and r e-segmentation into a single online decision-making problem.

We discussed practical considerations in applying reinforcement learning to speaker diarization problems, including the limitations of data availability, the challenge of sparse feedback, and the need for domain adaptation. We also reviewed various reinforcement learning algorithms, including multi-armed bandits, contextual bandits, reinforcement learning, inverse reinforcement learning, behavioral cloning, and imitation learning, and discussed their potential applications and limitations in the context of speaker diarization. To intrigue the audience into this growing ﬁeld, we provided a demonstration system which illustrates the potential of our reinforcement learning framework in a desktop application that allows users to perform real-time speaker recognition and diarization.

In summary, our reinforcement learning framework offers a promising approach to online speaker diarization, which has the potential to be more adaptive, lightweight, and generalizable to new users. With further development and research, we hope that our framework can become a useful tool in various real-world applications, including multi-user teleconferencing, call center management, and security monitoring.

## References

[1] Sreenivas Sremath Tirumala, Seyed Reza Shahamiri, Abhimanyu Singh Garhwal, and Ruili Wang. Speaker identiﬁcation features extraction methods: A systematic review. Expert Systems with Applications, 90:250–271, 2017. [2] Xavier Anguera, Simon Bozonnet, Nicholas Evans, Corinne Fredouille, Gerald Friedland, and Oriol Vinyals. Speaker diarization: A review of recent research. IEEE Transactions on Audio, Speech, and Language Processing,

## 20(2):356–370, 2012.

[3] Z. Zajíc, M. Hrúz, and L. Müller. Speaker diarization using convolutional neural network for statistics accumulation reﬁnement. In INTERSPEECH, 2017. [4] Mohammed Senoussaoui, Patrick Kenny, Themos Stafylakis, and Pierre Dumouchel. A study of the cosine distance-based mean shift for telephone speech diarization. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22:217–227, 2013. [5] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansﬁeld, and Ignacio Lopz Moreno. Speaker diarization with lstm. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages

## 5239–5243. IEEE, 2018.

[6] Stephen H Shum, Najim Dehak, Réda Dehak, and James R Glass. Unsupervised methods for speaker diarization: An integrated and iterative approach. IEEE Transactions on Audio, Speech, and Language Processing, 21(10):2015–

## 2028, 2013.

[7] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur. X-vectors: Robust dnn embeddings for speaker recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5329–5333. IEEE, 2018.

7A Reinforcement Learning Framework for Online Speaker Diarization[8] Baihan Lin and Xinxin Zhang. VoiceID on the ﬂy: A speaker recognition system that learns from scratch. In INTERSPEECH, 2020. [9] Baihan Lin and Xinxin Zhang. Speaker diarization as a fully online learning problem in minivox. arXiv preprint arXiv:2006.04376, 2020. [10] Baihan Lin and Xinxin Zhang. Speaker diarization as a fully online bandit learning problem in minivox. In Asian Conference on Machine Learning, pages 1660–1674. PMLR, 2021. [11] Baihan Lin. Reinforcement learning and bandits for speech and language processing: Tutorial, review and outlook. arXiv preprint arXiv:2210.13623, 2022. [12] Donald A Berry, Robert W Chen, Alan Zame, David C Heath, and Larry A Shepp. Bandit problems with inﬁnitely many arms. The Annals of Statistics, pages 2103–2116, 1997. [13] T. L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4–22, 1985. [14] Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235–256, 2002. [15] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pages 2249–2257, 2011. [16] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661–670, 2010. [17] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in neural information processing systems, volume 20, pages 817–824, 2008. [18] Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payoff functions. In Geoffrey J. Gordon, David B. Dunson, and Miroslav Dudik, editors, AISTATS, volume 15 of JMLR Proceedings, pages 208–214. JMLR.org, 2011. [19] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279–292, 1992. [20] Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37. University of Cambridge, Department of Engineering Cambridge, England, 1994. [21] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems, 12, 1999. [22] Andrew Y Ng, Stuart J Russell, et a l. Algorithms for inverse reinforcement learning. In Icml, volume 1, page 2, 2000. [23] Baihan Lin, Djallel Bouneffouf, and Guillermo Cecchi. Split q learning: reinforcement learning with two-stream rewards. In Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence, pages 6448–6449. AAAI Press, International Joint Conferences on Artiﬁcial Intelligence Organization, 7 2019. [24] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016. [25] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International conference on machine learning, pages 49–58. PMLR, 2016. [26] Dean A Pomerleau. Efﬁcient training of artiﬁcial neural networks for autonomous navigation. Neural computation,

## 3(1):88–97, 1991.

[27] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to n o-regret online learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics, pages 627–635. JMLR Workshop and Conference Proceedings, 2011. [28] Baihan Lin, Djallel Bouneffouf, and Guillermo Cecchi. Online learning in iterated prisoner’s dilemma to mimic human behavior. In Paciﬁc Rim International Conference on Artiﬁcial Intelligence. Springer, 2022. [29] Avinash Balakrishnan, Djallel Bouneffouf, Nicholas Mattei, and Francesca Rossi. Incorporating behavioral constraints in online AI systems. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1,

## 2019, pages 3–11. AAAI Press, 2019.

[30] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-ﬁrst international conference on Machine learning, page 1. ACM, 2004.

8A Reinforcement Learning Framework for Online Speaker Diarization[31] Oliver B Kroemer, Renaud Detry, Justus Piater, and Jan Peters. Combining active learning and reactive control for robot grasping. Robotics and Autonomous systems, 58(9):1105–1116, 2010. [32] Richard S Sutton, Andrew G Barto, et a l. Introduction to reinforcement learning, volume 135. MIT press Cambridge, 1998. [33] Baihan Lin. Voice2alliance: automatic speaker diarization and quality assurance of conversational alignment. In INTERSPEECH, 2022. [34] Baihan Lin. Online semi-supervised learning in contextual bandits with episodic reward. In Australasian Joint Conference on Artiﬁcial Intelligence, pages 407–419. Springer, 2020. [35] Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017. [36] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 32, 2018. [37] Mark Collier and Hector Urdiales Llorens. Deep contextual multi-armed bandits. arXiv preprint arXiv:1807.09809, 2018. [38] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [39] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. [40] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based ofﬂine reinforcement learning. arXiv preprint arXiv:2005.05951, 2020. [41] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine reinforcement learning. arXiv preprint arXiv:2006.04779, 2020. [42] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345–1359, 2009. 9