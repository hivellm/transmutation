Providedproperattributionisprovided,Googleherebygrantspermissionto

reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor

scholarlyworks.

AttentionIsAllYouNeed

3

2

0

2

g

u

∗∗∗∗AshishVaswaniNoamShazeerNikiParmarJakobUszkoreitA

GoogleBrainGoogleBrainGoogleResearchGoogleResearch

2avaswani@google.comnoam@google.comnikip@google.comusz@google.com

∗∗†∗LlionJonesAidanN.GomezŁukaszKaiser]

LGoogleResearchUniversityofTorontoGoogleBrain

Clukaszkaiser@google.comllion@google.comaidan@cs.toronto.edu

.

s

∗‡IlliaPolosukhinc

[illia.polosukhin@gmail.com

7

vAbstract

2

6

Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor7

convolutionalneuralnetworksthatincludeanencoderandadecoder.Thebest3

performingmodelsalsoconnecttheencoderanddecoderthroughanattention0

.mechanism.Weproposeanewsimplenetworkarchitecture,theTransformer,

6basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions

0entirely.Experimentsontwomachinetranslationtasksshowthesemodelsto

7besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly

1lesstimetotrain.Ourmodelachieves28.4BLEUontheWMT2014English-

:

to-Germantranslationtask,improvingovertheexistingbestresults,includingv

ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,i

ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8afterX

trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofther

bestmodelsfromtheliterature.WeshowthattheTransformergeneralizeswelltoa

othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith

largeandlimitedtrainingdata.

∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted

theefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand

hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head

attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery

detail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand

tensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and

efficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand

implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating

ourresearch.

†WorkperformedwhileatGoogleBrain.

‡WorkperformedwhileatGoogleResearch.

31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.

1Introduction

Recurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks

inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand

transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5].Numerous

effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder

architectures[38,24,15].

Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput

sequences.Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden

hhtstates,asafunctionoftheprevioushiddenstateandtheinputforposition.Thisinherently-tt1

sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger

sequencelengths,asmemoryconstraintslimitbatchingacrossexamples.Recentworkhasachieved

significantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional

computation[32],whilealsoimprovingmodelperformanceincaseofthelatter.Thefundamental

constraintofsequentialcomputation,however,remains.

Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-

tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein

theinputoroutputsequences[2,19].Inallbutafewcases[27],however,suchattentionmechanisms

areusedinconjunctionwitharecurrentnetwork.

InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead

relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.

TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin

translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.

2Background

ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU

[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding

block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,

thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows

inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet.Thismakes

itmoredifficulttolearndependenciesbetweendistantpositions[12].IntheTransformerthisis

reducedtoaconstantnumberofoperations,albeitatthecostofreducedeffectiveresolutiondue

toaveragingattention-weightedpositions,aneffectwecounteractwithMulti-HeadAttentionas

describedinsection3.2.

Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions

ofasinglesequenceinordertocomputearepresentationofthesequence.Self-attentionhasbeen

usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,

textualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].

End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-

alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand

languagemodelingtasks[34].

Tothebestofourknowledge,however,theTransformeristhefirsttransductionmodelrelying

entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-

alignedRNNsorconvolution.Inthefollowingsections,wewilldescribetheTransformer,motivate

self-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].

3ModelArchitecture

Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].

(x,...,x)Here,theencodermapsaninputsequenceofsymbolrepresentationstoasequence1n

=(z,...,z)zzofcontinuousrepresentations.Given,thedecoderthengeneratesanoutput1n

(y,...,y)sequenceofsymbolsoneelementatatime.Ateachstepthemodelisauto-regressive1m

[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.

2

Figure1:TheTransformer-modelarchitecture.

TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully

connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,

respectively.

3.1EncoderandDecoderStacks

N=6Theencoderiscomposedofastackofidenticallayers.EachlayerhastwoEncoder:

sub-layers.Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-

wisefullyconnectedfeed-forwardnetwork.Weemployaresidualconnection[11]aroundeachof

thetwosub-layers,followedbylayernormalization[1].Thatis,theoutputofeachsub-layeris

LayerNorm(x+Sublayer(x))Sublayer(x),whereisthefunctionimplementedbythesub-layer

itself.Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding

d=512layers,produceoutputsofdimension.model

N=6Thedecoderisalsocomposedofastackofidenticallayers.InadditiontothetwoDecoder:

sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head

attentionovertheoutputoftheencoderstack.Similartotheencoder,weemployresidualconnections

aroundeachofthesub-layers,followedbylayernormalization.Wealsomodifytheself-attention

sub-layerinthedecoderstacktopreventpositionsfromattendingtosubsequentpositions.This

masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe

iipredictionsforpositioncandependonlyontheknownoutputsatpositionslessthan.

3.2Attention

Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,

wherethequery,keys,values,andoutputareallvectors.Theoutputiscomputedasaweightedsum

3

ScaledDot-ProductAttentionMulti-HeadAttention

Figure2:(left)ScaledDot-ProductAttention.(right)Multi-HeadAttentionconsistsofseveral

attentionlayersrunninginparallel.

ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe

querywiththecorrespondingkey.

3.2.1ScaledDot-ProductAttention

Wecallourparticularattention"ScaledDot-ProductAttention"(Figure2).Theinputconsistsof

ddqueriesandkeysofdimension,andvaluesofdimension.Wecomputethedotproductsofthekv√

dquerywithallkeys,divideeachby,andapplyasoftmaxfunctiontoobtaintheweightsonthek

values.

Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether

QKVintoamatrix.Thekeysandvaluesarealsopackedtogetherintomatricesand.Wecompute

thematrixofoutputsas:

TQK

√Attention(Q,K,V)=softmax()V(1)

dk

Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-

plicative)attention.Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor

1√of.Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith

dk

asinglehiddenlayer.Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis

muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized

matrixmultiplicationcode.

dWhileforsmallvaluesofthetwomechanismsperformsimilarly,additiveattentionoutperformsk

ddotproductattentionwithoutscalingforlargervaluesof[3].Wesuspectthatforlargevaluesofk

d,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithask

14√extremelysmallgradients.Tocounteractthiseffect,wescalethedotproductsby.

dk

3.2.2Multi-HeadAttention

dInsteadofperformingasingleattentionfunctionwith-dimensionalkeys,valuesandqueries,model

hwefounditbeneficialtolinearlyprojectthequeries,keysandvaluestimeswithdifferent,learned

dddlinearprojectionsto,anddimensions,respectively.Oneachoftheseprojectedversionsofkkv

dqueries,keysandvalueswethenperformtheattentionfunctioninparallel,yielding-dimensionalv

4qkToillustratewhythedotproductsgetlarge,assumethatthecomponentsofandareindependentrandom

dk∑·01qk=qk0dvariableswithmeanandvariance.Thentheirdotproduct,,hasmeanandvariance.

iiki=1

4

outputvalues.Theseareconcatenatedandonceagainprojected,resultinginthefinalvalues,as

depictedinFigure2.

Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation

subspacesatdifferentpositions.Withasingleattentionhead,averaginginhibitsthis.

OMultiHead(Q,K,V)=Concat(head,...,head)W

1h

QKV

head=Attention(QW,KW,VW)whereiiii

Q×××ddKddVddvkkmodelmodelmodelRRR

∈∈∈WWWWheretheprojectionsareparametermatrices,,iii

×OhddvmodelR∈Wand.

h=8Inthisworkweemployparallelattentionlayers,orheads.Foreachoftheseweuse

d=d=d/h=64.Duetothereduceddimensionofeachhead,thetotalcomputationalcostkvmodel

issimilartothatofsingle-headattentionwithfulldimensionality.

3.2.3ApplicationsofAttentioninourModel

TheTransformerusesmulti-headattentioninthreedifferentways:

•In"encoder-decoderattention"layers,thequeriescomefromthepreviousdecoderlayer,

andthememorykeysandvaluescomefromtheoutputoftheencoder.Thisallowsevery

positioninthedecodertoattendoverallpositionsintheinputsequence.Thismimicsthe

typicalencoder-decoderattentionmechanismsinsequence-to-sequencemodelssuchas

[38,2,9].

•Theencodercontainsself-attentionlayers.Inaself-attentionlayerallofthekeys,values

andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe

encoder.Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe

encoder.

•Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto

allpositionsinthedecoderuptoandincludingthatposition.Weneedtopreventleftward

informationflowinthedecodertopreservetheauto-regressiveproperty.Weimplementthis

-∞insideofscaleddot-productattentionbymaskingout(settingto)allvaluesintheinput

ofthesoftmaxwhichcorrespondtoillegalconnections.SeeFigure2.

3.3Position-wiseFeed-ForwardNetworks

Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully

connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically.This

consistsoftwolineartransformationswithaReLUactivationinbetween.

FFN(x)=max(0,xW+b)W+b(2)1122

Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters

fromlayertolayer.Anotherwayofdescribingthisisastwoconvolutionswithkernelsize1.

d=512Thedimensionalityofinputandoutputis,andtheinner-layerhasdimensionalitymodel

d=2048.ff

3.4EmbeddingsandSoftmax

Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput

dtokensandoutputtokenstovectorsofdimension.Wealsousetheusuallearnedlineartransfor-model

mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities.In

ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-softmax√

dlineartransformation,similarto[30].Intheembeddinglayers,wemultiplythoseweightsby.model

5

Table1:Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations

ndkfordifferentlayertypes.isthesequencelength,istherepresentationdimension,isthekernel

rsizeofconvolutionsandthesizeoftheneighborhoodinrestrictedself-attention.

LayerTypeComplexityperLayerSequentialMaximumPathLength

Operations

2·O(nd)O(1)O(1)Self-Attention

2·O(nd)O(n)O(n)Recurrent

2··O(knd)O(1)O(log(n))Convolutional

k

··O(rnd)O(1)O(n/r)Self-Attention(restricted)

3.5PositionalEncoding

Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe

orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe

tokensinthesequence.Tothisend,weadd"positionalencodings"totheinputembeddingsatthe

dbottomsoftheencoderanddecoderstacks.Thepositionalencodingshavethesamedimensionmodel

astheembeddings,sothatthetwocanbesummed.Therearemanychoicesofpositionalencodings,

learnedandfixed[9].

Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies:

2i/dmodelPE=sin(pos/10000)

(pos,2i)

2i/dmodelPE=cos(pos/10000)

(pos,2i+1)

posiwhereisthepositionandisthedimension.Thatis,eachdimensionofthepositionalencoding

·2π100002πcorrespondstoasinusoid.Thewavelengthsformageometricprogressionfromto.We

chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby

kPErelativepositions,sinceforanyfixedoffset,canberepresentedasalinearfunctionofpos+k

PE.pos

Wealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo

versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion

becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered

duringtraining.

4WhySelf-Attention

Inthissectionwecomparevariousaspectsofself-attentionlayerstotherecurrentandconvolu-

tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations

dR∈(x,...,x)(z,...,z)x,ztoanothersequenceofequallength,with,suchasahidden

1n1nii

layerinatypicalsequencetransductionencoderordecoder.Motivatingouruseofself-attentionwe

considerthreedesiderata.

Oneisthetotalcomputationalcomplexityperlayer.Anotheristheamountofcomputationthatcan

beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.

Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork.Learninglong-range

dependenciesisakeychallengeinmanysequencetransductiontasks.Onekeyfactoraffectingthe

abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto

traverseinthenetwork.Theshorterthesepathsbetweenanycombinationofpositionsintheinput

andoutputsequences,theeasieritistolearnlong-rangedependencies[12].Hencewealsocompare

themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe

differentlayertypes.

AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially

O(n)executedoperations,whereasarecurrentlayerrequiressequentialoperations.Intermsof

computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence

6

ndlengthissmallerthantherepresentationdimensionality,whichismostoftenthecasewith

sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece

[38]andbyte-pair[31]representations.Toimprovecomputationalperformancefortasksinvolving

rverylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizein

theinputsequencecenteredaroundtherespectiveoutputposition.Thiswouldincreasethemaximum

O(n/r)pathlengthto.Weplantoinvestigatethisapproachfurtherinfuturework.

k<nAsingleconvolutionallayerwithkernelwidthdoesnotconnectallpairsofinputandoutput

O(n/k)positions.Doingsorequiresastackofconvolutionallayersinthecaseofcontiguouskernels,

O(log(n))orinthecaseofdilatedconvolutions[18],increasingthelengthofthelongestpathsk

betweenanytwopositionsinthenetwork.Convolutionallayersaregenerallymoreexpensivethan

krecurrentlayers,byafactorof.Separableconvolutions[6],however,decreasethecomplexity

2···O(knd+nd)k=nconsiderably,to.Evenwith,however,thecomplexityofaseparable

convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,

theapproachwetakeinourmodel.

Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions

fromourmodelsandpresentanddiscussexamplesintheappendix.Notonlydoindividualattention

headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic

andsemanticstructureofthesentences.

5Training

Thissectiondescribesthetrainingregimeforourmodels.

5.1TrainingDataandBatching

WetrainedonthestandardWMT2014English-Germandatasetconsistingofabout4.5million

sentencepairs.Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-

targetvocabularyofabout37000tokens.ForEnglish-French,weusedthesignificantlylargerWMT

2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece

vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining

batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000

targettokens.

5.2HardwareandSchedule

Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs.Forourbasemodelsusing

thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds.We

trainedthebasemodelsforatotalof100,000stepsor12hours.Forourbigmodels,(describedonthe

bottomlineoftable3),steptimewas1.0seconds.Thebigmodelsweretrainedfor300,000steps

(3.5days).

5.3Optimizer

-9β=0.9β=0.98ϵ=10WeusedtheAdamoptimizer[20]with,and.Wevariedthelearning

12

rateoverthecourseoftraining,accordingtotheformula:

-0.5--0.51.5··lrate=dmin(stepnum,stepnumwarmupsteps)

\_\_\_(3)model

warmupstepsThiscorrespondstoincreasingthelearningratelinearlyforthefirst\_trainingsteps,

anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber.Weused

warmupsteps=4000\_.

5.4Regularization

Weemploythreetypesofregularizationduringtraining:

7

Table2:TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe

English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.

BLEUTrainingCost(FLOPs)

Model

EN-DEEN-FREN-DEEN-FR

ByteNet[18]23.75

20·1.010Deep-Att+PosUnk[39]39.2

1920··2.3101.410GNMT+RL[38]24.639.92

1820··9.6101.510ConvS2S[9]25.1640.46

1920··2.0101.210MoE[32]26.0340.56

20·8.010Deep-Att+PosUnkEnsemble[39]40.4

2021··1.8101.110GNMT+RLEnsemble[38]26.3041.16

1921··7.7101.210ConvS2SEnsemble[9]26.3641.29

18·3.310Transformer(basemodel)27.338.1

19·2.310Transformer(big)28.441.8

Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtotheResidualDropout

sub-layerinputandnormalized.Inaddition,weapplydropouttothesumsoftheembeddingsandthe

positionalencodingsinboththeencoderanddecoderstacks.Forthebasemodel,weusearateof

P=0.1.drop

ϵ=0.1Duringtraining,weemployedlabelsmoothingofvalue[36].ThisLabelSmoothingls

hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.

6Results

6.1MachineTranslation

OntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)

2.0inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan

28.4BLEU,establishinganewstate-of-the-artBLEUscoreof.Theconfigurationofthismodelis

3.58listedinthebottomlineofTable3.TrainingtookdaysonP100GPUs.Evenourbasemodel

surpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof

thecompetitivemodels.

41.0OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof,

1/4outperformingallofthepreviouslypublishedsinglemodels,atlessthanthetrainingcostofthe

previousstate-of-the-artmodel.TheTransformer(big)modeltrainedforEnglish-to-Frenchused

P=0.10.3dropoutrate,insteadof.drop

Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which

werewrittenat10-minuteintervals.Forthebigmodels,weaveragedthelast20checkpoints.We

4α=0.6usedbeamsearchwithabeamsizeofandlengthpenalty[38].Thesehyperparameters

werechosenafterexperimentationonthedevelopmentset.Wesetthemaximumoutputlengthduring

50inferencetoinputlength+,butterminateearlywhenpossible[38].

Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel

architecturesfromtheliterature.Weestimatethenumberoffloatingpointoperationsusedtotraina

modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained

5single-precisionfloating-pointcapacityofeachGPU.

6.2ModelVariations

ToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel

indifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe

5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.

8

Table3:VariationsontheTransformerarchitecture.Unlistedvaluesareidenticaltothoseofthebase

model.AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013.Listed

perplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto

per-wordperplexities.

trainPPLBLEUparams

NddhddPϵkvdroplsmodelff6

×10steps(dev)(dev)

base65122048864640.10.1100K4.9225.865

15125125.2924.9

41281285.0025.5

(A)

1632324.9125.8

3216165.0125.4

165.1625.158

(B)

325.0125.460

26.1123.736

45.1925.350

84.8825.580

(C)25632325.7524.528

10241281284.6626.0168

10245.1225.453

40964.7526.290

0.05.7724.6

0.24.9525.5

(D)

0.04.6725.3

0.25.4725.7

(E)positionalembeddinginsteadofsinusoids4.9225.7

big610244096160.3300K2134.3326.4

developmentset,newstest2013.Weusedbeamsearchasdescribedintheprevioussection,butno

checkpointaveraging.WepresenttheseresultsinTable3.

InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,

keepingtheamountofcomputationconstant,asdescribedinSection3.2.2.Whilesingle-head

attentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.

dInTable3rows(B),weobservethatreducingtheattentionkeysizehurtsmodelquality.Thisk

suggeststhatdeterminingcompatibilityisnoteasyandthatamoresophisticatedcompatibility

functionthandotproductmaybebeneficial.Wefurtherobserveinrows(C)and(D)that,asexpected,

biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour

sinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical

resultstothebasemodel.

6.3EnglishConstituencyParsing

ToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish

constituencyparsing.Thistaskpresentsspecificchallenges:theoutputissubjecttostrongstructural

constraintsandissignificantlylongerthantheinput.Furthermore,RNNsequence-to-sequence

modelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].

d=1024Wetraineda4-layertransformerwithontheWallStreetJournal(WSJ)portionofthemodel

PennTreebank[25],about40Ktrainingsentences.Wealsotraineditinasemi-supervisedsetting,

usingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences

[37].Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens

forthesemi-supervisedsetting.

Weperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual

(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters

remainedunchangedfromtheEnglish-to-Germanbasetranslationmodel.Duringinference,we

9

Table4:TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23

ofWSJ)

ParserTrainingWSJ23F1

Vinyals&Kaiserelal.(2014)[37]WSJonly,discriminative88.3

Petrovetal.(2006)[29]WSJonly,discriminative90.4

Zhuetal.(2013)[40]WSJonly,discriminative90.4

Dyeretal.(2016)[8]WSJonly,discriminative91.7

Transformer(4layers)WSJonly,discriminative91.3

Zhuetal.(2013)[40]semi-supervised91.3

Huang&Harper(2009)[14]semi-supervised91.3

McCloskyetal.(2006)[26]semi-supervised92.1

Vinyals&Kaiserelal.(2014)[37]semi-supervised92.1

Transformer(4layers)semi-supervised92.7

Luongetal.(2015)[23]multi-task93.0

Dyeretal.(2016)[8]generative93.3

30021α=0.3increasedthemaximumoutputlengthtoinputlength+.Weusedabeamsizeofand

forbothWSJonlyandthesemi-supervisedsetting.

OurresultsinTable4showthatdespitethelackoftask-specifictuningourmodelperformssur-

prisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe

RecurrentNeuralNetworkGrammar[8].

IncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-

Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.

7Conclusion

Inthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon

attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith

multi-headedself-attention.

Fortranslationtasks,theTransformercanbetrainedsignificantlyfasterthanarchitecturesbased

onrecurrentorconvolutionallayers.OnbothWMT2014English-to-GermanandWMT2014

English-to-Frenchtranslationtasks,weachieveanewstateoftheart.Intheformertaskourbest

modeloutperformsevenallpreviouslyreportedensembles.

Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks.We

plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand

toinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs

suchasimages,audioandvideo.Makinggenerationlesssequentialisanotherresearchgoalsofours.

Thecodeweusedtotrainandevaluateourmodelsisavailableathttps://github.com/

.tensorflow/tensor2tensor

WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitfulAcknowledgements

comments,correctionsandinspiration.

References

arXivpreprint[1]JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton.Layernormalization.

arXiv:1607.06450,2016.

[2]DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointly

CoRRlearningtoalignandtranslate.,abs/1409.0473,2014.

[3]DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le.Massiveexplorationofneural

CoRRmachinetranslationarchitectures.,abs/1703.03906,2017.

[4]JianpengCheng,LiDong,andMirellaLapata.Longshort-termmemory-networksformachine

arXivpreprintarXiv:1601.06733reading.,2016.

10

[5]KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,

andYoshuaBengio.Learningphraserepresentationsusingrnnencoder-decoderforstatistical

CoRRmachinetranslation.,abs/1406.1078,2014.

arXiv[6]FrancoisChollet.Xception:Deeplearningwithdepthwiseseparableconvolutions.

preprintarXiv:1610.02357,2016.

[7]JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio.Empiricalevaluation

CoRRofgatedrecurrentneuralnetworksonsequencemodeling.,abs/1412.3555,2014.

[8]ChrisDyer,AdhigunaKuncoro,MiguelBallesteros,andNoahA.Smith.Recurrentneural

Proc.ofNAACLnetworkgrammars.In,2016.

[9]JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin.Convolu-

arXivpreprintarXiv:1705.03122v2tionalsequencetosequencelearning.,2017.

arXivpreprint[10]AlexGraves.Generatingsequenceswithrecurrentneuralnetworks.

arXiv:1308.0850,2013.

[11]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforim-

ProceedingsoftheIEEEConferenceonComputerVisionandPatternagerecognition.In

Recognition,pages770-778,2016.

[12]SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber.Gradientflowin

recurrentnets:thedifficultyoflearninglong-termdependencies,2001.

Neuralcomputation[13]SeppHochreiterandJürgenSchmidhuber.Longshort-termmemory.,

9(8):1735-1780,1997.

[14]ZhongqiangHuangandMaryHarper.Self-trainingPCFGgrammarswithlatentannotations

Proceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalacrosslanguages.In

LanguageProcessing,pages832-841.ACL,August2009.

[15]RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu.Exploring

arXivpreprintarXiv:1602.02410thelimitsoflanguagemodeling.,2016.

AdvancesinNeural[16]ŁukaszKaiserandSamyBengio.Canactivememoryreplaceattention?In

InformationProcessingSystems,(NIPS),2016.

InternationalConference[17]ŁukaszKaiserandIlyaSutskever.NeuralGPUslearnalgorithms.In

onLearningRepresentations(ICLR),2016.

[18]NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-

arXivpreprintarXiv:1610.10099v2rayKavukcuoglu.Neuralmachinetranslationinlineartime.,

2017.

[19]YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush.Structuredattentionnetworks.

InternationalConferenceonLearningRepresentationsIn,2017.

ICLR[20]DiederikKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.In,2015.

arXivpreprint[21]OleksiiKuchaievandBorisGinsburg.FactorizationtricksforLSTMnetworks.

arXiv:1703.10722,2017.

[22]ZhouhanLin,MinweiFeng,CiceroNogueiradosSantos,MoYu,BingXiang,Bowen

arXivpreprintZhou,andYoshuaBengio.Astructuredself-attentivesentenceembedding.

arXiv:1703.03130,2017.

[23]Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser.Multi-task

arXivpreprintarXiv:1511.06114sequencetosequencelearning.,2015.

[24]Minh-ThangLuong,HieuPham,andChristopherDManning.Effectiveapproachestoattention-

arXivpreprintarXiv:1508.04025basedneuralmachinetranslation.,2015.

11

[25]MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated

Computationallinguisticscorpusofenglish:Thepenntreebank.,19(2):313-330,1993.

[26]DavidMcClosky,EugeneCharniak,andMarkJohnson.Effectiveself-trainingforparsing.In

ProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,

pages152-159.ACL,June2006.

[27]AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit.Adecomposableattention

EmpiricalMethodsinNaturalLanguageProcessingmodel.In,2016.

[28]RomainPaulus,CaimingXiong,andRichardSocher.Adeepreinforcedmodelforabstractive

arXivpreprintarXiv:1705.04304summarization.,2017.

[29]SlavPetrov,LeonBarrett,RomainThibaux,andDanKlein.Learningaccurate,compact,

Proceedingsofthe21stInternationalConferenceonandinterpretabletreeannotation.In

ComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433-440.ACL,July

2006.

arXiv[30]OfirPressandLiorWolf.Usingtheoutputembeddingtoimprovelanguagemodels.

preprintarXiv:1608.05859,2016.

[31]RicoSennrich,BarryHaddow,andAlexandraBirch.Neuralmachinetranslationofrarewords

arXivpreprintarXiv:1508.07909withsubwordunits.,2015.

[32]NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,

andJeffDean.Outrageouslylargeneuralnetworks:Thesparsely-gatedmixture-of-experts

arXivpreprintarXiv:1701.06538layer.,2017.

[33]NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-

JournalofMachinenov.Dropout:asimplewaytopreventneuralnetworksfromoverfitting.

LearningResearch,15(1):1929-1958,2014.

[34]SainbayarSukhbaatar,ArthurSzlam,JasonWeston,andRobFergus.End-to-endmemory

networks.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,

AdvancesinNeuralInformationProcessingSystems28,pages2440-2448.CurranAssociates,

Inc.,2015.

[35]IlyaSutskever,OriolVinyals,andQuocVVLe.Sequencetosequencelearningwithneural

AdvancesinNeuralInformationProcessingSystemsnetworks.In,pages3104-3112,2014.

[36]ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.

CoRRRethinkingtheinceptionarchitectureforcomputervision.,abs/1512.00567,2015.

[37]Vinyals&Kaiser,Koo,Petrov,Sutskever,andHinton.Grammarasaforeignlanguage.In

AdvancesinNeuralInformationProcessingSystems,2015.

[38]YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,MohammadNorouzi,Wolfgang

Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal.Google'sneuralmachine

arXivpreprinttranslationsystem:Bridgingthegapbetweenhumanandmachinetranslation.

arXiv:1609.08144,2016.

[39]JieZhou,YingCao,XuguangWang,PengLi,andWeiXu.Deeprecurrentmodelswith

CoRRfast-forwardconnectionsforneuralmachinetranslation.,abs/1606.04199,2016.

[40]MuhuaZhu,YueZhang,WenliangChen,MinZhang,andJingboZhu.Fastandaccurate

Proceedingsofthe51stAnnualMeetingoftheACL(Volumeshift-reduceconstituentparsing.In

1:LongPapers),pages434-443.ACL,August2013.

12

Input-InputLayer5AttentionVisualizations

st

nneo

nitam

sytat>dgclnsiri

>>>>>>rrtegnruSeiteedddddd9seesio

ncescitjkriicrOaaaaaavsvw0tsif

ogiaeaaowfnmEppppppoarrfae0iopiehhht

nsaittmgpsmovp<<<<IisaoAhnl2trmd.<<<

t.tfttrsslissysIaeenenegd9gn>>>>>>>iiwittraoor

siiuvchane0noddddddwSheriinihtotcepantcst

t0aaaaaaiainikoelOfcsjsrhoa2ppppppm

fsarioavaEemt<<<<<<r

dmpsn<mpimr

geAev

ro

g

Figure3:Anexampleoftheattentionmechanismfollowinglong-distancedependenciesinthe

encoderself-attentioninlayer5of6.Manyoftheattentionheadsattendtoadistantdependencyof

theverb'making',completingthephrase'making...moredifficult'.Attentionshereshownonlyfor

theword'making'.Differentcolorsrepresentdifferentheads.Bestviewedincolor.

13

Input-InputLayer5

n

oit

gt>nad

cn>rlcoiSiit

edleusfntaO

elavrtwpiossleiyisihe

hpEaeeeuperphshntus

wnpbsjiw,im<TLb,iab-twamo.<

l,,.tttt-rlssyseeeenegdnn>>iiiw

tcsiluareihbbnoodShwa

meuiuiiwvbhatjftaTLs

onreOwiaps

heniInput-InputLayer5pcE<is

plo<m

p

p

a

n

oit

gt>nad

cn>rlcoiSiit

edleusfntaO

elawvrtpiossleiyisieh

hpEeeuaeperphshusnt

wnpbsjiw,im<TLb,iab-twamo.<

l,,.tttt-rlsssyeeeenegdnn>>iiiw

tcsiluareihbbnoodShaw

meuiuiiwvbhatjftaTLs

onreOwiaps

henipcE<is

plo<m

p

p

a

Figure4:Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution.Top:

Fullattentionsforhead5.Bottom:Isolatedattentionsfromjusttheword'its'forattentionheads5

and6.Notethattheattentionsareverysharpforthisword.

14

Input-InputLayer5

n

oit

gt>nad

cn>rlcoiSiit

edleusfntaO

elavrtwpiossleiyisieh

hpEeeuaeperphshntus

i<Lwnbp,biasbj-tiwwa,mo<Tm.

l.,,tttt-rlssyseeeneegdnn>>iiiw

tcsiluareihbbnoodShaw

meuiuiiwvbhatjftaTLs

onreOwiaps

henipcEInput-InputLayer5<is

plo<m

p

p

a

n

oit

gt>nad

cn>rlcoiSiit

edleusfntaO

elavrtwpiossleiyisieh

hpEeeuaeperphshntus

i<Lwnbp,biasbj-tiwwa,mo<Tm.

l.,,tttt-rlssyseeeenegdnn>>iiiw

tcsiluareihbbnoodShaw

meuiuiiwvbhatjftaTLs

onreOwiaps

henipcE<is

plo<m

p

p

a

Figure5:Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe

sentence.Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention

atlayer5of6.Theheadsclearlylearnedtoperformdifferenttasks.

15