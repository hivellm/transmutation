## Table 4: Multi-Passage Knowledge Incorporation Hyperparameters

## Parameter Search Space

LoRA Rank ( R ) [ 32 , 64] LoRA Alpha ( ) [32, 64 ] Learning Rate [1e-4, 3e-4, 5e-4, 1e-3 , 2e-3] Epochs [1, 3 , 5] Batch Size [1, 4, 8 , 16] Let's answer a question directly and concisely. Question: {question} Answer: B.4 Evaluation Details We evaluate on a 200-passage subset of the SQuAD evaluation set, consisting of a combined 974 evaluation questions (roughly 5 corresponding to each passage). The pipeline of generating synthetic data and netuning on i ti s the same as above. For automated grading, we use gpt-4.1-2025-04-14 [82] via the OpenAI API with greedy decoding. The grading prompt is as follows: You are a grading assistant. Your job is to determine whether a student's answer correctly answers the question based solely on the provided gold answer. Do not use any outside knowledge. The student answer can include additional information, but it must at least fully convey the gold answer and must not contradict i t. Ignore style, phrasing, or extra details that do not affect correctness. Respond ONLY with `yes' or `n o'. Question: {question} Gold answer: {gold} Student answer: {pred} Is the student answer correct based solely on the gold answer? Respond `yes' or `n o'. B.5 Compute Resources All experiments are performed on 2  H100 or 2  H200. We use DeepSpeed ZeRO-3 [ 83 ] for SFT in ReST EM training. We use vLLM [ 84 ] for e fcient inference. The most compute-intensive portion of our training and evaluation is the E-step of ReST EM training, where the model generates completions and is graded through the inner-loop process of netuning and running inference. Doing a single round requires a batch of 50 passages over 5 completions and 3 runs per completion, meaning 750 inner loop iterations. This takes about 6 hours on 2  H100s. B.6 Standard Error of the Mean in Catastrophic Forgetting Experiment The standard errors of the mean (SEM) for each entry in Figure 6 is shown below in Table B.6. B.7 Scaling Model Size We further experimented with the 3B-parameter Qwen variant, with the same single-passage setup as in Figure 4. The results are given in Table 6. To compare the benet of SEAL over using self-edits generated by the base model, we compute the ratio of SEAL's improvement over the base model to the improvement from base model self-edits. This ratio is 1 : 75  for the 3B model and 2 : 04  for the 7B model. The relative improvement is greater for the 7B model, which provides some evidence that not only are stronger base models more effective at leveraging synthetic data for self-adaptation, but reinforcement learning may have compounding 21