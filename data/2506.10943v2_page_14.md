[44] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th


## International Conference on Machine Learning

, Proceedings of Machine Learning Research. PMLR, 2017. URL https://proceedings.mlr.press/v70/finn17a.html . [45] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL 2 : Fast reinforcement learning via slow reinforcement learning, 2016. URL https://arxiv. org/abs/1611.02779 . [46] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn,


## 2017. URL

https://arxiv.org/abs/1611.05763 . [47] Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. In The Sixth International Conference on Learning Representations , 2018. URL https://openreview.net/forum?i d=SyX0IeWAW . [48] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-reinforcement learning of structured exploration strategies. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems . Curran Associates, Inc.,


## 2018. URL

https://proceedings.neurips.c c/paper_files/paper/2018/file/ 4de754248c196c85ee4fbdcee89179bd-Paper.pdf . [49] Qi Sun, Edoardo Cetin, and Yujin Tang. Transformer-Squared: Self-adaptive LLMs, 2025. URL https://arxiv.org/abs/2501.06252 . [50] Jurgen Schmidhuber. Steps towards `self-referential' neural learning: A thought experiment,


## 1992. URL

https://people.idsia.c h/~juergen/selfref1992.pdf . [51] Kazuki Irie, Imanol Schlag, R�bert Csord�s, and J�rgen Schmidhuber. A modern self-referential weight matrix that learns to modify itself. In International Conference on Machine Learning . PMLR, 2022. URL https://proceedings.mlr.press/v162/irie22b.html . [52] Chenmien Tan, Ge Zhang, and Jie Fu. Massive editing for large language models via meta learning, 2024. URL https://arxiv.org/abs/2311.04661 . [53] Nathan Hu, Eric Mitchell, Christopher Manning, and Chelsea Finn. Meta-learning online adaptation of language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023. emnlp-main.268/ . [54] Tong Chen, Hao Fang, Patrick Xia, Xiaodong Liu, Benjamin Van Durme, Luke Zettlemoyer, Jianfeng Gao, and Hao Cheng. Generative Adapter: Contextualizing language models in parameters with a single forward pass. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?i d=bc3sUsS6ck . [55] Dan A. Calian, Gregory Farquhar, Iurii Kemaev, Luisa M. Zintgraf, Matteo Hessel, Jeremy Shar, Junhyuk Oh, Andr�s Gy�rgy, Tom Schaul, Jeffrey Dean, Hado van Hasselt, and David Silver. DataRater: Meta-learned dataset curation, 2025. URL https://arxiv.org/abs/ 2505.17895 . [56] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hateld-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback, 2022. URL https://arxiv.org/abs/ 2212.08073 . 14