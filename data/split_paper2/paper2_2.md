Figure 1: Overview of SEAL.


## In each RL outer loop iteration, the model generates candidate

self-edits (SE)�directives on how to update the weights�applies updates, evaluates performance on a downstream task, and uses the resulting rewards to improve the self-edit generation policy. current approaches do not enable models to develop bespoke strategies for how to best transform and learn from their training data. As a step towards better language model adaptation, we propose equipping LLMs with the ability to generate their own training data and netuning directives in response to new inputs. In particular, we introduce a reinforcement learning algorithm that trains LLMs to generate �self-edits� �naturallanguage instructions that specify the data and, optionally, optimization hyperparameters for updating the model's weights (see Figure 1). We refer to such models as Se lfA dapting L LMs (SEAL). We evaluate SEAL on two applications. We rst consider the task of integrating new factual knowledge into an LLM. Rather than netuning directly on the passage text, we netune on synthetic data generated by the SEAL model. Our results show that, following reinforcement learning (RL) training, netuning on self-generated synthetic data improves question-answering performance on the n o-passage-i n-context variant of SQuAD [ 13 ] from 33.5% to 47.0%. Notably, self-generated data from SEAL outperforms synthetic data generated by GPT-4.1. We further evaluate SEAL on few-shot learning on a simplie d subset of the ARC-AGI benchmark [ 14 ], where the model leverages a set of tools to autonomously select both synthetic data augmentations and optimization hyperparameters (e.g., learning rate, training epochs, selective loss computation over token types). Our experiments demonstrate that automatic selection and conguration of these tools using SEAL enhances performance compared to both standard i n-context learning (ICL) and self-editing without RL training to use the tools effectively. These results collectively show that SEAL is a versatile framework for enabling language models to self-adapt.


## 2 Related Work

Synthetic Data Generation. The creation of synthetic data for LLM training is increasingly common, from large-scale pretraining datasets [ 15 , 16 , 17 , 18 , 19 ] to task-specic data augmentation [ 20 , 21 , 22 ] and instruction-tuning sets [ 23 , 24 ]. For incorporation of a smaller corpus, Yang et a l. [25] use synthetic data generation via graph-based prompting. SEAL builds on this line of work by using reinforcement learning to train a generative policy that directly maximizes the downstream utility of synthetic data when applied for gradient-based self-updates, rather than relying on static or heuristic generation strategies that are manually tuned. Knowledge Updating. Several recent works aim to modify or inject factual knowledge into language models via weight updates. Some methods attempt to directly locate specic parameters that correspond to individual facts [ 26 , 27 , 28 ]. Others propose generating additional netuning data using the information in context [ 29 , 30 , 25 , 31 , 32 ]. We adopt the latter strategy, following Aky�rek et a l. [30] , who propose generating logical implications of a fact and netuning on them, and Lampinen et a l. [31] , who show that implication-based netuning can even outperform i n-context learning. We build on these approaches by training models through RL to generate more optimal netuning data. Park et a l. [32] show that prompting language models to generate question�answer (QA) pairs directly can outperform implication-style prompting. Because the SEAL framework is agnostic to the prompt and format of the self-edit data, it can also be trained to generate QA pairs or other output formats, as explored in �B.11. 2