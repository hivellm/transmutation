Few-Shot Learning. The Abstraction and Reasoning Corpus (ARC) [ 8 ] is a benchmark designed to test abstract reasoning and generalization from very limited examples. Each task includes a small set of input-output demonstrations and a held-out test input whose correct output must be predicted. We adopt the test-time training (TTT) protocol of Aky�rek et a l. [36] , where augmentations of the few-shot examples are used to perform gradient-based adaptation. Rather than relying on manually tuned heuristics for selecting augmentations and optimization settings, we train SEAL to learn these decisions. This setting tests whether SEAL can autonomously congure the adaptation pipeline� determining which augmentations to apply and what optimization parameters to use. To implement this, we d en ea set of tools , each of which is a pre-d ened function from Aky�rek et a l. [36] that transforms data or specie s training parameters. These include: � Data augmentations: rotations, ips, r eections, transpositions, resizing operations (e.g., changing grid resolution), and chained or repeated transformations. � Optimization parameters: learning rate, number of training epochs, and whether the loss is computed over all tokens or only output tokens. The model is prompted with a task's few-shot demonstrations to generate a self-edit, which in this case is a specication of which tools to invoke and how to congure them, as shown in Figure 3. The self-edit is then applied to adapt the model via LoRA netuning. The adapted model is evaluated on the held-out test input, and the result determines the reward for the self-edit generation.


## 4 Results

In this section we empirically evaluate SEAL across our two adaptation domains: few-shot learning and knowledge incorporation. Full training, hyperparameter, and evaluation details are provided in �A and �B.


## 4.1 Few-Shot Learning

We conduct our experiments using Llama-3.2-1B-Instruct , a small open-source model with no ARC-specic pretraining. Since most ARC tasks are challenging for models that have not been pretrained on ARC, we curate a subset of 11 tasks from the ARC training set and 8 from the evaluation set, ltered to ensure that they are solvable under optimal TTT congurations for a base Llama-3.2-1B-Instruct . While this is a small number of examples, note that Aky�rek et a l. [36] used the same TTT conguration for all tasks, and thus we do not need a large training set for learning a xed self-edit. More details are included in �A. The model is trained using ReST EM by sampling 15 self-edits per training task. Each self-edit is applied individually to generate 15 updated models, which are then evaluated on the corresponding held-out test example. We reinforce only those self-edits that lead to correctly adapted models, i.e., models that produce the correct output for the test input after adaptation. After training, we evaluate the model by generating 5 self-edits per held-out evaluation task and apply each one independently. We then report the percentage of self-edits that lead to correct outputs, yielding a success rate that r eects the quality of the learned self-edit generation policy. We compare against the following baselines: 1. ICL (In-Context Learning): Llama-3.2-1B-Instruct is prompted with the given few-shot examples using Aky�rek et a l. [36]'s protocol, and directly queried on the test input. 2. TTT + Self-Edit (w/o prior RL): Llama-3.2-1B-Instruct performs test-time training (TTT) using few-shot examples and synthetic augmentations, but without any prior RL to optimize which augmentations or training congurations to use. 3. Oracle TTT: The model performs test-time training (TTT) using the optimal human-crafted conguration from Aky�rek et a l. [36]. This provides an upper bound of our method. We record results in Table 4.1. SEAL substantially improves adaptation success rate compared to baselines: 72.5% v s. 20% (with self-edits from the base model without RL training) and 0% (n o adaptation), though performance remains below Oracle TTT, suggesting room for further improvement. 6