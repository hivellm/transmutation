[19] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa


## Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common


## Crawl into ar ened long-horizon pretraining dataset, 2025. URL

https://arxiv.org/abs/ 2412.02595 . [20] Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of LLMs help clinical text mining?, 2023. URL https://arxiv.org/abs/2303.04360 . [21] Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. Better synthetic data by retrieving and transforming existing datasets. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics . Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024. findings-acl.385/ . [22] Yangjun Ruan, Neil Band, Chris J. Maddison, and Tatsunori Hashimoto. Reasoning to learn from latent thoughts, 2025. URL https://arxiv.org/abs/2503.18866 . [23] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the


## 61st Annual Meeting of the Association for Computational Linguistics

. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.acl-long.754/ . [24] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4, 2023. URL https://arxiv.org/abs/2304.03277 . [25] Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candes, and Tatsunori Hashimoto. Synthetic continued pretraining. In The Thirteenth International Conference on Learning Representations ,


## 2025. URL

https://openreview.net/forum?i d=07yvxWDSla . [26] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. In The Tenth International Conference on Learning Representations ,


## 2022. URL

https://openreview.net/forum?i d=0DcZxeWfOPt . [27] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems . Curran Associates, Inc., 2022. URL https://proceedings.neurips.c c/paper_files/paper/2022/file/ 6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf . [28] Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Massediting memory in a transformer. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?i d=MkbcAHIYgyS . [29] Asaf Yehudai, Boaz Carmeli, Yosi Mass, Or Arviv, Nathaniel Mills, Eyal Shnarch, and Leshem Choshen. Achieving human parity in content-grounded datasets generation. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview. net/forum?i d=RjYKTQ0L0W . [30] Afra Feyza Aky�rek, Ekin Aky�rek, Leshem Choshen, Derry Wijaya, and Jacob Andreas. Deductive closure training of language models for coherence, accuracy, and updatability. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics . Association for Computational Linguistics, 2024. URL https: //aclanthology.org/2024.findings-acl.584/ . [31] Andrew K. Lampinen, Arslan Chaudhry, Stephanie C. Y. Chan, Cody Wild, Diane Wan, Alex Ku, J�r g Bornschein, Razvan Pascanu, Murray Shanahan, and James L. McClelland. On the generalization of language models from i n-context learning and netuning: a controlled study,


## 2025. URL

https://arxiv.org/abs/2505.00661 . [32] Core Francisco Park, Zechen Zhang, and Hidenori Tanaka. New News : System-2 n e-tuning for robust integration of new knowledge, 2025. URL https://arxiv.org/abs/2505.01812 . 12