## 3.2 Domain Instantiations

We instantiate the SEAL framework in two distinct domains: knowledge incorporation and few-shot learning. These domains were chosen to highlight two complementary forms of model adaptation: (1) the ability to integrate new information into a model's weights so that it can be recalled without relying on context (evaluated using an o-context variant of SQuAD) and (2) the ability to generalize to novel tasks after seeing only a small number of examples (evaluated using ARC). Knowledge Incorporation. Our goal is t oe fciently incorporate the information provided in a passage into the model's weights. A promising recent approach involves using a language model to generate content derived from the passage, followed by netuning on both the original passage and the generated content [ 29 , 30 , 25 , 31 , 32 ]. While the form of generated content may vary, we adopt what we consider the canonical format: implications derived from the passage . This approach, introduced in deductive closure training [ 30 ], converts a given context C into a set of implications SE = fs 1 ; s 2 ; : : : ; sn g by prompting the model to �List several implications derived from the content.� The output may include inferences, logical consequences, or restatements of the original passage. In �B.11, we also explore alternative prompts such as �rewrite the passage in different ways� or �rewrite in a question-answer format� and show that our method improves performance by similar or greater margins regardless of the base prompt. Figure 2: Knowledge Incorporation Setup. Given a new passage, the model generates synthetic data (the self-edit ) in the form of �implications� of the passage. We then netune on these outputs using LoRA. The updated model is evaluated on questions about the passage without access to the original text, and the resulting accuracy serves as the reward signal for reinforcement learning. These self-generated statements form the training data for a supervised netuning (SFT) update: we compute the standard causal language-modeling loss over each sequence si and update the model parameters, yielding  0 . Since the amount of data per update is small and the number of updates we d oi n total is large, we use low-rank adapters (LoRA [ 72 ]) for e fcient, lightweight tuning. Finally, the adapted model LM  0 is evaluated on the task  . This process is shown in Figure 2. During RL training, the adapted model's accuracy on  d enes the reward r that drives the outer RL optimization. This trains the model to restructure the passage in a way that is most effective for assimilation via netuning. Figure 3: Few-Shot Learning with SEAL. Left: example ARC demonstrations. Center: the model generates a self-edit specifying augmentations and training hyperparameters. Right: the adapted model is evaluated on a held-out test input. 5