Method Success Rate (%) ICL 0 TTT + Self-Edit (w/o prior RL) 20 SEAL 72.5 Oracle TTT 100 Table 1: Few-shot Abstract Reasoning


## 4.2 Knowledge Incorporation

We experiment with Qwen2.5-7B on incorporating novel factual content from SQuAD passages [ 13 ]. We use the relatively simple SQuAD dataset because its passages can be fully �understood� by the base model i n-context, yet the model cannot reliably answer questions about them without that context. We do 2 rounds of ReST EM with a batch of 50 contexts (see �B for further details). We compare SEAL on knowledge incorporation against the following baseline approaches: 1. Base Model: The pretrained model is evaluated on downstream QA tasks without any adaptation or access to the passage. 2. Train on Passage Only: The model is netuned directly on the passage using the standard language modeling loss, without any synthetic data. 3. Train on Passage + Synthetic Data: The model is trained on the passage along with self-generated implications. This is the same setup as SEAL but without any prior RL training. 4. Train on Passage + GPT-4.1 Synthetic Data: The model is trained on the passage along with model-generated implications collected from GPT-4.1 via the OpenAI API. Figure 4: Accuracy over RL iterations. Each iteration consists of a minibatch of 50 contexts, each with 5 sampled self-edits. SEAL surpasses GPT-


## 4.1 synthetic data after two iterations o f

ReST EM on the n o-context SQuAD set. Table 4.2 reports mean n o-context SQuAD accuracy under two regimes: single-passage updating (with LoRA), and small-scale continued pretraining (with full netuning). We run continued pretraining (CPT) experiments with n = 200 documents, as well as the full SQuAD validation set of n = 2067 documents. In the single-passage setting, netuning directly on the passage yields a negligible gain over the frozen base model (33.5% v s. 32.7%), conrming that using the raw data alone is insufcient. Augmenting with synthetic implications generated by GPT-4.1 boosts accuracy to 46.3%, an improvement of 12.8 percentage points over the passage-only baseline. Using synthetic data produced by the base Qwen-2.5-7B model yields


## 39.7%, a 6.2-point increase. After reinforcement learn-

ing, SEAL further improves accuracy to 47.0% , notably outperforming using synthetic data from GPT-4.1, despite being a much smaller model. In the CPT setting, the model assimilates information from many passages in a single continued pretraining run. It is then evaluated on the union of all corresponding questions. In this setting, we sample 5 self-edit generations for each passage and take the aggregate synthetic dataset for continued pretraining. As shown in Table 4.2, we observe a similar ranking of methods as in the single-passage case, but with synthetic data from GPT-4.1 slightly outperforming SEAL. In the n = 200 setting, SEAL achieves an accuracy of 58.2%, exceeding its single-passage performance. We attribute this gain to the aggregation of multiple self-edit generations. Overall, the strong continued pretraining results of SEAL suggest that the self-editing policy generalizes beyond the original RL setup of creating synthetic data in a single generation for a single passage. Figure 4 tracks accuracy after each outer RL iteration. Two iterations sufc e for SEAL to overtake GPT-4.1 data; subsequent iterations yield diminishing returns, suggesting that the policy quickly converges to an edit style that distills the passage into easily learnable atomic facts (see qualitative examples in Figure 5). All results use tuned hyperparameters (see �B). 7