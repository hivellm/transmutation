suggest that RL-trained self-edits and structured heuristic methods are both strong approaches for synthetic data generation. Table 8: Synthetic Continued Pretraining (SCPT) on SQuAD (n o passage in context). Best in each column is bolded. Method Continued Pretraining (n=200) Continued Pretraining (n=2067) SEAL 58.2 46.4 Entigraph (pairs) 46.2 38.6 Entigraph (pairs+triples) 56.0 48.6 B.10 Proxy Reward We experiment with replacing the inner loop with a proxy reward based on a human-crafted rubric with 4 categories: length, diversity, quality, and correctness. A GPT-4.1 grader scores each category on a 1-5 scale, and the sum of these scores is used as the RL reward. Table 9 reports nal results and RL training times. Table 9: Full Reward v s. Proxy Reward Performance (%). Model Base Post-RL Time SEAL 32.0 47.0 


## 6 h r

SEAL w/ Proxy-Reward 32.0 45.6 


## 5 min

While further tuning of the rubric or metric design could strengthen the reward signal, the advantage of the full SEAL loop is that no such manual specication is required�the model directly learns which edits improve its own performance. Both approaches appear promising for scaling to larger model sizes and compute budgets: proxy metrics offer dramatically lower cost, and with r enement, they may even surpass the �true� reward of directly optimizing for post-netuning performance. B.11 Prompting Recent works have shown that reinforcement learning baselines and outcomes can be highly sensitive to prompting. We experiment with 6 additional self-edit prompts in the knowledge-incorporation setting. The seven prompts� implications , implications-long , implications-very-long , implications-chain-o f-thought , rewrite , self-q a , and n o-prompt �are shown below. All results in the main content of the paper used the implications prompt, which we consider to be the most prototypical [ 30 , 31 ]. However, prior work has found prompts involving rewriting or generating question-answer pairs can be more effective, as discussed in �2. Furthermore, as we see qualitatively in Figure 5, RL appears to have dramatically increased the length of the response of the example. We therefore experiment with prompting for longer generations with implications-long and implications-very-long to test if we can achieve similar gains through prompting alone. The results are shown in Table 10. Notably, the baselines for implications-long and rewrite the RL-trained version of implications . However, using these prompts as the base of RL training yields even greater improvements. In all cases, ReST EM enhanced performance by roughly 6 to 11 percentage points. Here, �Chain-o f-thought-eval� refers to having the model reason before answering the questions (letting the model �pull out� information from its weights), rather than chain-o f-thought before generating synthetic data, which is done with the base implications prompt. However, we did not notice a substantial difference in our setting when chain-o f-thought was applied, whether before answering and before writing synthetic data. Letting the model �determine its own� self-edit format, with n o-prompt , was not able to achieve the same results as predened prompting formats in our experiments, achieving only 18 : 9% after 2 rounds of training. 23