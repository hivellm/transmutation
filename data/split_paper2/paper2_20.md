## B Experimental Details: Knowledge Incorporation

## B.1 Model and Setup

We use the Qwen-2.5-7B base model [ 5 ] in the knowledge incorporation experiments. We repurpose the SQuAD dataset v 1.1 [ 13 ] for the task of answering questions without the passage i n-context. We use the training set for RL training and a 200-article subset of the evaluation set for evaluation. Within the training set and evaluation set, there are some overlapping topics of passages, but there is no overlap between these sets, so we can be sure that there is no data contamination of the test passages due to RL training. B.2 RL Training Procedure We run 2 rounds of ReST EM training [ 40 ]. On each round, we take a batch of 50 context-questionsanswers triples from the SQuAD training set. For each context, we sample 5 self-edit generations at temperature 1 . We evaluate each self-edit over 3 random seeds, training on the sequences and then evaluating the updated model on the corresponding questions. We average each generation's results over 3 seeds and then keep the single best generation for each of the 50 contexts. Finally, to nish the round of ReST EM , we perform supervised netuning on the 50 resulting prompt-completion pairs. Supervised netuning here is done with batch size of 10, for 2 epochs, with learning rate 3e-4, using LoRA [72] with rank 64 and alpha 128, applied to all MLP and attention projection layers. B.3 Synthetic Data Generation and Finetuning Details In all models, we generate synthetic data by prompting to generate implications of the passage: Let's read the following passage and produce a list of implications derived directly or indirectly from the content. Passage: {passage} Implications: We then take the resulting generated sequence. In the single-passage case, we split it by newlines into a set of training documents. In the multi-passage case, we use the full generated sequence as a single training document. In the case of synthetic data from GPT-4.1 ( gpt-4.1-2025-04-14 ), an instruct-model, we additionally have the following rule: If the second line begins with a �1.� then we omit the rst line from the training set. This is because we found that the rst line often contained ller text (e.g. �Sure, here is the list of implications:�). We then use the following training hyperparameters: Table 3: Single-Passage Knowledge Incorporation Hyperparameters Parameter Search Space LoRA Rank ( r ) [ 32 , 64] LoRA Alpha ( ) [32, 64 ] Learning Rate [1e-4, 3e-4, 5e-4, 1e-3 , 2e-3] Epochs [1, 5, 10 , 15, 20] Batch Size [ 1 , 4] In the multi-passage n = 200 case, we sample 5 self-edit completions for each passage and take the aggregate dataset of all self-edits across all passages to train o n. To answer the corresponding questions, we use the following prompt: 20