## A Experimental Details: Few-shot Learning

## A.1 Model and Setup

For the few-shot learning experiments, we use Llama-3.2-1B-Instruct [ 3 ] as the base language model. Since this model has no specialized training on ARC, its ability to solve ARC tasks is limited. To enable controlled evaluation, we curated a small set of ARC problems from the training and evaluation splits that are solvable with optimal TTT hyperparameters. Training Set: We selected 11 ARC tasks from the training set as the environment for RL optimization. Evaluation Set: We selected 8 distinct ARC problems from the evaluation set for measuring generalization performance. These 8 were explicitly ltered for being amenable to TTT out of the full evaluation set. These sets were chosen to isolate the effect of self-edit learning rather than general ARC ability. A.2 Training Procedure We trained SEAL using a single outer loop of reinforcement learning on the 11 training problems. For each problem, the model generated 15 self-edit congurations consisting o f: � Data augmentation strategy: Controls whether to include variations such as basic examples, size-based augmentations, chained reasoning, and repeated inputs: include_basic , include_size , include_chain , include_repeat . � Optimization conguration: Specie s hyperparameters such as learning rate, number of epochs, and loss function type (e.g., whether to compute loss over all tokens or only the nal output tokens). Each conguration was evaluated via test-time training (TTT), and assigned a binary reward: 1 if the adapted model produced the correct solution, 0 otherwise using Aky�rek et a l. [36] 's evaluation protocol. To encourage e fcient adaptation, we discarded congurations requiring more than 375 training steps, retained only congurations with reward 1 and acceptable cost for LoRA-based SFT. SFT for TTT was done with the following hyperparameters: Parameter Value LoRA rank 128 LoRA alpha 16 Learning rate N/A (model chooses i t) num_train_epochs N/A (model chooses i t) LoRA adapters were inserted into the following modules: q_proj , v_proj , gate_proj , down_proj , up_proj For the RL SFT we used the following hyperparameters: Parameter Value lora_rank 16 lora_alpha 16 num_train_epochs 8 per_device_train_batch_size 5 learning_rate 5e-5 When generating the self-edits, we add the following prompt as a sufx to the few-shot examples. 18