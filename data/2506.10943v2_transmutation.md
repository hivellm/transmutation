## Self-Adapting Language Models

Adam Zweiger ∗ Jyothish Pari ∗† Han Guo Ekin Akyürek Yoon Kim Pulkit Agrawal
† Massachusetts Institute o f Technology {adamz, jyop, hanguo, akyurek, yoonkim, pulkitag}@mit.edu


## Abstract

Large language models (LLMs) are powerful but static; they lack mechanisms t o adapt their weights i n response t o new tasks, knowledge, o r examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs t o self-adapt b y generating their own finetuning data and update directives. Given a new input, the model produces a self-edit—a generation that may restructure the information i n different ways, specify optimization hyperparameters, o r invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result i n persistent weight updates, enabling lasting adaptation. To train the model t o produce effective self-edits, w e use a reinforcement learning loop, using the downstream performance o f the updated model a s the reward signal. Unlike prior approaches that rely o n separate adaptation modules o r auxiliary networks, SEAL directly uses the model’s generation t o parameterize and control its own adaptation process. Experiments o n knowledge incorporation and fewshot generalization show that SEAL i s a promising step toward language models capable o f self-directed adaptation i n response t o new data. Our website and code i s available a t https://jyopari.github.i o/posts/seal.

## 1 Introduction

Large language models (LLMs) pretrained o n vast text corpora exhibit remarkable abilities i n language understanding and generation [1, 2, 3, 4, 5]. However, adapting these powerful models for specific tasks [6], integrating new information [7], o r mastering novel reasoning skills [8] remains challenging due t o the limited availability o f task-specific data. In this paper, w e explore a n intriguing hypothesis: can a n LLM self-adapt b y transforming o r generating its own training data and learning procedure?

As a n analogy, consider a human student preparing for the final exam o f a machine learning class. Many students rely o n their notes t o prepare for the exam. These notes are often derived from the lecture content, textbooks, o r information available o n the internet. Instead o f relying o n the raw content, assimilating and rewriting the information i n the form o f notes often improves the ability o f students t o understand the content and answer exam questions. This phenomenon o f reinterpreting and augmenting external knowledge i n a way that i s easier t o understand i s not limited t o just taking exams, but seems t o b e universally true o f human learning across tasks. Furthermore, different humans assimilate information i n different ways—some might condense the information into a visual diagram, some into text, o r some might rely more o n concrete mathematical descriptions.

Such assimilation, restructuring, o r rewriting o f data a s part o f the learning process i s i n contrast with how large language models (LLMs) are typically trained and deployed. Given a new task, current LLMs consume and learn from the task data “a s-i s” via finetuning o r i n-context learning [9, 10, 11, 12]. However, such data may not b e i n a n optimal format (o r volume) for learning, and

∗ Equal contribution.

† Improbable AI Lab, CSAIL MIT

39th Conference o n Neural Information Processing Systems (NeurIPS 2025).arXiv:2506.10943v2  [c s.LG]  18 Sep 2025 RL Outer Loop IterationCtx LM PolicyUpdateRL DataTest LM

## 1 1

23 OptimizeOptimizeOptimize SE 2 SE 3 SE Test LMTest LM Ans ✓✓✘ ✘AnsAnsSESESE S tate A ction R ewardCtx SE ✓Figure 1: Overview o f SEAL. In each RL outer loop iteration, the model generates candidate self-edits (SE)—directives o n how t o update the weights—applies updates, evaluates performance o n a downstream task, and uses the resulting rewards t o improve the self-edit generation policy.

current approaches d o not enable models t o develop bespoke strategies for how t o best transform and learn from their training data.

As a step towards better language model adaptation, w e propose equipping LLMs with the ability t o generate their own training data and finetuning directives i n response t o new inputs. In particular, w e introduce a reinforcement learning algorithm that trains LLMs t o generate “self-edits”—naturallanguage instructions that specify the data and, optionally, optimization hyperparameters for updating the model’s weights (see Figure 1). We refer t o such models a s Self-Adapting LLMs (SEAL).

We evaluate SEAL o n two applications. We first consider the task o f integrating new factual knowledge into a n LLM. Rather than finetuning directly o n the passage text, w e finetune o n synthetic data generated b y the SEAL model. Our results show that, following reinforcement learning (RL) training, finetuning o n self-generated synthetic data improves question-answering performance o n the n o-passage-i n-context variant o f SQuAD [13] from 33.5% t o 47.0%. Notably, self-generated data from SEAL outperforms synthetic data generated b y GPT-4.1.

We further evaluate SEAL o n few-shot learning o n a simplified subset o f the ARC-AGI benchmark [14], where the model leverages a set o f tools t o autonomously select both synthetic data augmentations and optimization hyperparameters (e.g., learning rate, training epochs, selective loss computation over token types). Our experiments demonstrate that automatic selection and configuration o f these tools using SEAL enhances performance compared t o both standard i n-context learning (ICL) and self-editing without RL training t o use the tools effectively. These results collectively show that SEAL i s a versatile framework for enabling language models t o self-adapt.

## 2 Related Work

Synthetic Data Generation. The creation o f synthetic data for LLM training i s increasingly common, from large-scale pretraining datasets [15, 16, 17, 18, 19] t o task-specific data augmentation [20, 21, 22] and instruction-tuning sets [23, 24]. For incorporation o f a smaller corpus, Yang e t a l. [25] use synthetic data generation via graph-based prompting. SEAL builds o n this line o f work b y using reinforcement learning t o train a generative policy that directly maximizes the downstream utility o f synthetic data when applied for gradient-based self-updates, rather than relying o n static o r heuristic generation strategies that are manually tuned.

Knowledge Updating. Several recent works aim t o modify o r inject factual knowledge into language models via weight updates. Some methods attempt t o directly locate specific parameters that correspond t o individual facts [26, 27, 28]. Others propose generating additional finetuning data using the information i n context [29, 30, 25, 31, 32]. We adopt the latter strategy, following Akyürek e t a l. [30], who propose generating logical implications o f a fact and finetuning o n them, and Lampinen e t a l. [31], who show that implication-based finetuning can even outperform i n-context learning. We build o n these approaches b y training models through RL t o generate more optimal finetuning data. Park e t a l. [32] show that prompting language models t o generate question–answer (QA) pairs directly can outperform implication-style prompting. Because the SEAL framework i s agnostic t o the prompt and format o f the self-edit data, i t can also b e trained t o generate QA pairs o r other output formats, a s explored i n §B.11. 2Test-Time Training. Test-Time Training (TTT) temporarily adapts model weights based o n the input the model receives [33, 34, 35, 36]. Akyürek e t a l. [36] show that combining TTT with ICL enables gradient-updates t o outperform standard ICL i n the few-shot setting. SEAL can b e viewed a s incorporating a round o f TTT i n its inner-loop optimization, leveraging TTT’s efficiency relative t o full-scale training t o perform multiple updates and reward the generated data that yields the greatest performance gain. Although our method i s trained using single-example TTT episodes, w e demonstrate i n the knowledge incorporation setting that i t generalizes t o the continued pretraining (CPT) regime—where placing data directly i n context i s n o longer feasible.

Reinforcement Learning for LLMs. Reinforcement learning has played a central role i n improving LLM behavior, originally through RLHF [37, 38]. More recently, RL with verifiable rewards has been applied t o boost reasoning performance b y optimizing the model directly for task success [39, 40, 41]. SEAL applies RL not t o optimize final answers o r trace revisions, but t o optimize the generation o f self-edit data that i s then used for weight updates.

Meta-Learning and Self-Modifying Systems. SEAL embodies meta-learning principles [42, 43, 44] b y learning a n adaptation strategy—how t o generate effective self-edits—via its outer optimization loop. The goal i s t o learn how t o learn efficiently from task contexts. In reinforcement learning, meta-learning has been used t o train agents that learn new tasks quickly [45, 46, 47, 48]. Sun e t a l. [49] similarly apply RL t o learn task-specific weight modulations, offering a n alternative t o LoRA finetuning that i s orthogonal t o our approach. A natural extension o f meta-learning i s self-referential networks, where models modify their own parameters [50, 51]. In the domain o f large language models, recent work has applied meta-learning t o improve LLM adaptation [52, 53, 54, 55, 49]. Notably, Hu e t a l. [53] train a smaller model t o output token-specific weights during finetuning, addressing a knowledge incorporation task similar t o ours, while Chen e t a l. [54] propose a hypernetwork that generates LoRA adapters conditioned o n the input, enabling dynamic and task-specific parameterization. However, SEAL offers greater generality b y leveraging the model’s existing generative capabilities t o parametrize updates.

Self-Improvement. Several recent works fall under the umbrella o f self-improvement o r selftraining. Methods such a s RLAIF [56, 57] and self-rewarding language models [58, 59] use the model itself t o provide reward signals, leveraging the observation that judging outputs i s often easier than generating them [60]. Other recent works improve performance o n mathematical tasks b y using majority-vote o r model confidence a s reinforcement learning rewards, enabling performance improvement without access t o ground-truth labels [61, 62, 63, 64, 65]. However, all o f these methods are fundamentally limited b y the model’s current evaluation abilities and self-consistency. In contrast, w e view self-improvement through interaction with external data a s a more powerful and scalable path. SEAL learns how t o best utilize this external data for self-improvement.

## 3 Methods

We propose Self-Adapting LLMs (SEAL), a framework that enables language models t o improve themselves b y generating their own synthetic data and optimization parameters (“self-edits”) i n response t o new data. The model i s trained t o produce these self-edits directly through token generation with the data provided i n the model’s context. Self-edit generation i s learned via reinforcement learning (RL) where the model i s rewarded for generating self-edits (SE) that, when applied, improve the model’s performance a t the target task. SEAL can therefore b e interpreted a s a n algorithm with two nested loops: a n outer RL loop, which optimizes the self-edit generation, and a n inner update loop, which uses the generated self-edit t o update the model via gradient descent. Our method can b e seen a s a n instance o f meta-learning where w e meta-learn how t o generate effective self-edits.

## 3.1 General Framework

Let θ denote the parameters o f the language model LMθ. SEAL operates o n individual task instances (C, τ ) where C i s a context containing information relevant t o the task, and τ defines the downstream evaluation used t o assess the model’s adaptation. For example, i n knowledge incorporation, C i s the passage intended t o b e integrated into the model’s internal knowledge, and τ i s a set o f questions and associated answers about the passage. In few-shot learning, C includes few-shot demonstrations o f a3novel task, and τ i s the query input and ground-truth output. Given C, the model generates a self-edit SE—the form o f which varies b y domain (see §3.2)—and updates its parameters via supervised finetuning: θ′ ← SFT(θ, SE).

We optimize the self-edit generation process using reinforcement learning: the model takes a n action (generating SE), receives a reward r based o n LMθ′’s performance o n τ , and updates its policy t o maximize expected reward:

LRL(θt) := −E(C,τ )∼D [ESE∼LMθt (·|C) [r(SE, τ, θt)] ] . (1)Algorithm 1 Self-Adapting LLMs (SEAL): Self-Edit Reinforcement Learning Loop

## 1: Input: LMθ, dataset D = {(C, τ )}

## 2: for outer iteration t = 1, 2, . . . d o

## 3: Sample (C, τ ) ∼ D

## 4: Generate self-edit SE ∼ LMθ(· | C)

## 5: Inner Loop Update: θ′

t ← SFT(θt, SE)

## 6: Evaluate: Ans ∼ LMθ′

t (· | τ )

## 7: Compute reward: r ← r(Ans, τ )

## 8: Update: θt+1 ← RL_Update(θt, r, SE)

## 9: end for

Unlike i n standard RL setups, the reward assigned t o a given action i n our setting depends o n the model parameters θ a t the time the action i s taken (since θ i s updated t o θ′, which i s then evaluated). As a result, the underlying RL state must include the policy’s parameters and i s given b y (C, θ), even though the policy’s observation i s limited t o C (placing θ directly i n context i s infeasible). The implication o f this i s that (state, action, reward) triples collected with a previous version o f the model, θold, may become stale and misaligned for the current model θcurrent. For this reason, w e adopt a n o n-policy approach, i n which self-edits are sampled from—and, crucially, rewards are computed using—the current model.

We experimented with various o n-policy methods such a s Group Relative Policy Optimization (GRPO) [66] and Proximal Policy Optimization (PPO) [67], but found the training t o b e unstable. Instead, w e adopt ReST EM [40], a simpler approach based o n filtered behavior cloning—also known a s “rejection sampling + SFT” [68, 69, 38, 39, 70].

ReST EM can b e viewed a s a n expectation-maximization (EM) procedure: the E-step samples candidate outputs from the current model policy, and the M-step reinforces only those samples that receive positive reward through supervised finetuning. This approach optimizes a n approximation o f our objective (1) under the binary reward:

r(SE, τ, θt) = {1 If o n τ , adaptation using SE improves LMθt’s performance 2

## 0 Otherwise (2)

More precisely, i n optimizing (1), w e must compute the gradient ∇θtLRL. However, a s w e noted, the reward term r(SE, τ, θt) depends o n θt i n our setup but i s not differentiable. We address this b y treating the reward a s fixed with respect t o θt. With this approximation, the Monte-Carlo estimator for a minibatch o f N contexts and M sampled self-edits per context becomes∇θtLRL ≈ − 1 N M N∑i=1 M∑j=1 rij ∇θt log pθt(SEij | Ci) (3)= − 1 N M N∑i=1 M∑j=1 rij T∑s=1 ∇θt log pθt( y(i,j) s | y( i,j) <s , Ci), (4)where pθt denotes the model’s autoregressive distribution and y( i,j) s i s the s t h token o f self-edit SEij, the jth sample for context Ci. Since sequences with r = 0 can b e ignored i n (4), w e have shown that ReSTEM, with simple “SFT o n good self-edits,” indeed optimizes (1) under the binary reward (2) (with a stop-gradient applied t o the reward term). The SEAL training loop i s summarized i n Alg. 1.

Finally, w e note that while the implementation i n this work uses a single model for both generating self-edits and learning from these self-edits, i t i s also possible t o decouple these roles. In such a “teacher-student” formulation [71], a student model would b e updated using edits proposed b y a separate teacher model. The teacher would then b e trained via RL t o generate edits that maximize student improvement.

2The reward may also b e assigned t o the single self-edit that yields the greatest improvement among sampled candidates, which w e d o i n knowledge incorporation, rather than t o all edits that yield a positive improvement.

4

## 3.2 Domain Instantiations

We instantiate the SEAL framework i n two distinct domains: knowledge incorporation and few-shot learning. These domains were chosen t o highlight two complementary forms o f model adaptation: (1) the ability t o integrate new information into a model’s weights s o that i t can b e recalled without relying o n context (evaluated using a n o-context variant o f SQuAD) and (2) the ability t o generalize t o novel tasks after seeing only a small number o f examples (evaluated using ARC).

Knowledge Incorporation. Our goal i s t o efficiently incorporate the information provided i n a passage into the model’s weights. A promising recent approach involves using a language model t o generate content derived from the passage, followed b y finetuning o n both the original passage and the generated content [29, 30, 25, 31, 32]. While the form o f generated content may vary, w e adopt what w e consider the canonical format: implications derived from the passage. This approach, introduced i n deductive closure training [30], converts a given context C into a set o f implications SE = {s 1, s 2, . . . , s n} b y prompting the model t o “List several implications derived from the content.” The output may include inferences, logical consequences, o r restatements o f the original passage. In §B.11, w e also explore alternative prompts such a s “rewrite the passage i n different ways” o r “rewrite i n a question-answer format” and show that our method improves performance b y similar o r greater margins regardless o f the base prompt. ✓ SFTKnowledge Incorporation SetupPassagetitle: Apollo program context: But even after NASA reached internal agreement, i t was far from smooth sailing... Self-Edit EvaluationLM LM

## 1. The Apollo program faced opposition from Kennedy's science advisor, Jerome Wiesner, who had...

Who was Kennedy's science adviser that opposed manned spacecraft flights? Jerome WiesnerFigure 2: Knowledge Incorporation Setup. Given a new passage, the model generates synthetic data (the self-edit) i n the form o f “implications” o f the passage. We then finetune o n these outputs using LoRA. The updated model i s evaluated o n questions about the passage without access t o the original text, and the resulting accuracy serves a s the reward signal for reinforcement learning.

These self-generated statements form the training data for a supervised finetuning (SFT) update: w e compute the standard causal language-modeling loss over each sequence s i and update the model parameters, yielding θ′. Since the amount o f data per update i s small and the number o f updates w e d o i n total i s large, w e use low-rank adapters (LoRA [72]) for efficient, lightweight tuning. Finally, the adapted model LMθ′ i s evaluated o n the task τ . This process i s shown i n Figure 2.

During RL training, the adapted model’s accuracy o n τ defines the reward r that drives the outer RL optimization. This trains the model t o restructure the passage i n a way that i s most effective for assimilation via finetuning.

Few-ShotExamples Self-Edit (SE) EvaluationLM SFT ✓LMbasic_augmentations: truesize_augmentations: false chain_augmentations: false repeat_augmentations: falsestrategy: loss o n all tokens learning rate: 1e-05epochs: 3 Few-Shot Setup Figure 3: Few-Shot Learning with SEAL. Left: example ARC demonstrations. Center: the model generates a self-edit specifying augmentations and training hyperparameters. Right: the adapted model i s evaluated o n a held-out test input. 5Few-Shot Learning. The Abstraction and Reasoning Corpus (ARC) [8] i s a benchmark designed t o test abstract reasoning and generalization from very limited examples. Each task includes a small set o f input-output demonstrations and a held-out test input whose correct output must b e predicted.

We adopt the test-time training (TTT) protocol o f Akyürek e t a l. [36], where augmentations o f the few-shot examples are used t o perform gradient-based adaptation. Rather than relying o n manually tuned heuristics for selecting augmentations and optimization settings, w e train SEAL t o learn these decisions. This setting tests whether SEAL can autonomously configure the adaptation pipeline— determining which augmentations t o apply and what optimization parameters t o use.

To implement this, w e define a set o f tools, each o f which i s a pre-defined function from Akyürek e t a l. [36] that transforms data o r specifies training parameters. These include:

• Data augmentations: rotations, flips, reflections, transpositions, resizing operations (e.g., changing grid resolution), and chained o r repeated transformations. • Optimization parameters: learning rate, number o f training epochs, and whether the loss i s computed over all tokens o r only output tokens.

The model i s prompted with a task’s few-shot demonstrations t o generate a self-edit, which i n this case i s a specification o f which tools t o invoke and how t o configure them, a s shown i n Figure 3. The self-edit i s then applied t o adapt the model via LoRA finetuning. The adapted model i s evaluated o n the held-out test input, and the result determines the reward for the self-edit generation.

## 4 Results

In this section w e empirically evaluate SEAL across our two adaptation domains: few-shot learning and knowledge incorporation. Full training, hyperparameter, and evaluation details are provided i n §A and §B.

## 4.1 Few-Shot Learning

We conduct our experiments using Llama-3.2-1B-Instruct, a small open-source model with n o ARC-specific pretraining. Since most ARC tasks are challenging for models that have not been pretrained o n ARC, w e curate a subset o f 11 tasks from the ARC training set and 8 from the evaluation set, filtered t o ensure that they are solvable under optimal TTT configurations for a base Llama-3.2-1B-Instruct. While this i s a small number o f examples, note that Akyürek e t a l. [36] used the same TTT configuration for all tasks, and thus w e d o not need a large training set for learning a fixed self-edit. More details are included i n §A.

The model i s trained using ReSTEM b y sampling 15 self-edits per training task. Each self-edit i s applied individually t o generate 15 updated models, which are then evaluated o n the corresponding held-out test example. We reinforce only those self-edits that lead t o correctly adapted models, i.e., models that produce the correct output for the test input after adaptation.

After training, w e evaluate the model b y generating 5 self-edits per held-out evaluation task and apply each one independently. We then report the percentage o f self-edits that lead t o correct outputs, yielding a success rate that reflects the quality o f the learned self-edit generation policy.

We compare against the following baselines:

## 1. ICL (In-Context Learning): Llama-3.2-1B-Instruct i s prompted with the given few-shot

examples using Akyürek e t a l. [36]’s protocol, and directly queried o n the test input.

## 2. TTT + Self-Edit (w/o prior RL): Llama-3.2-1B-Instruct performs test-time training (TTT)

using few-shot examples and synthetic augmentations, but without any prior RL t o optimize which augmentations o r training configurations t o use.

## 3. Oracle TTT: The model performs test-time training (TTT) using the optimal human-crafted

configuration from Akyürek e t a l. [36]. This provides a n upper bound o f our method.

We record results i n Table 4.1. SEAL substantially improves adaptation success rate compared t o baselines: 72.5% v s. 20% (with self-edits from the base model without RL training) and 0% (n o adaptation), though performance remains below Oracle TTT, suggesting room for further improvement.

6Method Success Rate (%)ICL 0 TTT + Self-Edit (w/o prior RL) 20 SEAL 72.5 Oracle TTT 

Table 1: Few-shot Abstract Reasoning

## 4.2 Knowledge Incorporation

We experiment with Qwen2.5-7B o n incorporating novel factual content from SQuAD passages [13]. We use the relatively simple SQuAD dataset because its passages can b e fully “understood” b y the base model i n-context, yet the model cannot reliably answer questions about them without that context. We d o 2 rounds o f ReST EM with a batch o f 50 contexts (see §B for further details). We compare SEAL o n knowledge incorporation against the following baseline approaches:

## 1. Base Model: The pretrained model i s evaluated o n downstream QA tasks without any adaptation

o r access t o the passage.

## 2. Train o n Passage Only: The model i s finetuned directly o n the passage using the standard

language modeling loss, without any synthetic data.

## 3. Train o n Passage + Synthetic Data: The model i s trained o n the passage along with self-generated

implications. This i s the same setup a s SEAL but without any prior RL training.

## 4. Train o n Passage + GPT-4.1 Synthetic Data: The model i s trained o n the passage along with

model-generated implications collected from GPT-4.1 via the OpenAI API.

## 0 1 2

ReST-EM RL Training Iterations32343638404244

## 46Knowledge Incorporation (%)

Single-Passage Knowledge IncorporationQwen 2.5 7B BaseRaw PassageQwen-Base SyntheticGPT-4.1 SyntheticSEAL Figure 4: Accuracy over RL iterations. Each iteration consists o f a minibatch o f 50 contexts, each with 5 sampled self-edits. SEAL surpasses GPT-

## 4.1 synthetic data after two iterations o f

ReSTEM o n the n o-context SQuAD set.

Table 4.2 reports mean n o-context SQuAD accuracy under two regimes: single-passage updating (with LoRA), and small-scale continued pretraining (with full finetuning). We run continued pretraining (CPT) experiments with n = 200 documents, a s well a s the full SQuAD validation set o f n = 2067 documents. In the single-passage setting, finetuning directly o n the passage yields a negligible gain over the frozen base model (33.5% v s. 32.7%), confirming that using the raw data alone i s insufficient. Augmenting with synthetic implications generated b y GPT-4.1 boosts accuracy t o 46.3%, a n improvement o f 12.8 percentage points over the passage-only baseline. Using synthetic data produced b y the base Qwen-2.5-7B model yields

## 39.7%, a 6.2-point increase. After reinforcement learn-

ing, SEAL further improves accuracy t o 47.0%, notably outperforming using synthetic data from GPT-4.1, despite being a much smaller model.

In the CPT setting, the model assimilates information from many passages i n a single continued pretraining run. It i s then evaluated o n the union o f all corresponding questions. In this setting, w e sample 5 self-edit generations for each passage and take the aggregate synthetic dataset for continued pretraining. As shown i n Table 4.2, w e observe a similar ranking o f methods a s i n the single-passage case, but with synthetic data from GPT-4.1 slightly outperforming SEAL. In the n = 200 setting, SEAL achieves a n accuracy o f 58.2%, exceeding its single-passage performance. We attribute this gain t o the aggregation o f multiple self-edit generations. Overall, the strong continued pretraining results o f SEAL suggest that the self-editing policy generalizes beyond the original RL setup o f creating synthetic data i n a single generation for a single passage.

Figure 4 tracks accuracy after each outer RL iteration. Two iterations suffice for SEAL t o overtake GPT-4.1 data; subsequent iterations yield diminishing returns, suggesting that the policy quickly converges t o a n edit style that distills the passage into easily learnable atomic facts (see qualitative examples i n Figure 5). All results use tuned hyperparameters (see §B).

7Method Single Passage (n = 1; LoRA) Continued Pretraining (n = 200; full-FT) Continued Pretraining (n = 2067; full-FT)Base model 32.7 32.7 29.0 Train o n Passage 33.5 36.0 31.2 Train o n Passage + Synthetic 39.7 50.6 43.4 Train o n Passage + GPT-4.1 Synthetic 46.3 59.4 49.2 SEAL 47.0 58.2 46.

Table 2: Knowledge Incorporation Performance Across Passage Settings.

A1:  *model response was empty*A2:  The Kayapo tribe uses GPS devices t o map their ancestral lands i n the Amazon rainforest. A3:  Some tribes i n the Amazon rainforest use remote sensing technology t o map out their ancestral lands. This helps them t o better protect their territories from commercial interests.

A1:  Drones are being used t o map and protect the lands o f the tribes i n the Amazon.

A2:  The Yanomami tribe uses GPS devices t o map their ancestral lands i n the Amazon rainforest. A3:  Some tribes use remote sensing technology t o protect their lands from commercial interests.

✘ ✘✘✘Input ContextRewrite Iteration 0Iter 0 Rewrite Iteration 1 Rewrite Iteration 2Iter 1 The use o f remote sensing for the conservation o f the Amazon i s also being used b y the indigenous tribes o f the basin t o protect their tribal lands from commercial interests. Using handheld GPS devices and programs like Google Earth, members o f the Trio Tribe,  who live i n the rainforests o f southern Suriname, map out their ancestral lands t o help strengthen their territorial claims. Currently, most tribes i n the Amazon d o not have clearly defined boundaries, making i t easier for commercial ventures t o target their territories. Q1:  What kind o f sending technology i s being used t o protect tribal lands i n the Amazon? Q2:  What tribe uses GPS devices t o map lands?

Q3:  Why d o some tribes use remote sensing technology?

1. Indigenous tribes can use remote sensing and mapping technology t o protect their lands from commercial interests. 2. Clear boundaries for tribal lands can prevent commercial ventures from targeting and exploiting the resources i n their territories.

## 1.  R emote sensing technology can b e effectively utilized for

the conservation o f the Amazon rainforest.

## 2. Indigenous tribes are becoming more empowered t o protect their lands from commercial interests.

## 3 . The use o f handheld GPS devices and programs like

Google Earth can b e a valuable tool for mapping ancestral lands.

## 4 . Clearing o f boundaries for indigenous territories can help

prevent commercial ventures from targeting these lands.

## 5 . The involvement o f indigenous communities i n the

conservation efforts o f the Amazon rainforest can lead t o a more sustainable future for the region.

## 1.  R emote sensing technology can b e used b y indigenous

tribes t o protect their lands.

## 2. The use o f GPS devices and programs like Google Earth

can help map out ancestral lands.

## 3 . Clearing o f forests for agricultural land, such a s cattle

ranches, i s a m a j o r cause o f deforestation i n the Amazon.

## 4 . Illegal mining for gold and other minerals i s a significant

threat t o the Amazon rainforest.

## 5 . The expansion o f infrastructure, such a s roads and dams,

i s also a m a j o r concern for the conservation o f the Amazon.

## 6 . The use o f remote sensing can help identify areas o f the

Amazon that are most vulnerable t o deforestation and other threats.

## 7 . The involvement o f indigenous tribes i n the protection o f

the Amazon rainforest i s crucial for its conservation.

## 8 . The mapping o f ancestral lands b y indigenous tribes can

help strengthen their territorial claims and protect their rights t o the land.

## 9 . The use o f remote sensing can help monitor the Amazon

rainforest and track changes over time.

## 1 0 . The protection o f the Amazon rainforest i s essential for

the preservation o f biodiversity and the maintenance o f ecosystem services. ✓✓ ✓ ✓ ✓Iter 2A1:  The use o f remote sensing technology i s being used t o protect tribal lands i n the Amazon. A2:  The use o f GPS devices b y the Trio Tribe, a tribe from the rainforests o f southern Suriname, i s being used t o help protect their ancestral lands.

A3:  Some tribes use remote sensing technology t o protect their lands from commercial interests.

Figure 5: Example Knowledge Incorporation Self-Edits Across RL Iterations. In this example, w e see how RL leads t o the generation o f more detailed self-edits, which i n turn results i n better performance. While the progression i s clear i n this case, the differences across iterations are sometimes more subtle i n other examples. We show i n §B.11 that prompting for longer self-edits i s effective, and that RL training further improves performance b y a similar margin.

## 5 Limitations

Passage IndexSelf-Edit Iteration Catastrophic Forgetting Figure 6: Catastrophic forgetting from continual self-edits. We sequentially update the model o n new passages and track degradation o n prior tasks. Entrywise standard errors are reported i n §B.6.

Catastrophic forgetting. One key motivation w e had for enabling language models t o self-edit i s t o move towards the ultimate goal o f continual learning—allowing models t o incorporate new information over time, whether through agentically interacting with a n environment o r through standard training. While our earlier experiments assess how well SEAL adapts t o individual edits i n isolation, a more ambitious goal i s t o support sequences o f edits: can the model adapt t o new information repeatedly while preserving prior knowledge?

This question relates directly t o the challenge o f catastrophic forgetting [73, 74], where new updates interfere destructively with past learning. We d o not explicitly optimize for retention i n our current training setup, but w e aim t o establish a baseline for how well SEAL handles sequential self-edits without dedicated mechanisms for handling catastrophic forgetting. To test this, w e simulate a continual learning setting i n the knowledge incorporation domain. The model receives a stream o f test passages, each triggering a new self-edit. After each update, we8re-evaluate the model o n all previously seen tasks t o measure retention. This setup tests the model’s ability t o integrate new edits without forgetting earlier ones.

As shown i n Figure 6, performance o n earlier tasks gradually declines a s the number o f edits increases, suggesting that SEAL i s still susceptible t o catastrophic forgetting. Still, i t can perform multiple updates without complete collapse, indicating possibility for improvement. Future work could enhance this ability through reward shaping [75, 76, 77] t o penalize regressions o n earlier tasks, o r b y integrating continual learning strategies such a s null-space constrained edits [78] o r representational superposition [79]. In addition, since RL has been shown t o forget less than SFT, SEAL’s inner loop could also employ RL instead o f SFT [80].

Computational overhead. The TTT reward loop i s significantly more computationally expensive than other reinforcement learning loops used with LLMs. For instance, reward signals based o n human preferences typically involve a single model forward pass, and those using verified solutions may rely o n simple pattern matching (e.g., regex). In contrast, our approach requires finetuning and evaluating a n entire model t o compute the reward—each self-edit evaluation takes approximately

## 30–45 seconds, introducing substantial overhead (see §B.5).

Context-dependent evaluation. Our current instantiations assume that every context i s paired with a n explicit downstream task: few-shot demonstrations arrive with a held-out query pair, and each passage comes bundled with reference QA. This coupling simplifies reward computation but prevents RL training o f SEAL from scaling t o unlabeled corpora. A potential solution i s t o let the model generate not only self-edits but also its own evaluation questions—e.g., draft QA items o r synthetic test cases for each passage—while the original content i s still i n context. These model-written queries could provide the immediate supervision required for reinforcement learning, broadening applicability t o general training domains where external question-and-answer sets are unavailable.

## 6 Discussion and Conclusion

Villalobos e t a l. [81] project that frontier LLMs will b e trained o n all publicly available humangenerated text b y 2028. We argue that this impending “data wall” will necessitate the adoption o f synthetic data augmentation. Once web-scale corpora are exhausted, progress will hinge o n a model’s capacity t o generate its own high-utility training signal. A natural next step i s t o meta-train a dedicated SEAL synthetic-data generator model that produces fresh pretraining corpora, allowing future models t o scale and achieve greater data efficiency without relying o n additional human text.

We can imagine a future i n which LLMs can ingest new data, such a s academic papers, and generate large quantities o f explanations and implications for themselves using their existing knowledge and reasoning with the i n-context data. This iterative loop o f self-expression and self-refinement could allow models t o keep improving o n rare o r underrepresented topics even i n the absence o f additional external supervision.

In addition, while modern reasoning models are often trained with RL t o generate chain-o f-thought (CoT) traces, SEAL could offer a complementary mechanism, allowing the model t o learn when and how t o update its own weights. These two approaches could synergize: the model may choose t o perform weight updates mid-reasoning t o guide its current trajectory, o r after completing reasoning t o distill key insights into its parameters—improving future inference through internalized learning.

This continual refinement loop i s also promising for building agentic systems—models that operate over extended interactions and adapt dynamically t o evolving goals. Agentic models must incrementally acquire and retain knowledge a s they act. Our approach supports such behavior b y enabling structured self-modification: after a n interaction, the agent could synthesize a self-edit which triggers a weight update. This could allow the agent t o develop over time, aligning its behavior with prior experience and reducing reliance o n repeated supervision.

SEAL demonstrates that large language models need not remain static after pretraining: b y learning t o generate their own synthetic self-edit data and t o apply i t through lightweight weight updates, they can autonomously incorporate new knowledge and adapt t o novel tasks. Looking ahead, w e envision extending the SEAL framework t o pretraining, continual learning, and agentic models, ultimately enabling language models t o self-learn and scale i n a data-constrained world.

9Acknowledgments and Disclosure o f FundingWe would like t o thank Shivam Duggal, Idan Shenfeld, Seungwook Han, Jeremy Bernstein, Akarsh Kumar, Linlu Qiu, Juno Kim, Brian Cheung, Moritz Reuss, Ayush Sekhari, Zhang-Wei Hong, Mehul Damani, Leshem Choshen, and Ryan Yang for their valuable discussions and feedback. We acknowledge support from ARO MURI grant number W911NF-23-1-0277. This research was also partly sponsored b y the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19- 2-1000. The views and conclusions contained i n this document are those o f the authors and should not b e interpreted a s representing the official policies, either expressed o r implied, o f the United States Air Force o r the U.S. Government. The U.S. Government i s authorized t o reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein. We acknowledge the MIT Office o f Research Computing and Data for providing high performance computing resources that have contributed t o the research results reported within this paper. This research was also partly supported b y the Stevens Fund for MIT UROP research and b y the MIT-IBM Watson AI Lab.

## References

[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances i n Neural Information Processing Systems, 2020. URL https://proceedings.neurips.c c/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.

[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971.

[3] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, e t a l. The Llama 3 herd o f models, 2024. URL https://arxiv.org/abs/2407.21783.

[4] Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah Smith, and Hannaneh Hajishirzi. OLMo: Accelerating the science o f language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings o f the 62nd Annual Meeting o f the Association for Computational Linguistics. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.acl-long.841/.

[5] Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115.

[6] Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don’t stop pretraining: Adapt language models t o domains and tasks. In10Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings o f the 58th Annual Meeting o f the Association for Computational Linguistics. Association for Computational Linguistics, 2020. URL https://aclanthology.org/2020.acl-main.740/.

[7] Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. Modifying memories i n transformer models, 2020. URL https://arxiv. org/abs/2012.00363.

[8] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. ARC prize 2024: Technical report, 2025. URL https://arxiv.org/abs/2412.04604.

[9] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference o n Learning Representations, 2022. URL https://openreview. net/forum?i d=gEZrGCozdqR.

[10] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950.

[11] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. MediTron-

## 70B: Scaling medical pretraining for large language models, 2023. URL https://arxiv.

org/abs/2311.16079.

[12] Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre F. T. Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia Morgado, and Michael Desa. SaulLM-7B: A pioneering large language model for law, 2024. URL https://arxiv.org/ abs/2403.03883.

[13] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension o f text. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings o f the 2016 Conference o n Empirical Methods i n Natural Language Processing. Association for Computational Linguistics, 2016. URL https://aclanthology. org/D16-1264/.

[14] François Chollet. On the measure o f intelligence, 2019. URL https://arxiv.org/abs/ 1911.01547.

[15] Ronen Eldan and Yuanzhi Li. TinyStories: How small can language models b e and still speak coherent English?, 2023. URL https://arxiv.org/abs/2305.07759.

[16] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Conti Kauffmann, Gustavo Henrique d e Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2024. URL https://openreview.net/forum?i d=Fq8tKtjACC.

[17] Zeyuan Allen-Zhu and Yuanzhi Li. Physics o f language models: Part 3.1, knowledge storage and extraction, 2024. URL https://arxiv.org/abs/2309.14316.

[18] Pratyush Maini, Skyler Seto, Richard Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephrasing the web: A recipe for compute and data-efficient language modeling. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings o f the 62nd Annual Meeting o f the Association for Computational Linguistics. Association for Computational Linguistics,

## 2024. URL https://aclanthology.org/2024.acl-long.757/.

11[19] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common Crawl into a refined long-horizon pretraining dataset, 2025. URL https://arxiv.org/abs/ 2412.02595.

[20] Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation o f LLMs help clinical text mining?, 2023. URL https://arxiv.org/abs/2303.04360.

[21] Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. Better synthetic data b y retrieving and transforming existing datasets. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings o f the Association for Computational Linguistics. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024. findings-acl.385/.

[22] Yangjun Ruan, Neil Band, Chris J. Maddison, and Tatsunori Hashimoto. Reasoning t o learn from latent thoughts, 2025. URL https://arxiv.org/abs/2503.18866.

[23] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings o f the

## 61st Annual Meeting o f the Association for Computational Linguistics. Association for Compu-

tational Linguistics, 2023. URL https://aclanthology.org/2023.acl-long.754/.

[24] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4, 2023. URL https://arxiv.org/abs/2304.03277.

[25] Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candes, and Tatsunori Hashimoto. Synthetic continued pretraining. In The Thirteenth International Conference o n Learning Representations,

## 2025. URL https://openreview.net/forum?i d=07yvxWDSla.

[26] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing a t scale. In The Tenth International Conference o n Learning Representations,

## 2022. URL https://openreview.net/forum?i d=0DcZxeWfOPt.

[27] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations i n GPT. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances i n Neural Information Processing Systems. Curran Associates, Inc., 2022. URL https://proceedings.neurips.c c/paper_files/paper/2022/file/ 6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf.

[28] Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Massediting memory i n a transformer. In The Eleventh International Conference o n Learning Representations, 2023. URL https://openreview.net/forum?i d=MkbcAHIYgyS.

[29] Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Eyal Shnarch, and Leshem Choshen. Achieving human parity i n content-grounded datasets generation. In The Twelfth International Conference o n Learning Representations, 2024. URL https://openreview. net/forum?i d=RjYKTQ0L0W.

[30] Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, and Jacob Andreas. Deductive closure training o f language models for coherence, accuracy, and updatability. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings o f the Association for Computational Linguistics. Association for Computational Linguistics, 2024. URL https: //aclanthology.org/2024.findings-acl.584/.

[31] Andrew K. Lampinen, Arslan Chaudhry, Stephanie C. Y. Chan, Cody Wild, Diane Wan, Alex Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, and James L. McClelland. On the generalization o f language models from i n-context learning and finetuning: a controlled study,

## 2025. URL https://arxiv.org/abs/2505.00661.

[32] Core Francisco Park, Zechen Zhang, and Hidenori Tanaka. New News: System-2 fine-tuning for robust integration o f new knowledge, 2025. URL https://arxiv.org/abs/2505.01812.

12[33] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Testtime training with self-supervision for generalization under distribution shifts. In Proceedings o f the 37th International Conference o n Machine Learning. PMLR, 2020. URL http:// proceedings.mlr.press/v119/sun20b.html.

[34] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with masked autoencoders. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances i n Neural Information Processing Systems. Curran Associates, Inc., 2022. URL https://proceedings.neurips.c c/paper_files/paper/2022/file/ bcdec1c2d60f94a93b6e36f937aa0530-Paper-Conference.pdf.

[35] Yu Sun, Xinhao Li, Karan Dalal, Chloe Hsu, Sanmi Koyejo, Carlos Guestrin, Xiaolong Wang, Tatsunori Hashimoto, and Xinlei Chen. Learning t o (learn a t test time), 2024. URL https: //arxiv.org/abs/2310.13807.

[36] Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and Jacob Andreas. The surprising effectiveness o f test-time training for few-shot learning,

## 2025. URL https://arxiv.org/abs/2411.07279.

[37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models t o follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances i n Neural Information Processing Systems. Curran Associates, Inc., 2022. URL https://proceedings.neurips.c c/paper_files/paper/2022/file/ b1efde53be364a73914f58805a001731-Paper-Conference.pdf.

[38] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862.

[39] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning with reasoning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances i n Neural Information Processing Systems. Curran Associates, Inc., 2022. URL https://proceedings.neurips.c c/paper_files/paper/2022/file/ 639a9a172c044fbb64175b5fad42e9a5-Paper-Conference.pdf.

[40] Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron T Parisi, Abhishek Kumar, Alexander A Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura A Culp, Lechao Xiao, Maxwell Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. Beyond human data: Scaling self-training for problem-solving with language models. Transactions o n Machine Learning Research, 2024. URL https://openreview.net/forum?i d=lNAyUngGFK.

[41] DeepSeek-AI. Deepseek-R1: Incentivizing reasoning capability i n LLMs via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948.

[42] Jürgen Schmidhuber. Evolutionary principles i n self-referential learning, 1987. URL https: //people.idsia.c h/~juergen/diploma1987ocr.pdf.

[43] Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning t o learn using gradient descent. In Georg Dorffner, Horst Bischof, and Kurt Hornik, editors, ICANN. Springer Berlin Heidelberg, 2001. URL https://link.springer.com/chapter/10.1007/3-540-44668-0_ 13. 13[44] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation o f deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings o f the 34th International Conference o n Machine Learning, Proceedings o f Machine Learning Research. PMLR, 2017. URL https://proceedings.mlr.press/v70/finn17a.html.

[45] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL 2: Fast reinforcement learning via slow reinforcement learning, 2016. URL https://arxiv. org/abs/1611.02779.

[46] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning t o reinforcement learn,

## 2017. URL https://arxiv.org/abs/1611.05763.

[47] Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. In The Sixth International Conference o n Learning Representations, 2018. URL https://openreview.net/forum?i d=SyX0IeWAW.

[48] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-reinforcement learning o f structured exploration strategies. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances i n Neural Information Processing Systems. Curran Associates, Inc.,

## 2018. URL https://proceedings.neurips.c c/paper_files/paper/2018/file/

4de754248c196c85ee4fbdcee89179bd-Paper.pdf.

[49] Qi Sun, Edoardo Cetin, and Yujin Tang. Transformer-Squared: Self-adaptive LLMs, 2025. URL https://arxiv.org/abs/2501.06252.

[50] Jurgen Schmidhuber. Steps towards ‘self-referential’ neural learning: A thought experiment,

## 1992. URL https://people.idsia.c h/~juergen/selfref1992.pdf.

[51] Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. A modern self-referential weight matrix that learns t o modify itself. In International Conference o n Machine Learning. PMLR, 2022. URL https://proceedings.mlr.press/v162/irie22b.html.

[52] Chenmien Tan, Ge Zhang, and Jie Fu. Massive editing for large language models via meta learning, 2024. URL https://arxiv.org/abs/2311.04661.

[53] Nathan Hu, Eric Mitchell, Christopher Manning, and Chelsea Finn. Meta-learning online adaptation o f language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings o f the 2023 Conference o n Empirical Methods i n Natural Language Processing. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023. emnlp-main.268/.

[54] Tong Chen, Hao Fang, Patrick Xia, Xiaodong Liu, Benjamin Van Durme, Luke Zettlemoyer, Jianfeng Gao, and Hao Cheng. Generative Adapter: Contextualizing language models i n parameters with a single forward pass. In The Thirteenth International Conference o n Learning Representations, 2025. URL https://openreview.net/forum?i d=bc3sUsS6ck.

[55] Dan A. Calian, Gregory Farquhar, Iurii Kemaev, Luisa M. Zintgraf, Matteo Hessel, Jeremy Shar, Junhyuk Oh, András György, Tom Schaul, Jeffrey Dean, Hado van Hasselt, and David Silver. DataRater: Meta-learned dataset curation, 2025. URL https://arxiv.org/abs/ 2505.17895.

[56] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback, 2022. URL https://arxiv.org/abs/ 2212.08073. 14[57] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF v s. RLHF: Scaling reinforcement learning from human feedback with AI feedback. In Proceedings o f the 41st International Conference o n Machine Learning, ICML ’24. JMLR.org, 2024.

[58] Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, and Yang Yu. Language model self-improvement b y reinforcement learning contemplation. In The Twelfth International Conference o n Learning Representations, 2024. URL https://openreview.net/forum?i d=38E4yUbrgr.

[59] Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, and Huaxiu Yao. CREAM: Consistency regularized self-rewarding language models. In The Thirteenth International Conference o n Learning Representations, 2025. URL https: //openreview.net/forum?i d=Vf6RDObyEF.

[60] Yuda Song, Hanlin Zhang, Carson Eisenach, Sham M. Kakade, Dean Foster, and Udaya Ghai. Mind the gap: Examining the self-improvement capabilities o f large language models. In The Thirteenth International Conference o n Learning Representations, 2025. URL https: //openreview.net/forum?i d=mtJSMcF3ek.

[61] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings o f the 2023 Conference o n Empirical Methods i n Natural Language Processing. Association for Computational Linguistics, 2023. URL https://aclanthology. org/2023.emnlp-main.67/.

[62] Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, and Jane Yu. Self-consistency preference optimization, 2024. URL https://arxiv.org/abs/2411.04109.

[63] Audrey Huang, Adam Block, Dylan J Foster, Dhruv Rohatgi, Cyril Zhang, Max Simchowitz, Jordan T. Ash, and Akshay Krishnamurthy. Self-improvement i n language models: The sharpening mechanism. In The Thirteenth International Conference o n Learning Representations,

## 2025. URL https://openreview.net/forum?i d=WJaUkwci9o.

[64] Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding, and Bowen Zhou. TTRL: Test-time reinforcement learning, 2025. URL https://arxiv.org/ abs/2504.16084.

[65] Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette. Can large reasoning models self-train?, 2025. URL https://arxiv.org/abs/2505.21444.

[66] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits o f mathematical reasoning i n open language models, 2024. URL https://arxiv.org/abs/ 2402.03300.

[67] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347.

[68] W. R. Gilks and P. Wild. Adaptive rejection sampling for gibbs sampling. Journal o f the Royal Statistical Society, 1992. URL http://www.jstor.org/stable/2347565.

[69] Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. When should w e prefer offline reinforcement learning over behavioral cloning? In The Tenth International Conference o n Learning Representations, 2022. URL https://openreview.net/forum?i d=AP1MKT37rJ.

[70] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship o n learning mathematical reasoning with large language models, 2023. URL https://arxiv.org/abs/2308.01825.

[71] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge i n a neural network,

## 2015. URL https://arxiv.org/abs/1503.02531.

15[72] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation o f large language models. In The Tenth International Conference o n Learning Representations, 2022. URL https: //openreview.net/forum?i d=nZeVKeeFYf9.

[73] Michael McCloskey and Neal J. Cohen. Catastrophic interference i n connectionist networks: The sequential learning problem, 1989. URL https://www.sciencedirect.com/science/ article/pii/S0079742108605368.

[74] Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation o f catastrophic forgetting i n gradient-based neural networks. In The Second International Conference o n Learning Representations, 2014. URL https://openreview. net/forum?i d=oXSw7laxwUpln.

[75] Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and Changjie Fan. Learning t o utilize shaping rewards: A new approach o f reward shaping. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances i n Neural Information Processing Systems. Curran Associates, Inc., 2020. URL https://proceedings.neurips.c c/paper_files/paper/2020/file/ b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf.

[76] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2Reward: Reward shaping with language models for reinforcement learning,

## 2024. URL https://arxiv.org/abs/2309.11489.

[77] Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, and Yanghua Xiao. Reward shaping t o mitigate reward hacking i n RLHF, 2025. URL https://arxiv.org/abs/2502. 18770.

[78] Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Jie Shi, Xiang Wang, Xiangnan He, and Tat-Seng Chua. AlphaEdit: Null-space constrained model editing for language models. In The Thirteenth International Conference o n Learning Representations, 2025. URL https: //openreview.net/forum?i d=HvSytvg3Jh.

[79] Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. Superposition o f many models into one. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances i n Neural Information Processing Systems. Curran Associates, Inc., 2019. URL https://proceedings.neurips.c c/paper_files/paper/ 2019/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf.

[80] Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rl’s razor: Why online reinforcement learning forgets less, 2025. URL https://arxiv.org/abs/2509.04259.

[81] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will w e run out o f data? Limits o f LLM scaling based o n human-generated data,

## 2024. URL https://arxiv.org/abs/2211.04325.

[82] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, e t a l. GPT-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774.

[83] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings o f the 26th ACM SIGKDD International Conference o n Knowledge Discovery and Data Mining, KDD ’20. Association for Computing Machinery, 2020. URL https: //doi.org/10.1145/3394486.3406703.

[84] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. In Proceedings o f the ACM SIGOPS 29th Symposium o n Operating Systems Principles, 2023. URL https://d l.acm.org/doi/10. 1145/3600006.3613165. 16[85] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego d e las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B, 2023. URL https://arxiv.org/abs/2310.06825. 17A Experimental Details: Few-shot LearningA.1 Model and SetupFor the few-shot learning experiments, w e use Llama-3.2-1B-Instruct [3] a s the base language model. Since this model has n o specialized training o n ARC, its ability t o solve ARC tasks i s limited. To enable controlled evaluation, w e curated a small set o f ARC problems from the training and evaluation splits that are solvable with optimal TTT hyperparameters.

Training Set: We selected 11 ARC tasks from the training set a s the environment for RL optimization. Evaluation Set: We selected 8 distinct ARC problems from the evaluation set for measuring generalization performance. These 8 were explicitly filtered for being amenable t o TTT out o f the full evaluation set.

These sets were chosen t o isolate the effect o f self-edit learning rather than general ARC ability.

A.2 Training ProcedureWe trained SEAL using a single outer loop o f reinforcement learning o n the 11 training problems. For each problem, the model generated 15 self-edit configurations consisting o f:

• Data augmentation strategy: Controls whether t o include variations such a s basic examples, size-based augmentations, chained reasoning, and repeated inputs: include_basic, include_size, include_chain, include_repeat.

• Optimization configuration: Specifies hyperparameters such a s learning rate, number o f epochs, and loss function type (e.g., whether t o compute loss over all tokens o r only the final output tokens).

Each configuration was evaluated via test-time training (TTT), and assigned a binary reward: 1 i f the adapted model produced the correct solution, 0 otherwise using Akyürek e t a l. [36]’s evaluation protocol. To encourage efficient adaptation, w e discarded configurations requiring more than 375 training steps, retained only configurations with reward 1 and acceptable cost for LoRA-based SFT.

SFT for TTT was done with the following hyperparameters:

Parameter Value LoRA rank 128 LoRA alpha 16 Learning rate N/A (model chooses i t) num_train_epochs N/A (model chooses i t)LoRA adapters were inserted into the following modules: q_proj, v_proj, gate_proj, down_proj, up_projFor the RL SFT w e used the following hyperparameters:

Parameter Value lora_rank 16 lora_alpha 16 num_train_epochs 8 per_device_train_batch_size 5 learning_rate 5e-5When generating the self-edits, w e add the following prompt a s a suffix t o the few-shot examples.

18You are configuring a model training pipeline b y selecting from predefined tools. You must make two decisions:

## 1. Data Generation Tools — For each o f the following, choose true o r false:

- use_basic_augmentations - use_size_augmentations - use_chain_augmentations - use_repeat_augmentations

## 2. Training Configuration — Choose one o f:

- "train_using_all_tokens" - "train_using_output_tokens"Also specify: - learning_rate (float) - num_train_epochs (integer)Output FormatRespond with a valid JSON object. Do not include any explanation, markdown, o r extra text. Use lowercase true/false for booleans and ensure correct JSON syntax. Example output: { "data_generation": { "use_basic_augmentations": ..., "use_size_augmentations": ..., "use_chain_augmentations": ..., "use_repeat_augmentations": ... }, "training": { "strategy": ..., "learning_rate": ..., "num_train_epochs": ... } }A.3 Evaluation DetailsFor each o f the 8 held-out evaluation tasks, the model generated 5 self-edit configurations, yielding a total o f 40 configurations. Success was measured a s the percentage o f configurations that led t o correct outputs after adaptation. We followed the evaluation protocol from Akyürek e t a l. [36].

For the Oracle TTT w e used the following configs:

Parameter Value lora_rank 128 lora_alpha 16 num_train_epochs 2 batch_size 2 learning_rate 1e-4A.4 Compute ResourcesWe performed all training runs o n a single A100, H100, o r H200. Each TTT per problem requires between half a minute t o a few minutes, which i s also why w e limited the number o f samples for ReSTEM and additionally limited the number o f gradient steps allowed per self-edit TTT. Overall ReSTEM took around 2-3 hours. 19B Experimental Details: Knowledge IncorporationB.1 Model and SetupWe use the Qwen-2.5-7B base model [5] i n the knowledge incorporation experiments. We repurpose the SQuAD dataset v 1.1 [13] for the task o f answering questions without the passage i n-context. We use the training set for RL training and a 200-article subset o f the evaluation set for evaluation. Within the training set and evaluation set, there are some overlapping topics o f passages, but there i s n o overlap between these sets, s o w e can b e sure that there i s n o data contamination o f the test passages due t o RL training.

B.2 RL Training ProcedureWe run 2 rounds o f ReSTEM training [40]. On each round, w e take a batch o f 50 context-questionsanswers triples from the SQuAD training set. For each context, w e sample 5 self-edit generations a t temperature 1. We evaluate each self-edit over 3 random seeds, training o n the sequences and then evaluating the updated model o n the corresponding questions. We average each generation’s results over 3 seeds and then keep the single best generation for each o f the 50 contexts. Finally, t o finish the round o f ReSTEM, w e perform supervised finetuning o n the 50 resulting prompt-completion pairs.

Supervised finetuning here i s done with batch size o f 10, for 2 epochs, with learning rate 3e-4, using LoRA [72] with rank 64 and alpha 128, applied t o all MLP and attention projection layers.

B.3 Synthetic Data Generation and Finetuning DetailsIn all models, w e generate synthetic data b y prompting t o generate implications o f the passage:

Let’s read the following passage and produce a list o f implications derived directly o r indirectly from the content.

Passage: {passage}Implications:

We then take the resulting generated sequence. In the single-passage case, w e split i t b y newlines into a set o f training documents. In the multi-passage case, w e use the full generated sequence a s a single training document. In the case o f synthetic data from GPT-4.1 (gpt-4.1-2025-04-14), a n instruct-model, w e additionally have the following rule: If the second line begins with a “1.” then w e omit the first line from the training set. This i s because w e found that the first line often contained filler text (e.g. “Sure, here i s the list o f implications:”).

We then use the following training hyperparameters:

Table 3: Single-Passage Knowledge Incorporation HyperparametersParameter Search SpaceLoRA Rank (r) [32, 64] LoRA Alpha (α) [32, 64] Learning Rate [1e-4, 3e-4, 5e-4, 1e-3, 2e-3] Epochs [1, 5, 10, 15, 20] Batch Size [1, 4]In the multi-passage n = 200 case, w e sample 5 self-edit completions for each passage and take the aggregate dataset o f all self-edits across all passages t o train o n.

To answer the corresponding questions, w e use the following prompt:



Table 4: Multi-Passage Knowledge Incorporation HyperparametersParameter Search SpaceLoRA Rank (R) [32, 64] LoRA Alpha (α) [32, 64] Learning Rate [1e-4, 3e-4, 5e-4, 1e-3, 2e-3] Epochs [1, 3, 5] Batch Size [1, 4, 8, 16]Let’s answer a question directly and concisely. Question: {question} Answer:

B.4 Evaluation DetailsWe evaluate o n a 200-passage subset o f the SQuAD evaluation set, consisting o f a combined 974 evaluation questions (roughly 5 corresponding t o each passage). The pipeline o f generating synthetic data and finetuning o n i t i s the same a s above. For automated grading, w e use gpt-4.1-2025-04-14 [82] via the OpenAI API with greedy decoding.

The grading prompt i s a s follows:

You are a grading assistant. Your job i s t o determine whether a student’s answer correctly answers the question based solely o n the provided gold answer. Do not use any outside knowledge. The student answer can include additional information, but i t must a t least fully convey the gold answer and must not contradict i t. Ignore style, phrasing, o r extra details that d o not affect correctness. Respond ONLY with ‘yes’ o r ‘n o’.

Question: {question} Gold answer: {gold} Student answer: {pred} Is the student answer correct based solely o n the gold answer? Respond ‘yes’ o r ‘n o’.

B.5 Compute ResourcesAll experiments are performed o n 2×H100 o r 2×H200. We use DeepSpeed ZeRO-3 [83] for SFT i n ReST EM training. We use vLLM [84] for efficient inference. The most compute-intensive portion o f our training and evaluation i s the E-step o f ReST EM training, where the model generates completions and i s graded through the inner-loop process o f finetuning and running inference. Doing a single round requires a batch o f 50 passages over 5 completions and 3 runs per completion, meaning 750 inner loop iterations. This takes about 6 hours o n 2×H100s.

B.6 Standard Error o f the Mean i n Catastrophic Forgetting ExperimentThe standard errors o f the mean (SEM) for each entry i n Figure 6 i s shown below i n Table B.6.

B.7 Scaling Model SizeWe further experimented with the 3B-parameter Qwen variant, with the same single-passage setup a s i n Figure 4. The results are given i n Table 6.

To compare the benefit o f SEAL over using self-edits generated b y the base model, w e compute the ratio o f SEAL’s improvement over the base model t o the improvement from base model self-edits. This ratio i s 1.75× for the 3B model and 2.04× for the 7B model. The relative improvement i s greater for the 7B model, which provides some evidence that not only are stronger base models more effective a t leveraging synthetic data for self-adaptation, but reinforcement learning may have compounding

Table 5: Entrywise standard errors o f the mean (SEM) across continual self-edits experiment.

## 1 2 3 4 5 6 7 8

## 0 0.0306 0.0315 0.0263 0.0318 0.0297 0.0370 0.0310 0.0284

## 1 0.0273 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000

## 2 0.0305 0.0277 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000

## 3 0.0277 0.0358 0.0406 0.0000 0.0000 0.0000 0.0000 0.0000

## 4 0.0272 0.0303 0.0337 0.0320 0.0000 0.0000 0.0000 0.0000

## 5 0.0296 0.0342 0.0290 0.0298 0.0319 0.0000 0.0000 0.0000

## 6 0.0289 0.0334 0.0271 0.0258 0.0320 0.0337 0.0000 0.0000

## 7 0.0255 0.0313 0.0264 0.0253 0.0309 0.0331 0.0363 0.0000

## 8 0.0237 0.0307 0.0211 0.0267 0.0273 0.0271 0.0358 0.0263

Table 6: Model Size Scaling Performance (%).

Model Base Model (No Training) Base Model Self-Edit SEALQwen2.5-3B 25.1 31.9 37.0 Qwen2.5-7B 32.7 39.7 47.0benefits a s model capacity increases. We acknowledge that i t i s hard t o draw conclusions though without actually scaling u p further.

B.8 Comparison t o Generative AdapterWe additionally compared with Generative Adapter [54], a hypernetwork approach that generates LoRA weights from context, using our evaluation setup. Table 7 reports results for both singlepassage (n=1) and continued pretraining (n = 200). We use the Mistral-7B-based model [85] for Generative Adapter, since that was the closest model for comparison. All values are o n the same evaluation set, but CPT batches updates over all documents while single-passage trains and evaluates a n adapter separately for each document. Generative Adapter achieves strong performance i n the n=1 case, but underperforms SEAL i n the CPT setting. SEAL’s parameterization o f weight updates through synthetic data generation allows reuse o f generated data for CPT, application t o arbitrary base models, and flexibility t o learn updates from diverse interaction types beyond LoRA finetuning.

Table 7: SEAL v s. Generative Adapter Performance (%).

Model Base Single-passage (n=1) CPT (n = 200)SEAL 32.0 47.0 58.2 Generative Adapter 24.4 66.8 28.0We note that parameterizing weight updates via synthetic data generation rather than directly predicting LoRA weights has several advantages: (1) generated data can b e reused for CPT o r applied t o arbitrary base models, (2) models can leverage reasoning and restructuring a s document scale and complexity grow, and (3) the framework i s not restricted t o LoRA finetuning, allowing for many different update types, including those arising from environment o r user interactions. By contrast, i t i s unclear how hypernetwork-based approaches would scale t o such settings, while next-token prediction o n generated data naturally exploits a model’s i n-context learning capabilities.

B.9 Comparison t o EntigraphWe additionally compare SEAL t o Entigraph [25] i n the Synthetic Continued Pretraining (SCPT) setting o n SQuAD. Results for both 200 and 2067 passages are shown i n Table 8. SEAL uses the same 5 synthetic data generations per document. For Entigraph, w e sample 5 synthetic data generations involving pairs and 5 triplets o f entities per document. Entigraph with all 10 synthetic data generations sampling i s competitive with SEAL, especially a t the larger scale. These results22suggest that RL-trained self-edits and structured heuristic methods are both strong approaches for synthetic data generation.

Table 8: Synthetic Continued Pretraining (SCPT) o n SQuAD (n o passage i n context). Best i n each column i s bolded.

Method Continued Pretraining (n=200) Continued Pretraining (n=2067)SEAL 58.2 46.4 Entigraph (pairs) 46.2 38.6 Entigraph (pairs+triples) 56.0 48.6B.10 Proxy RewardWe experiment with replacing the inner loop with a proxy reward based o n a human-crafted rubric with 4 categories: length, diversity, quality, and correctness. A GPT-4.1 grader scores each category o n a 1-5 scale, and the sum o f these scores i s used a s the RL reward. Table 9 reports final results and RL training times. Table 9: Full Reward v s. Proxy Reward Performance (%).

Model Base Post-RL TimeSEAL 32.0 47.0 ≈6 h r SEAL w/ Proxy-Reward 32.0 45.6 ≈5 minWhile further tuning o f the rubric o r metric design could strengthen the reward signal, the advantage o f the full SEAL loop i s that n o such manual specification i s required—the model directly learns which edits improve its own performance. Both approaches appear promising for scaling t o larger model sizes and compute budgets: proxy metrics offer dramatically lower cost, and with refinement, they may even surpass the “true” reward o f directly optimizing for post-finetuning performance.

B.11 PromptingRecent works have shown that reinforcement learning baselines and outcomes can b e highly sensitive t o prompting. We experiment with 6 additional self-edit prompts i n the knowledge-incorporation setting. The seven prompts—implications, implications-long, implications-very-long, implications-chain-o f-thought, rewrite, self-q a, and n o-prompt—are shown below. All results i n the main content o f the paper used the implications prompt, which w e consider t o b e the most prototypical [30, 31]. However, prior work has found prompts involving rewriting o r generating question-answer pairs can b e more effective, a s discussed i n §2.

Furthermore, a s w e see qualitatively i n Figure 5, RL appears t o have dramatically increased the length o f the response o f the example. We therefore experiment with prompting for longer generations with implications-long and implications-very-long t o test i f w e can achieve similar gains through prompting alone.

The results are shown i n Table 10. Notably, the baselines for implications-long and rewrite the RL-trained version o f implications. However, using these prompts a s the base o f RL training yields even greater improvements. In all cases, ReSTEM enhanced performance b y roughly 6 t o 11 percentage points.

Here, “Chain-o f-thought-eval” refers t o having the model reason before answering the questions (letting the model “pull out” information from its weights), rather than chain-o f-thought before generating synthetic data, which i s done with the base implications prompt. However, w e did not notice a substantial difference i n our setting when chain-o f-thought was applied, whether before answering and before writing synthetic data.

Letting the model “determine its own” self-edit format, with n o-prompt, was not able t o achieve the same results a s predefined prompting formats i n our experiments, achieving only 18.9% after 2 rounds o f training. 23Method Original Round 1 Round 2 gpt-4.1 synthetic No self-edit 33.5 – – – Implications 39.7 43.7 47.0 46.3 Implications-long 49.3 52.4 54.4 54.1 Implications-very-long 45.0 51.5 52.1 40.9 Rewrite 49.4 55.3 55.6 54.4 Self-QA 37.3 42.8 48.7 39.2 No-Prompt 13.8 12.7 18.9 28.6 Implications-chain-o f-thought 38.7 – – – Chain-o f-thought-eval 37.8 – – –Table 10: Performance across 2 rounds o f ReSTEM RL training o n various prompts i n the singledocument knowledge incorporation setting. The gpt-4.1 column reports performance using synthetic data generated b y gpt-4.1 with the corresponding prompt format.

The five prompts are shown below.

implicationsLet’s read the following passage and produce a list o f implications derived directly o r indirectly from the content.

Passage: {passage}Implications:

implications-longLet’s read the following passage and produce a long list o f implications derived directly o r indirectly from the content.

Passage: {passage}Implications:

implications-very-longLet’s read the following passage and produce a very long list o f implications derived directly o r indirectly from the content.

Passage: {passage}Implications:

implications-chain-o f-thoughtLet’s read the following passage, think step b y step, and then produce a list o f implications derived directly o r indirectly from the content. We should first generate a "Thought Process" and then "Implications"Passage: {passage}Thought Process: 24no-prompt{passage}rewriteLet’s read the following passage and rewrite i t i n a few different ways, each one separated b y a newline.

Passage: {passage}Rewritten passages:

self-qaLet’s read the following passage and rewrite i t i n a question-answer format.

Passage: {passage}Question 1:

Note: For self-q a, w e apply additional formatting s o that training documents consist o f question–answer pairs, rather than using our standard approach o f splitting b y newline characters. Specifically, w e split the output using occurrences o f “Question n:” instead o f newlines.

25